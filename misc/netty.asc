= netty
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:homepage: http://orientye.com
<<<

== 概览

=== 特点
参考: https://netty.io/index.html

netty5.0已经废弃:
主要原因: The major change of using a ForkJoinPool increases complexity and has not demonstrated a clear performance benefit. Also keeping all the branches in sync is quite some work without a real need for it as there is nothin in current master which I think justifies a new major release.
参考: https://github.com/netty/netty/issues/4466

说明: 本文基于netty最新稳定版本4.1。

=== java nio
since 1.4, the new non-blocking API
https://docs.oracle.com/javase/8/docs/api/java/nio/package-summary.html

- Buffer
- Channel
- Selector

nio2:
since java7
主要变化:
1. 加入了一种异步IO模式，利用事件回调机制(CompletionHandler等接口)，处理Accept、Read等操作，例如AsynchronousServerSocketChannel(对应ServerSocketChannel)及AsynchronousSocketChannel(对应SocketChannel)。
2. introduced better file management(java.nio.file package).
参考: https://stackoverflow.com/questions/25537675/java-what-exactly-is-the-difference-between-nio-and-nio-2

JDK Epoll空轮询bug:
https://zhuanlan.zhihu.com/p/92133508
https://www.jianshu.com/p/3ec120ca46b2
https://www.cnblogs.com/JAYIT/p/8241634.html

=== 与JDK NIO相比
- 易用性
JDK NIO需要了解很多复杂的概念，例如Channels、Selectors、Sockets、Buffers等，编码复杂。Netty则在NIO基础上进行了更高层次的封装，屏蔽了NIO的复杂性。

- 稳定性
Netty更加可靠稳定，修复与完善了JDK NIO较多的已知问题，例如select空转导致CPU消耗100%，TCP断线重连，keep-alive检测等问题。

- 可扩展性
例如：
可定制化的线程模型，可以通过启动的配置参数选择Reactor线程模型；
可扩展的事件驱动模型，将框架层和业务层的关注点分离。大部分情况下，开发者只需要关注ChannelHandler的业务逻辑实现。

== 线程模型
=== 类型
- 单线程模型

    EventLoopGroup只包含一个EventLoop
    Boss和Worker使用同一个EventLoopGroup

- 多线程模型

    EventLoopGroup包含多个EventLoop
    Boss和Worker使用同一个EventLoopGroup

- 主从多线程模型

    EventLoopGroup包含多个EventLoop
    Boss是主Reactor，Worker是从Reactor，分别使用不同的EventLoopGroup
    主Reactor负责新的网络连接Channel的创建，然后把Channel注册到从Reactor

=== reactor单线程
- 所有IO操作在同一个NIO线程上完成

[source, java]
----
EventLoopGroup bossGroup = new NioEventLoopGroup(1);
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup)
 .channel(NioServerSocketChannel.class)
 ...
----

=== reactor多线程
- 一个连接只对应一个NIO线程

[source, java]
----
EventLoopGroup bossGroup = new NioEventLoopGroup(4);
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup)
 .channel(NioServerSocketChannel.class)
 ...
----
如果NioEventLoopGroup()构造时没有设置线程数目，则线程数是CPU核数的2倍?

=== 主从reactor多线程
- 主reactor线程池: acceptor线程池
- 从reactor线程池(sub reactor线程池)
- acceptor线程池仅用于客户端的登录，握手和安全认证，一旦连接建立成功，便将链路注册到subreactor线程池的IO线程上，由IO线程负责后续的IO操作

[source, java]
----
EventLoopGroup bossGroup = new NioEventLoopGroup(4);
EventLoopGroup workerGroup = new NioEventLoopGroup();
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup, workerGroup)
 .channel(NioServerSocketChannel.class)
 ...
----

=== Netty线程最佳实践
- 时间可控的简单业务直接在IO线程上处理
    如果业务非常简单，执行时间非常短，不需要与外部交互、访问数据库和磁盘，不需要等待其它资源，则建议直接在业务ChannelHandler中执行，不需要再启业务的线程或者线程池。避免线程上下文切换，也不存在线程并发问题。

- 复杂和时间不可控业务建议投递到后端业务线程池统一处理
    对于此类业务，不建议直接在业务ChannelHandler中启动线程或者线程池处理，建议将不同的业务统一封装成Task，统一投递到后端的业务线程池中进行处理。
    过多的业务ChannelHandler会带来开发效率和可维护性问题，不要把Netty当作业务容器，对于大多数复杂的业务产品，仍然需要集成或者开发自己的业务容器，做好和Netty的架构分层。

- 业务线程避免直接操作ChannelHandler
    对于ChannelHandler，IO线程和业务线程都可能会操作，因为业务通常是多线程模型，这样就会存在多线程操作ChannelHandler。为了尽量避免多线程并发问题，建议按照Netty自身的做法，通过将操作封装成独立的Task由NioEventLoop统一执行，而不是业务线程直接操作。

参考: https://www.infoq.cn/article/netty-threading-model

=== 参考
https://gee.cs.oswego.edu/dl/cpjslides/nio.pdf[Doug Lea 《scalable io in java》]

== 核心组件
=== BootStrap与ServerBootStrap
引导器主要负责整个Netty程序的启动、初始化、服务器连接等过程，串联了Netty的其它核心组件。

引导器共分为两种类型:
用于客户端引导的Bootstrap，用于服务端引导ServerBootStrap，均继承自抽象类AbstractBootstrap。

=== EventLoop和EventLoopGroup
==== 概念
- EventLoop与EventLoopGroup:
    NioEventLoopGroup实际上是个线程池
    一个EventLoopGroup包含一个或者多个EventLoop

- EventLoop与Thread:
    一个EventLoop在它的生命周期内只有一个Thread绑定
    EnventLoop处理的I/O事件都将在它专有的Thread上进行

- EventLoop与Channel:
    一个Channel在它的生命周期内只注册于一个EventLoop
    每个EventLoop负责处理一个或多个Channel

也就是说，一个TCP连接是与一个固定的线程绑定的，即: 对于一个给定的Channel，其I/O操作都是由相同的Thread执行的。

==== EventLoopGroup
class NioEventLoopGroup:
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/nio/NioEventLoopGroup.java

BossEventLoopGroup与WorkerEventLoopGroup包含一个或者多个NioEventLoop。

BossEventLoopGroup负责监听客户端的Accept事件，当事件触发时，将事件注册至WorkerEventLoopGroup中的一个NioEventLoop上。每新建一个Channel，只选择一个NioEventLoop与其绑定，因此Channel生命周期的所有事件处理都是线程独立的，不同的NioEventLoop线程之间不会发生任何交集。

NioEventLoop完成数据读取后，会调用绑定的ChannelPipeline进行事件传播，ChannelPipeline也是线程安全的，数据会被传递到ChannelPipeline的第一个ChannelHandler中。数据处理完成后，将加工完成的数据再传递给下一个ChannelHandler，整个过程是串行化执行，不会发生线程上下文切换的问题。

==== NioEventLoop
===== run()
核心方法: run()
[source, java]
.https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/nio/NioEventLoop.java
----
@Override
    protected void run() {
        int selectCnt = 0;
        for (;;) {
            try {
                int strategy;
                try {
                    strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());
                    switch (strategy) {
                    case SelectStrategy.CONTINUE:
                        continue;

                    case SelectStrategy.BUSY_WAIT:
                        // fall-through to SELECT since the busy-wait is not supported with NIO

                    case SelectStrategy.SELECT:
                        long curDeadlineNanos = nextScheduledTaskDeadlineNanos();
                        if (curDeadlineNanos === -1L) {
                            curDeadlineNanos = NONE; // nothing on the calendar
                        }
                        nextWakeupNanos.set(curDeadlineNanos);
                        try {
                            if (!hasTasks()) {
                                strategy = select(curDeadlineNanos);
                            }
                        } finally {
                            // This update is just to help block unnecessary selector wakeups
                            // so use of lazySet is ok (no race condition)
                            nextWakeupNanos.lazySet(AWAKE);
                        }
                        // fall through
                    default:
                    }
                } catch (IOException e) {
                    // If we receive an IOException here its because the Selector is messed up. Let's rebuild
                    // the selector and retry. https://github.com/netty/netty/issues/8566
                    rebuildSelector0();
                    selectCnt = 0;
                    handleLoopException(e);
                    continue;
                }

                selectCnt++;
                cancelledKeys = 0;
                needsToSelectAgain = false;
                final int ioRatio = this.ioRatio;
                boolean ranTasks;
                if (ioRatio === 100) {
                    try {
                        if (strategy > 0) {
                            processSelectedKeys();
                        }
                    } finally {
                        // Ensure we always run tasks.
                        ranTasks = runAllTasks();
                    }
                } else if (strategy > 0) {
                    final long ioStartTime = System.nanoTime();
                    try {
                        processSelectedKeys();
                    } finally {
                        // Ensure we always run tasks.
                        final long ioTime = System.nanoTime() - ioStartTime;
                        ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio);
                    }
                } else {
                    ranTasks = runAllTasks(0); // This will run the minimum number of tasks
                }

                if (ranTasks || strategy > 0) {
                    if (selectCnt > MIN_PREMATURE_SELECTOR_RETURNS && logger.isDebugEnabled()) {
                        logger.debug("Selector.select() returned prematurely {} times in a row for Selector {}.",
                                selectCnt - 1, selector);
                    }
                    selectCnt = 0;
                } else if (unexpectedSelectorWakeup(selectCnt)) { // Unexpected wakeup (unusual case)
                    selectCnt = 0;
                }
            } catch (CancelledKeyException e) {
                // Harmless exception - log anyway
                if (logger.isDebugEnabled()) {
                    logger.debug(CancelledKeyException.class.getSimpleName() + " raised by a Selector {} - JDK bug?",
                            selector, e);
                }
            } catch (Error e) {
                throw e;
            } catch (Throwable t) {
                handleLoopException(t);
            } finally {
                // Always handle shutdown even if the loop processing threw an exception.
                try {
                    if (isShuttingDown()) {
                        closeAll();
                        if (confirmShutdown()) {
                            return;
                        }
                    }
                } catch (Error e) {
                    throw e;
                } catch (Throwable t) {
                    handleLoopException(t);
                }
            }
        }
    }
----

NioEventLoop每次循环的处理流程都包含事件轮询select、事件处理processSelectedKeys、任务处理runAllTasks几个步骤，并且提供了一个参数ioRatio，可以调整I/O事件处理和任务处理的时间比例。

===== select()与JDK空轮询Bug
https://solthx.github.io/2020/10/15/JDK%E7%A9%BA%E8%BD%AE%E8%AF%A2Bug%E5%8F%8A%E5%87%BA%E7%8E%B0%E5%8E%9F%E5%9B%A0/

===== 任务处理
NioEventLoop不仅负责处理I/O事件，还兼顾执行任务队列中的任务。

任务队列遵循FIFO规则，可以保证任务执行的公平性。

NioEventLoop处理的任务类型基本可以分为三类:

- 普通任务
通过NioEventLoop的execute()向任务队列taskQueue中添加的任务。
例如Netty在写数据时会封装WriteAndFlushTask提交给taskQueue。
taskQueue的实现类是多生产者单消费者队列MpscChunkedArrayQueue，在多线程并发添加任务时，可以保证线程安全。

- 定时任务
通过调用NioEventLoop的schedule()向定时任务队列scheduledTaskQueue添加一个定时任务，用于周期性执行该任务。
例如心跳消息发送等。
定时任务队列scheduledTaskQueue采用优先队列PriorityQueue实现。

- 尾部任务
tailTasks相比于普通任务队列优先级较低，在每次执行完taskQueue中任务后会去获取尾部队列中任务执行。尾部任务并不常用，主要用于做一些收尾工作，例如统计事件循环的执行时间、监控信息上报等。

[source, java]
.https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/concurrent/SingleThreadEventExecutor.java
----
protected boolean runAllTasks() {
    assert inEventLoop();
    boolean fetchedAll;
    boolean ranAtLeastOne = false;

    do {
        fetchedAll = fetchFromScheduledTaskQueue();
        if (runAllTasksFrom(taskQueue)) {
            ranAtLeastOne = true;
        }
    } while (!fetchedAll); // keep on processing until we fetched all scheduled tasks.

    if (ranAtLeastOne) {
        lastExecutionTime = getCurrentTimeNanos();
    }
    afterRunningAllTasks();
    return ranAtLeastOne;
}
----

===== EventLoop最佳实践
- 网络连接建立过程中三次握手、安全认证的过程会消耗不少时间。建议采用Boss和Worker两个EventLoopGroup，有助于分担Reactor线程的压力。

- 由于Reactor线程模式适合处理耗时短的任务场景，对于耗时较长的ChannelHandler可以考虑维护一个业务线程池，将编解码后的数据封装成Task进行异步处理，避免ChannelHandler阻塞而造成EventLoop不可用。

- 如果业务逻辑执行时间较短，建议直接在ChannelHandler中执行。例如编解码操作，这样可以避免过度设计而造成架构的复杂性。

- 不宜设计过多的ChannelHandler。对于系统性能和可维护性都会存在问题，在设计业务架构的时候，需要明确业务分层和Netty分层之间的界限。不要一味地将业务逻辑都添加到ChannelHandler中。

=== Channel
Channel(通道)是网络通信的载体。
Channel提供了基本的API用于网络I/O操作，例如register、bind、connect、read、write、flush等。
Netty的Channel以JDK NIO Channel为基础，相较于JDK NIO，提供了更高层次的抽象，屏蔽了底层Socket的复杂性，提供了更强大的功能。

https://netty.io/4.1/api/io/netty/channel/Channel.html

ChannelGroup:
https://netty.io/4.1/api/io/netty/channel/group/ChannelGroup.html

=== ChannelPipeline
ChannelPipeline是Netty的核心编排组件，负责组装各种ChannelHandler。
ChannelPipeline通过双向链表将不同的ChannelHandler链接在一起。
当读写事件触发时，ChannelPipeline会依次调用ChannelHandler列表对Channel的数据进行拦截和处理。

https://netty.io/4.1/api/io/netty/channel/ChannelPipeline.html

ChannelPipeline是线程安全的。
每一个新的Channel都会对应绑定一个新的ChannelPipeline。
一个ChannelPipeline关联一个EventLoop。

客户端和服务端都有各自的ChannelPipeline。

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelPipeline.java
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPipeline.java

ChannelPipeline的双向链表分别维护了HeadContext和TailContext的头尾节点。
程序自定义的ChannelHandler会插入到Head和Tail之间，即HeadContext与TailContext。

HeadContext是Inbound处理器，也是Outbound处理器。
实现了ChannelInboundHandler和ChannelOutboundHandler。
网络数据写入操作的入口就是由HeadContext节点完成的。
HeadContext作为Pipeline的头结点负责读取数据并开始传递InBound事件，当数据处理完成后，数据会反方向经过Outbound处理器，最终传递到HeadContext，因此HeadContext又是处理Outbound事件的最后一站。此外HeadContext在传递事件之前，还会执行一些前置操作。

TailContext只实现了ChannelInboundHandler接口。
它会在ChannelInboundHandler调用链路的最后一步执行，主要用于终止Inbound事件传播，例如释放Message数据资源等。TailContext节点作为OutBound事件传播的第一站，仅仅是将OutBound事件传递给上一个节点。

HeadContext与TailContext:
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPipeline.java

从整个ChannelPipeline调用链路来看，如果由Channel直接触发事件传播，那么调用链路将贯穿整个ChannelPipeline。然而也可以在其中某一个ChannelHandlerContext触发同样的方法，这样只会从当前的ChannelHandler开始执行事件传播，该过程不会从头贯穿到尾，在一定场景下，可以提高程序性能。

在Netty中，有两种发送消息的方式: 可以直接写到Channel中，也可以写到和ChannelHandler相关联的ChannelHandlerContext对象中。前一种方式将会导致消息从ChannelPipeline的尾端开始流动，而后者将导致消息从ChannelPipeline中的下一个ChannelHandler开始流动。

=== ChannelHandler
==== 概念
数据的编解码工作以及其他转换工作都是通过ChannelHandler处理的。
一般来说，开发者最关注的是ChannelHandler(很少会直接操作Channel，都是通过ChannelHandler间接完成)。

https://netty.io/4.1/api/io/netty/channel/ChannelHandler.html
https://netty.io/4.1/api/io/netty/channel/ChannelHandlerAdapter.html

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelHandler.java

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelInboundHandler.java
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelOutboundHandler.java

https://netty.io/4.1/api/io/netty/channel/ChannelDuplexHandler.html
典型的双向ChannelHandler:
HttpServerKeepAliveHandler, IdleStateHandler, LoggingHandler

==== ChannelHandlerContext
每创建一个Channel都会绑定一个新的ChannelPipeline，ChannelPipeline中每加入一个ChannelHandler都会绑定一个ChannelHandlerContext。

ChannelHandlerContext用于保存ChannelHandler上下文，ChannelHandlerContext可以实现ChannelHandler之间的交互，ChannelHandlerContext包含了ChannelHandler生命周期的所有事件，如connect、bind、read、flush、write、close等。

ChannelHandlerContext 代表了 ChannelHandler 和 ChannelPipeline 之间的关联，每当有 ChannelHandler 添加到 ChannelPipeline 中时，都会创建 ChannelHandlerContext。ChannelHandlerContext 的主要功能是管理它所关联的 ChannelHandler 和在同一个 ChannelPipeline 中的其他 ChannelHandler 之间的交互。

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelHandlerContext.java

==== 事件传播
Inbound事件和Outbound事件的传播方向是不一样的。
Inbound事件的传播方向为Head -> Tail，而Outbound事件传播方向是Tail -> Head，两者相反。
推荐在系统设计时模拟客户端和服务端的场景画出ChannelPipeline的内部结构图，以避免搞混调用关系。

==== 异常处理
异常事件的处理顺序与ChannelHandler的添加顺序相同，会依次向后传播，与Inbound事件和Outbound事件无关。

Netty中TailContext提供了兜底的异常处理逻辑，但是在很多场景下，不一定能满足应用的需求。

异常处理的最佳实践:
推荐对异常进行统一拦截(在ChannelPipeline自定义处理器的末端添加统一的异常处理器)，然后根据场景实现更加完善的异常处理机制。
示例代码:
[source, java]
----
public class ExceptionHandler extends ChannelDuplexHandler {
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        if (cause instanceof RuntimeException) {
            System.out.println("Handle Business Exception Success.");
        }
    }
}
----

==== 执行与阻塞
通常 ChannelPipeline 中的每一个 ChannelHandler 都是通过它的 EventLoop(I/O 线程)来处理传递给它的事件的。不要阻塞这个线程，因为这会对整体的 I/O 处理产生负面的影响。
但有时可能需要与那些使用阻塞 API 的遗留代码进行交互。对于这种情况，ChannelPipeline 有一些接受一个 EventExecutorGroup 的 add()方法。如果一个事件被传递给一个自定义EventExecutor-Group，它将被包含在这个 EventExecutorGroup 中的某个 EventExecutor 所处理，从而被从该Channel 本身的 EventLoop 中移除。对于这种用例， Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。

=== 编解码器
==== 概念
编解码器可以分为一次解码器和二次解码器:

    一次解码器用于解决TCP拆包/粘包问题，按协议解析后得到字节数据。
    如果需要对解析后的字节数据做对象模型的转换，便需要用到二次解码器。
    编码器同理。

一次编解码器: MessageToByteEncoder与ByteToMessageDecoder
二次编解码器: MessageToMessageEncoder与MessageToMessageDecoder
https://netty.io/4.1/api/io/netty/handler/codec/ByteToMessageDecoder.html
https://netty.io/4.1/api/io/netty/handler/codec/MessageToByteEncoder.html
https://netty.io/4.1/api/io/netty/handler/codec/MessageToMessageDecoder.html
https://netty.io/4.1/api/io/netty/handler/codec/MessageToMessageEncoder.html

==== 编解码器中的引用计数
一旦消息被编码或者解码，它就会被ReferenceCountUtil.release(message)调用自动释放。
如果需要保留引用以便稍后使用，可以调用ReferenceCountUtil.retain(message)方法。

==== ReplayingDecoder
ReplayingDecoder扩展了ByteToMessageDecoder类，使得不必调用readableBytes()。
https://netty.io/4.1/api/io/netty/handler/codec/ReplayingDecoder.html

如果使用ByteToMessageDecoder不会引入太多的复杂性，就不要使用ReplayingDecoder。

==== TooLongFrameException类
Netty提供了 TooLongFrameException 类，其将由解码器在帧超出指定的大小限制时抛出。
为了避免这种情况，可以设置一个最大字节数的阈值，如果超出该阈值，则会导致抛出一个 TooLongFrameException随后会被 ChannelHandler.exceptionCaught()方法捕获）。如何处理该异常则完全取决于该解码器的用户: 某些协议（如HTTP）可能允许返回一个特殊的响应，其它情况下，唯一的选择可能就是关闭对应的连接。

==== 抽象的编解码器类
在同一个类中管理入站和出站数据和消息的转换是有用的。Netty的抽象编解码器类正好用于这个目的，它们每
个都将捆绑一个解码器/编码器对。
然而，为什么并没有优先于单独的解码器和编码器使用这些复合类呢？这是因为尽可能地将这两种功能分开，最大化代码的可重用性和可扩展性。

ByteToMessageCodec:
https://netty.io/4.1/api/io/netty/handler/codec/ByteToMessageDecoder.html

MessageToMessageCodec:
https://netty.io/4.1/api/io/netty/handler/codec/MessageToMessageCodec.html

CombinedChannelDuplexHandler:
不会牺牲将一个解码器和一个编码器作为一个单独的单元部署所带来的便利性
https://netty.io/4.1/api/io/netty/channel/CombinedChannelDuplexHandler.html

==== 实现
https://github.com/netty/netty/tree/4.1/codec/src/main/java/io/netty/handler/codec

=== 内置的ChannelHandler与编解码器
==== SSL/TLS
SslHandler

==== HTTP/HTTPS
HttpRequestEncoder
HttpResponseEncoder
HttpRequestDecoder
HttpResponseDecoder
HttpClientCodec
HttpServerCodec
HttpContentCompressor

FullHttpRequest:
https://netty.io/4.1/api/io/netty/handler/codec/http/FullHttpRequest.html
FullHttpResponse:
https://netty.io/4.1/api/io/netty/handler/codec/http/FullHttpResponse.html

==== WebSocket
WebSocketServerProtocolHandler

BinaryWebSocketFrame 包含了二进制数据
TextWebSocketFrame 包含了文本数据
ContinuationWebSocketFrame 包含属于上一个BinaryWebSocketFrame或TextWebSocketFrame的文本数据或者二进制数据
CloseWebSocketFrame 表示一个 CLOSE 请求，包含一个关闭的状态码和关闭的原因
PingWebSocketFrame 请求传输一个 PongWebSocketFrame
PongWebSocketFrame 作为一个对于 PingWebSocketFrame 的响应被发送

==== 空闲的连接和超时
IdleStateHandler:
当连接空闲时间太长时，将会触发一个IdleStateEvent事件。
可以通过ChannelInboundHandler中重写userEventTriggered()方法来处理该IdleStateEvent事件。

ReadTimeoutHandler:
在指定的时间间隔内没有收到任何的入站数据，则抛出一个ReadTimeoutException并关闭对应的Channel。
可以通过重写ChannelHandler中的exceptionCaught()方法来检测该ReadTimeoutException。

WriteTimeoutHandler:
在指定的时间间隔内没有任何出站数据写入，则抛出一个WriteTimeoutException并关闭对应的Channel。
可以通过重写ChannelHandler中的exceptionCaught()方法检测该WriteTimeoutException。

==== 基于分隔符与长度的协议
DelimiterBasedFrameDecoder
LineBasedFrameDecoder

FixedLengthFrameDecoder

LengthFieldBasedFrameDecoder:
解决TCP拆包/粘包问题最常用的解码器，它基本上可以覆盖大部分基于长度的拆包场景。
LengthFieldBasedFrameDecoder比FixedLengthFrameDecoder和DelimiterBasedFrameDecoder要稍微复杂一点，但是功能比较强大。
https://netty.io/4.1/api/io/netty/handler/codec/LengthFieldBasedFrameDecoder.html

==== 写大型数据
ChunkedWriteHandler

==== 序列化数据
===== JDK序列化
CompatibleObjectDecoder
CompatibleObjectEncoder
ObjectDecoder
ObjectEncoder

===== Protobuf
ProtobufDecoder与ProtobufEncoder
https://netty.io/4.1/api/io/netty/handler/codec/protobuf/ProtobufDecoder.html
https://netty.io/4.1/api/io/netty/handler/codec/protobuf/ProtobufEncoder.html

=== writeAndFlush()
[source, java]
.https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPipeline.java
----
    @Override
    public final ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {
        return tail.writeAndFlush(msg, promise);
    }

    @Override
    public final ChannelFuture writeAndFlush(Object msg) {
        return tail.writeAndFlush(msg);
    }
----

tail.writeAndFlush调用了AbstractChannelHandlerContext上的writeAndFlush方法(
final class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler):
[source, java]
.https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/AbstractChannelHandlerContext.java
----
    @Override
    public ChannelFuture write(Object msg) {
        return write(msg, newPromise());
    }

    @Override
    public ChannelFuture write(final Object msg, final ChannelPromise promise) {
        write(msg, false, promise);

        return promise;
    }

    void invokeWrite(Object msg, ChannelPromise promise) {
        if (invokeHandler()) {
            invokeWrite0(msg, promise);
        } else {
            write(msg, promise);
        }
    }

    private void invokeWrite0(Object msg, ChannelPromise promise) {
        try {
            // DON'T CHANGE
            // Duplex handlers implements both out/in interfaces causing a scalability issue
            // see https://bugs.openjdk.org/browse/JDK-8180450
            final ChannelHandler handler = handler();
            final DefaultChannelPipeline.HeadContext headContext = pipeline.head;
            if (handler === headContext) {
                headContext.write(this, msg, promise);
            } else if (handler instanceof ChannelDuplexHandler) {
                ((ChannelDuplexHandler) handler).write(this, msg, promise);
            } else if (handler instanceof ChannelOutboundHandlerAdapter) {
                ((ChannelOutboundHandlerAdapter) handler).write(this, msg, promise);
            } else {
                ((ChannelOutboundHandler) handler).write(this, msg, promise);
            }
        } catch (Throwable t) {
            notifyOutboundHandlerException(t, promise);
        }
    }

    @Override
    public ChannelHandlerContext flush() {
        final AbstractChannelHandlerContext next = findContextOutbound(MASK_FLUSH);
        EventExecutor executor = next.executor();
        if (executor.inEventLoop()) {
            next.invokeFlush();
        } else {
            Tasks tasks = next.invokeTasks;
            if (tasks === null) {
                next.invokeTasks = tasks = new Tasks(next);
            }
            safeExecute(executor, tasks.invokeFlushTask, channel().voidPromise(), null, false);
        }

        return this;
    }

    private void invokeFlush() {
        if (invokeHandler()) {
            invokeFlush0();
        } else {
            flush();
        }
    }

    private void invokeFlush0() {
        try {
            // DON'T CHANGE
            // Duplex handlers implements both out/in interfaces causing a scalability issue
            // see https://bugs.openjdk.org/browse/JDK-8180450
            final ChannelHandler handler = handler();
            final DefaultChannelPipeline.HeadContext headContext = pipeline.head;
            if (handler === headContext) {
                headContext.flush(this);
            } else if (handler instanceof ChannelDuplexHandler) {
                ((ChannelDuplexHandler) handler).flush(this);
            } else if (handler instanceof ChannelOutboundHandlerAdapter) {
                ((ChannelOutboundHandlerAdapter) handler).flush(this);
            } else {
                ((ChannelOutboundHandler) handler).flush(this);
            }
        } catch (Throwable t) {
            invokeExceptionCaught(t);
        }
    }

    @Override
    public ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {
        write(msg, true, promise);
        return promise;
    }

    void invokeWriteAndFlush(Object msg, ChannelPromise promise) {
        if (invokeHandler()) {
            invokeWrite0(msg, promise);
            invokeFlush0();
        } else {
            writeAndFlush(msg, promise);
        }
    }

    private void write(Object msg, boolean flush, ChannelPromise promise) {
        ObjectUtil.checkNotNull(msg, "msg");
        try {
            if (isNotValidPromise(promise, true)) {
                ReferenceCountUtil.release(msg);
                // cancelled
                return;
            }
        } catch (RuntimeException e) {
            ReferenceCountUtil.release(msg);
            throw e;
        }

        final AbstractChannelHandlerContext next = findContextOutbound(flush ?
                (MASK_WRITE | MASK_FLUSH) : MASK_WRITE);
        final Object m = pipeline.touch(msg, next);
        EventExecutor executor = next.executor();
        if (executor.inEventLoop()) {
            if (flush) {
                next.invokeWriteAndFlush(m, promise);
            } else {
                next.invokeWrite(m, promise);
            }
        } else {
            final WriteTask task = WriteTask.newInstance(next, m, promise, flush);
            if (!safeExecute(executor, task, promise, m, !flush)) {
                // We failed to submit the WriteTask. We need to cancel it so we decrement the pending bytes
                // and put it back in the Recycler for re-use later.
                //
                // See https://github.com/netty/netty/issues/8343.
                task.cancel();
            }
        }
    }

    @Override
    public ChannelFuture writeAndFlush(Object msg) {
        return writeAndFlush(msg, newPromise());
    }
----

== 内存管理
=== 堆外内存
=== ByteBuf
网络通信中的数据载体。

Netty的ByteBuf相比于JDK的ByteBuffer性能更高，易用性更好:

    容量可以按需动态扩展，类似于StringBuffer
    读写采用了不同的指针，读写模式可以随意切换，不需要调用flip方法
    通过内置的复合缓冲类型可以实现零拷贝
    支持引用计数
    支持缓存池

ByteBuf的使用模式:
堆缓冲区；直接缓冲区；复合缓冲区(CompositeByteBuf)

ByteBuf有多种实现类，每种都有不同的特性，可以划分为三个不同的维度:
Heap/Direct、Pooled/Unpooled和Unsafe/非Unsafe。

Heap/Direct就是堆内和堆外内存。
Heap指的是在JVM堆内分配，底层依赖的是字节数据；
Direct则是堆外内存，不受JVM限制，分配方式依赖JDK底层的ByteBuffer。

Pooled/Unpooled表示池化还是非池化内存。
Pooled是从预先分配好的内存中取出，使用完可以放回ByteBuf内存池，等待下一次分配。
Unpooled是直接调用系统API去申请内存，确保能够被JVM GC管理回收。

Unsafe/非Unsafe的区别在于操作方式是否安全。
Unsafe表示每次调用JDK的Unsafe对象操作物理内存，依赖offset + index的方式操作数据。
非Unsafe则不需要依赖JDK的Unsafe对象，直接通过数组下标的方式操作数据。

实现:
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/ByteBuf.java

=== 核心实现
- 在单线程或者多线程的场景下，如何高效地进行内存分配和回收？
- 如何减少内存碎片，提高内存的有效利用率？

netty内存管理借鉴了jemalloc。

==== PoolArena
PoolArena是内存管理的统筹者。

内部有一个PoolChunkList组成的链表，链表是按PoolChunkList所管理的使用率划分。

PoolArena在分配内存时会存在竞争的，PoolArena会通过synchronized来保证线程的安全。
Netty会分配多个PoolArena，让线程尽量使用不同的PoolArena，减少出现竞争的情况。

[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolArena.java
----
abstract class PoolArena<T> implements PoolArenaMetric {
    //...
    enum SizeClass {
        Small,
        Normal
    }

    final PooledByteBufAllocator parent;

    final PoolSubpage<T>[] smallSubpagePools;

    private final PoolChunkList<T> q050;
    private final PoolChunkList<T> q025;
    private final PoolChunkList<T> q000;
    private final PoolChunkList<T> qInit;
    private final PoolChunkList<T> q075;
    private final PoolChunkList<T> q100;
    //...
}
----

qInit: 内存使用率为 0 ~ 25% 的Chunk
q000:  内存使用率为 1 ~ 50% 的Chunk
q025:  内存使用率为 25% ~ 75% 的Chunk
q050:  内存使用率为 50% ~ 100% 的Chunk
q075:  内存使用率为 75% ~ 100% 的Chunk
q100:  内存使用率为 100% 的Chunk

六种类型的PoolChunkList除了qInit，它们之间都形成了双向链表。

qInit用于存储初始分配的PoolChunk，因为在第一次内存分配时，PoolChunkList中并没有可用的PoolChunk，所以需要新创建一个PoolChunk并添加到qInit列表中。qInit中的PoolChunk即使内存被完全释放也不会被回收，避免PoolChunk的重复初始化工作。

q000则用于存放内存使用率为1 ~ 50%的PoolChunk，q000中的PoolChunk内存被完全释放后，PoolChunk从链表中移除，对应分配的内存也会被回收。

在分配大于8K的内存时，其链表的访问顺序是q050->q025->q000->qInit->q075，遍历检查 PoolChunkList中是否有PoolChunk可以用于内存分配:
[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolArena.java
----
private void allocateNormal(PooledByteBuf<T> buf, int reqCapacity, int sizeIdx, PoolThreadCache threadCache) {
    assert lock.isHeldByCurrentThread();
    if (q050.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        q025.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        q000.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        qInit.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        q075.allocate(buf, reqCapacity, sizeIdx, threadCache)) {
        return;
    }

    // Add a new chunk.
    PoolChunk<T> c = newChunk(sizeClass.pageSize, sizeClass.nPSizes, sizeClass.pageShifts, sizeClass.chunkSize);
    boolean success = c.allocate(buf, reqCapacity, sizeIdx, threadCache);
    assert success;
    qInit.add(c);
}
----
为什么会优先选择q050，而不是从q000开始呢？
这是一个折中的选择，在频繁分配内存的场景下，如果从q000开始，会有大部分的PoolChunk面临频繁的创建和销毁，造成内存分配的性能降低。如果从q050开始，会使PoolChunk的使用率范围保持在中间水平，降低了PoolChunk被回收的概率，从而兼顾了性能。

==== PoolChunkList
[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolChunkList.java
----
final class PoolChunkList<T> implements PoolChunkListMetric {
    private static final Iterator<PoolChunkMetric> EMPTY_METRICS = Collections.<PoolChunkMetric>emptyList().iterator();
    private final PoolArena<T> arena;
    private final PoolChunkList<T> nextList;
    private final int minUsage;
    private final int maxUsage;
    private final int maxCapacity;
    private PoolChunk<T> head;
    private final int freeMinThreshold;
    private final int freeMaxThreshold;

    // This is only update once when create the linked like list of PoolChunkList in PoolArena constructor.
    private PoolChunkList<T> prevList;
    //...
}
----
PoolChunkList内部有一个PoolChunk组成的链表。通常一个PoolChunkList中的所有PoolChunk使用率(已分配内存/ChunkSize)都在相同的范围内。
每个PoolChunkList有自己的最小使用率或者最大使用率的范围，PoolChunkList与PoolChunkList之间又会形成链表，并且使用率范围小的PoolChunkList会在链表中更加靠前。
随着PoolChunk的内存分配和使用，其使用率发生变化后，PoolChunk会在PoolChunkList的链表中，前后调整，移动到合适范围的PoolChunkList内。
这样做的好处是，使用率的小的PoolChunk可以先被用于内存分配，从而维持PoolChunk的利用率都在一个较高的水平，避免内存浪费。

==== PoolChunk
PoolChunk可以理解为Page的集合。
Netty会使用伙伴算法将PoolChunk分配成2048个Page，最终形成一颗满二叉树，二叉树中所有子节点的内存都属于其父节点管理.
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolChunk.java

==== Page
Page只是一种抽象的概念，实际在Netty中Page所指的是PoolChunk所管理的子内存块，每个子内存块采用PoolSubpage表示。
PoolChunk所能管理的最小内存叫做Page，大小由PageSize(默认为8K)，即一次向PoolChunk申请的内存都要以Page为单位(一个或多个Page)。
当需要由PoolChunk分配内存时，PoolChunk会查看通过内部记录的信息找出满足此次内存分配的Page的位置，分配给使用者。

==== PoolSubpage
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolSubpage.java

==== PoolThreadCache
线程本地缓存，减少内存分配时的竞争
Netty中负责线程分配的组件有两个: PoolArena和PoolThreadCache。PoolArena是多个线程共享的，每个线程会固定绑定一个 PoolArena，PoolThreadCache则是每个线程私有的缓存空间。

https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolThreadCache.java

回收:
[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolThreadCache.java
----
void trim() {
    trim(smallSubPageDirectCaches);
    trim(normalDirectCaches);
    trim(smallSubPageHeapCaches);
    trim(normalHeapCaches);
}
----

==== 申请内存过程
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PooledByteBufAllocator.java

=== 轻量级对象池
==== 概念
Netty为了减少频繁new对象的性能损耗，引进了一个通用的对象池，即Recycler。

https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/Recycler.java

==== Stack
Stack是整个对象池的顶层数据结构，描述了整个对象池的构造，用于存储当前本线程回收的对象。
在多线程的场景下，Netty为了避免锁竞争问题，每个线程都会持有各自的对象池，内部通过FastThreadLocal来实现每个线程的私有化。FastThreadLocal可以理解为Java里的ThreadLocal。

==== WeakOrderQueue
WeakOrderQueue存储其它线程回收到当前线程所分配的对象，并且在合适的时机，Stack会从异线程的WeakOrderQueue中收割对象。例如，ThreadB回收到ThreadA所分配的内存时，就会被放到ThreadA的WeakOrderQueue当中。

==== Link
每个WeakOrderQueue中都包含一个Link链表，回收对象都会被存在Link链表中的节点上，每个Link节点默认存储16个对象，当每个Link节点存储满了会创建新的Link节点放入链表尾部。

==== DefaultHandle
DefaultHandle实例中保存了实际回收的对象，Stack和WeakOrderQueue都使用DefaultHandle存储回收的对象。
在Stack中包含一个elements数组，该数组保存的是DefaultHandle实例。
DefaultHandle中每个Link节点所存储的16个对象也是使用DefaultHandle表示的。

==== 应用
find . -name "*.java" | xargs grep -ns "ObjectPool.newPool"

https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/internal/ObjectPool.java

=== 零拷贝
Netty除了支持操作系统级别的零拷贝，更多提供了面向用户态的零拷贝特性，主要体现在:
堆外内存、CompositeByteBuf、Unpooled.wrappedBuffer、ByteBuf.slice 以及FileRegion。
以操作系统的角度来看，零拷贝是一个广义的概念，只要能够减少不必要的CPU拷贝，都可以理解为是零拷贝。

=== 内存泄漏
为了诊断潜在的资源泄漏问题，Netty提供了class ResourceLeakDetector检测内存泄露，将会产生如下类似的日志消息:
LEAK: ByteBuf.release() was not called before it's garbage-collected.

Netty目前定义了4种泄漏检测级别:
DISABLED 禁用泄漏检测
SIMPLE 使用1%的默认采样率检测并报告任何发现的泄露。默认级别，适合绝大部分的情况
ADVANCED 使用默认的采样率，报告所发现的任何的泄露以及对应的消息被访问的位置
PARANOID 类似于ADVANCED，但是将会对每次对消息的访问都进行采样。对性能将会有很大的影响，一般在调试阶段使用

泄露检测级别可以通过类似java -Dio.netty.leakDetectionLevel=ADVANCED来定义。

=== 参考
https://zhuanlan.zhihu.com/p/422289486
https://zhuanlan.zhihu.com/p/259819465

== 流程

=== 服务器启动流程

=== 网络请求处理流程

== 优化
=== FastThreadLocal
https://netty.io/4.1/api/io/netty/util/concurrent/FastThreadLocal.html
https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/concurrent/FastThreadLocal.java

- 查找高效
FastThreadLocal在定位数据的时候可以直接根据数组下标index获取，时间复杂度O(1)。而JDK原生的ThreadLocal在数据较多时哈希表很容易发生Hash冲突，线性探测法在解决Hash冲突时需要不停地向下寻找，效率较低。此外，FastThreadLocal相比ThreadLocal数据扩容更加简单高效，FastThreadLocal以index为基准向上取整到2的次幂作为扩容后容量，然后把原数据拷贝到新数组。而ThreadLocal由于采用的哈希表，在扩容后需要再做一轮rehash。

- 安全性更高
JDK原生的ThreadLocal使用不当可能造成内存泄漏，只能等待线程销毁。在使用线程池的场景下，ThreadLocal只能通过主动检测的方式防止内存泄漏，从而造成了一定的开销。然而FastThreadLocal不仅提供了remove()主动清除对象的方法，而且在线程池场景中还封装了FastThreadLocalRunnable，FastThreadLocalRunnable最后会执行FastThreadLocal.removeAll()将Set集合中所有的FastThreadLocal对象都清理掉。

=== HashedWheelTimer
https://netty.io/4.1/api/io/netty/util/HashedWheelTimer.html
https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/HashedWheelTimer.java

HashedWheelTimer存在的问题:
如果长时间没有到期任务，那么会存在时间轮空推进的现象
只适用于处理耗时较短的任务，由于Worker是单线程的，如果一个任务执行的时间过长，会造成Worker线程阻塞
相比传统定时器的实现方式，内存占用较大

=== Mpsc Queue
https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/internal/PlatformDependent.java
引用了org.jctools.queues包。(主要是mpsc多生产者单消费者与spsc单生产者单消费者)

JCTools: https://github.com/JCTools/JCTools。
JCTools是适用于JVM并发开发的工具，主要提供了一些JDK缺失的并发数据结构，例如非阻塞Map、非阻塞Queue等。

=== 线程绑定
绑定线程到某个固定的CPU。

https://github.com/OpenHFT/Java-Thread-Affinity
该类库可以和Netty轻松集成，常用的方式是创建一个AffinityThreadFactory，然后传递给EventLoopGroup，AffinityThreadFactory负责创建Worker线程并完成绑核:

    EventLoopGroup bossGroup = new NioEventLoopGroup(1);
    ThreadFactory threadFactory = new AffinityThreadFactory("worker", AffinityStrategies.DIFFERENT_CORE);
    EventLoopGroup workerGroup = new NioEventLoopGroup(4, threadFactory);
    ServerBootstrap serverBootstrap = new ServerBootstrap().group(bossGroup, workerGroup);

=== 参考
https://www.infoq.cn/article/netty-million-level-push-service-design-points

== 示例
https://github.com/netty/netty/tree/4.1/example
https://github.com/Azure/DotNetty/tree/dev/examples

== 参考
https://netty.io/
https://netty.io/wiki/index.html
https://github.com/netty/netty
c#: https://github.com/Azure/DotNetty
《Netty IN ACTION》
