= netty
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:homepage: http://orientye.com
<<<

== 概览

=== 特点
参考: https://netty.io/index.html

netty5.0已经废弃:
主要原因: The major change of using a ForkJoinPool increases complexity and has not demonstrated a clear performance benefit. Also keeping all the branches in sync is quite some work without a real need for it as there is nothin in current master which I think justifies a new major release.
参考: https://github.com/netty/netty/issues/4466

说明: 本文基于netty最新稳定版本4.1。

=== java nio
since 1.4, the new non-blocking API
https://docs.oracle.com/javase/8/docs/api/java/nio/package-summary.html

- Buffer
- Channel
- Selector

nio2:
since java7
主要变化:
1. 加入了一种异步IO模式，利用事件回调机制(CompletionHandler等接口)，处理Accept、Read等操作，例如AsynchronousServerSocketChannel(对应ServerSocketChannel)及AsynchronousSocketChannel(对应SocketChannel)。
2. introduced better file management(java.nio.file package).
参考: https://stackoverflow.com/questions/25537675/java-what-exactly-is-the-difference-between-nio-and-nio-2

== 线程模型
=== 类型
- 单线程模型

    EventLoopGroup只包含一个EventLoop
    Boss和Worker使用同一个EventLoopGroup

- 多线程模型

    EventLoopGroup包含多个EventLoop
    Boss和Worker使用同一个EventLoopGroup

- 主从多线程模型

    EventLoopGroup包含多个EventLoop
    Boss是主Reactor，Worker是从Reactor，分别使用不同的EventLoopGroup
    主Reactor负责新的网络连接Channel的创建，然后把Channel注册到从Reactor

=== reactor单线程
- 所有IO操作在同一个NIO线程上完成

[source, java]
----
EventLoopGroup bossGroup = new NioEventLoopGroup(1);
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup)
 .channel(NioServerSocketChannel.class)
 ...
----

=== reactor多线程
- 一个连接只对应一个NIO线程

[source, java]
----
EventLoopGroup bossGroup = new NioEventLoopGroup(4);
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup)
 .channel(NioServerSocketChannel.class)
 ...
----
如果NioEventLoopGroup()构造时没有设置线程数目，则线程数是CPU核数的2倍?

=== 主从reactor多线程
- 主reactor线程池: acceptor线程池
- 从reactor线程池(sub reactor线程池)
- acceptor线程池仅用于客户端的登录，握手和安全认证，一旦连接建立成功，便将链路注册到subreactor线程池的IO线程上，由IO线程负责后续的IO操作

[source, java]
----
EventLoopGroup bossGroup = new NioEventLoopGroup(4);
EventLoopGroup workerGroup = new NioEventLoopGroup();
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup, workerGroup)
 .channel(NioServerSocketChannel.class)
 ...
----

=== Netty线程最佳实践
- 时间可控的简单业务直接在IO线程上处理
    如果业务非常简单，执行时间非常短，不需要与外部交互、访问数据库和磁盘，不需要等待其它资源，则建议直接在业务ChannelHandler中执行，不需要再启业务的线程或者线程池。避免线程上下文切换，也不存在线程并发问题。

- 复杂和时间不可控业务建议投递到后端业务线程池统一处理
    对于此类业务，不建议直接在业务ChannelHandler中启动线程或者线程池处理，建议将不同的业务统一封装成Task，统一投递到后端的业务线程池中进行处理。
    过多的业务ChannelHandler会带来开发效率和可维护性问题，不要把Netty当作业务容器，对于大多数复杂的业务产品，仍然需要集成或者开发自己的业务容器，做好和Netty的架构分层。

- 业务线程避免直接操作ChannelHandler
    对于ChannelHandler，IO线程和业务线程都可能会操作，因为业务通常是多线程模型，这样就会存在多线程操作ChannelHandler。为了尽量避免多线程并发问题，建议按照Netty自身的做法，通过将操作封装成独立的Task由NioEventLoop统一执行，而不是业务线程直接操作。

参考: https://www.infoq.cn/article/netty-threading-model

=== 参考
https://gee.cs.oswego.edu/dl/cpjslides/nio.pdf[Doug Lea 《scalable io in java》]

== 核心组件
=== BootStrap与ServerBootStrap
引导器主要负责整个Netty程序的启动、初始化、服务器连接等过程，相当于一条主线，串联了Netty的其他核心组件。

引导器共分为两种类型:
用于客户端引导的Bootstrap，用于服务端引导 ServerBootStrap，均继承自抽象类AbstractBootstrap。

=== EventLoop和EventLoopGroup
==== 概念
- EventLoop与EventLoopGroup:
    NioEventLoopGroup实际上是个线程池
    一个EventLoopGroup包含一个或者多个EventLoop

- EventLoop与Thread:
    一个EventLoop在它的生命周期内只有一个Thread绑定
    EnventLoop处理的I/O事件都将在它专有的Thread上进行

- EventLoop与Channel:
    一个Channel在它的生命周期内只注册于一个EventLoop
    每个EventLoop负责处理一个或多个Channel

也就是说，一个TCP连接是与一个固定的线程绑定的。

==== EventLoopGroup
class NioEventLoopGroup:
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/nio/NioEventLoopGroup.java

BossEventLoopGroup与WorkerEventLoopGroup包含一个或者多个NioEventLoop。

BossEventLoopGroup负责监听客户端的Accept事件，当事件触发时，将事件注册至WorkerEventLoopGroup中的一个NioEventLoop上。每新建一个Channel，只选择一个NioEventLoop与其绑定，因此Channel生命周期的所有事件处理都是线程独立的，不同的NioEventLoop线程之间不会发生任何交集。

NioEventLoop完成数据读取后，会调用绑定的ChannelPipeline进行事件传播，ChannelPipeline也是线程安全的，数据会被传递到ChannelPipeline的第一个ChannelHandler中。数据处理完成后，将加工完成的数据再传递给下一个ChannelHandler，整个过程是串行化执行，不会发生线程上下文切换的问题。

==== NioEventLoop
===== run()
核心方法: run()
[source, java]
.https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/nio/NioEventLoop.java
----
@Override
    protected void run() {
        int selectCnt = 0;
        for (;;) {
            try {
                int strategy;
                try {
                    strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());
                    switch (strategy) {
                    case SelectStrategy.CONTINUE:
                        continue;

                    case SelectStrategy.BUSY_WAIT:
                        // fall-through to SELECT since the busy-wait is not supported with NIO

                    case SelectStrategy.SELECT:
                        long curDeadlineNanos = nextScheduledTaskDeadlineNanos();
                        if (curDeadlineNanos === -1L) {
                            curDeadlineNanos = NONE; // nothing on the calendar
                        }
                        nextWakeupNanos.set(curDeadlineNanos);
                        try {
                            if (!hasTasks()) {
                                strategy = select(curDeadlineNanos);
                            }
                        } finally {
                            // This update is just to help block unnecessary selector wakeups
                            // so use of lazySet is ok (no race condition)
                            nextWakeupNanos.lazySet(AWAKE);
                        }
                        // fall through
                    default:
                    }
                } catch (IOException e) {
                    // If we receive an IOException here its because the Selector is messed up. Let's rebuild
                    // the selector and retry. https://github.com/netty/netty/issues/8566
                    rebuildSelector0();
                    selectCnt = 0;
                    handleLoopException(e);
                    continue;
                }

                selectCnt++;
                cancelledKeys = 0;
                needsToSelectAgain = false;
                final int ioRatio = this.ioRatio;
                boolean ranTasks;
                if (ioRatio === 100) {
                    try {
                        if (strategy > 0) {
                            processSelectedKeys();
                        }
                    } finally {
                        // Ensure we always run tasks.
                        ranTasks = runAllTasks();
                    }
                } else if (strategy > 0) {
                    final long ioStartTime = System.nanoTime();
                    try {
                        processSelectedKeys();
                    } finally {
                        // Ensure we always run tasks.
                        final long ioTime = System.nanoTime() - ioStartTime;
                        ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio);
                    }
                } else {
                    ranTasks = runAllTasks(0); // This will run the minimum number of tasks
                }

                if (ranTasks || strategy > 0) {
                    if (selectCnt > MIN_PREMATURE_SELECTOR_RETURNS && logger.isDebugEnabled()) {
                        logger.debug("Selector.select() returned prematurely {} times in a row for Selector {}.",
                                selectCnt - 1, selector);
                    }
                    selectCnt = 0;
                } else if (unexpectedSelectorWakeup(selectCnt)) { // Unexpected wakeup (unusual case)
                    selectCnt = 0;
                }
            } catch (CancelledKeyException e) {
                // Harmless exception - log anyway
                if (logger.isDebugEnabled()) {
                    logger.debug(CancelledKeyException.class.getSimpleName() + " raised by a Selector {} - JDK bug?",
                            selector, e);
                }
            } catch (Error e) {
                throw e;
            } catch (Throwable t) {
                handleLoopException(t);
            } finally {
                // Always handle shutdown even if the loop processing threw an exception.
                try {
                    if (isShuttingDown()) {
                        closeAll();
                        if (confirmShutdown()) {
                            return;
                        }
                    }
                } catch (Error e) {
                    throw e;
                } catch (Throwable t) {
                    handleLoopException(t);
                }
            }
        }
    }
----

NioEventLoop每次循环的处理流程都包含事件轮询select、事件处理processSelectedKeys、任务处理runAllTasks几个步骤，并且提供了一个参数ioRatio，可以调整I/O事件处理和任务处理的时间比例。

===== select()与JDK空轮询Bug
https://solthx.github.io/2020/10/15/JDK%E7%A9%BA%E8%BD%AE%E8%AF%A2Bug%E5%8F%8A%E5%87%BA%E7%8E%B0%E5%8E%9F%E5%9B%A0/

===== 任务处理
NioEventLoop不仅负责处理I/O 事件，还兼顾执行任务队列中的任务。

任务队列遵循FIFO规则，可以保证任务执行的公平性。

NioEventLoop处理的任务类型基本可以分为三类:

- 普通任务
通过NioEventLoop的execute()向任务队列taskQueue中添加的任务。
例如Netty在写数据时会封装WriteAndFlushTask提交给taskQueue。
taskQueue的实现类是多生产者单消费者队列MpscChunkedArrayQueue，在多线程并发添加任务时，可以保证线程安全。

- 定时任务
通过调用NioEventLoop的schedule()向定时任务队列scheduledTaskQueue添加一个定时任务，用于周期性执行该任务。
例如心跳消息发送等。
定时任务队列scheduledTaskQueue采用优先队列PriorityQueue实现。

- 尾部任务
tailTasks相比于普通任务队列优先级较低，在每次执行完taskQueue中任务后会去获取尾部队列中任务执行。尾部任务并不常用，主要用于做一些收尾工作，例如统计事件循环的执行时间、监控信息上报等。

[source, java]
.https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/concurrent/SingleThreadEventExecutor.java
----
protected boolean runAllTasks() {
    assert inEventLoop();
    boolean fetchedAll;
    boolean ranAtLeastOne = false;

    do {
        fetchedAll = fetchFromScheduledTaskQueue();
        if (runAllTasksFrom(taskQueue)) {
            ranAtLeastOne = true;
        }
    } while (!fetchedAll); // keep on processing until we fetched all scheduled tasks.

    if (ranAtLeastOne) {
        lastExecutionTime = getCurrentTimeNanos();
    }
    afterRunningAllTasks();
    return ranAtLeastOne;
}
----

===== EventLoop最佳实践
- 网络连接建立过程中三次握手、安全认证的过程会消耗不少时间。建议采用Boss和Worker两个EventLoopGroup，有助于分担Reactor线程的压力。

- 由于Reactor线程模式适合处理耗时短的任务场景，对于耗时较长的ChannelHandler可以考虑维护一个业务线程池，将编解码后的数据封装成Task进行异步处理，避免ChannelHandler阻塞而造成EventLoop不可用。

- 如果业务逻辑执行时间较短，建议直接在ChannelHandler中执行。例如编解码操作，这样可以避免过度设计而造成架构的复杂性。

- 不宜设计过多的ChannelHandler。对于系统性能和可维护性都会存在问题，在设计业务架构的时候，需要明确业务分层和Netty分层之间的界限。不要一味地将业务逻辑都添加到ChannelHandler中。

=== Channel
Channel(通道)是网络通信的载体。
Channel提供了基本的API用于网络I/O操作，例如register、bind、connect、read、write、flush等。
Netty的Channel以JDK NIO Channel为基础，相比较于JDK NIO，提供了更高层次的抽象，屏蔽了底层Socket的复杂性，赋予了Channel更强大的功能。

=== ChannelPipeline
ChannelPipeline是Netty的核心编排组件，负责组装各种ChannelHandler。
ChannelPipeline通过双向链表将不同的ChannelHandler链接在一起。
当读写事件触发时，ChannelPipeline会依次调用ChannelHandler列表对Channel的数据进行拦截和处理。

ChannelPipeline是线程安全的。
每一个新的Channel都会对应绑定一个新的ChannelPipeline。
一个ChannelPipeline关联一个EventLoop。

客户端和服务端都有各自的ChannelPipeline。

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelPipeline.java
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPipeline.java

ChannelPipeline的双向链表分别维护了HeadContext和TailContext的头尾节点。
程序自定义的ChannelHandler会插入到Head和Tail之间，即HeadContext与TailContext。

HeadContext是Inbound处理器，也是Outbound处理器。
实现了ChannelInboundHandler和ChannelOutboundHandler。
网络数据写入操作的入口就是由HeadContext节点完成的。
HeadContext作为Pipeline的头结点负责读取数据并开始传递InBound事件，当数据处理完成后，数据会反方向经过Outbound处理器，最终传递到HeadContext，因此HeadContext又是处理Outbound事件的最后一站。此外HeadContext在传递事件之前，还会执行一些前置操作。

TailContext只实现了ChannelInboundHandler接口。
它会在ChannelInboundHandler调用链路的最后一步执行，主要用于终止Inbound 件传播，例如释放Message数据资源等。TailContext节点作为OutBound事件传播的第一站，仅仅是将OutBound事件传递给上一个节点。

HeadContext与TailContext:
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPipeline.java


从整个ChannelPipeline调用链路来看，如果由Channel直接触发事件传播，那么调用链路将贯穿整个ChannelPipeline。然而也可以在其中某一个ChannelHandlerContext触发同样的方法，这样只会从当前的ChannelHandler开始执行事件传播，该过程不会从头贯穿到尾，在一定场景下，可以提高程序性能。

=== ChannelHandler
==== 概念
数据的编解码工作以及其他转换工作都是通过ChannelHandler处理的。
一般来说，开发者最关注的是ChannelHandler(很少会直接操作Channel，都是通过ChannelHandler间接完成)。

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelHandler.java

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelInboundHandler.java
https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelOutboundHandler.java

==== ChannelHandlerContext
每创建一个Channel都会绑定一个新的ChannelPipeline，ChannelPipeline中每加入一个ChannelHandler都会绑定一个ChannelHandlerContext。

ChannelHandlerContext用于保存ChannelHandler上下文，ChannelHandlerContext可以实现ChannelHandler之间的交互，ChannelHandlerContext包含了ChannelHandler生命周期的所有事件，如connect、bind、read、flush、write、close等。

https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/ChannelHandlerContext.java

==== 事件传播
Inbound事件和Outbound事件的传播方向是不一样的。
Inbound事件的传播方向为Head -> Tail，而Outbound事件传播方向是Tail -> Head，两者相反。
推荐在系统设计时模拟客户端和服务端的场景画出ChannelPipeline的内部结构图，以避免搞混调用关系。

==== 异常处理
异常事件的处理顺序与ChannelHandler的添加顺序相同，会依次向后传播，与Inbound事件和Outbound事件无关。

Netty中TailContext提供了兜底的异常处理逻辑，但是在很多场景下，不一定能满足应用的需求。

异常处理的最佳实践:
推荐对异常进行统一拦截(在ChannelPipeline自定义处理器的末端添加统一的异常处理器)，然后根据场景实现更加完善的异常处理机制。
示例代码:
[source, java]
----
public class ExceptionHandler extends ChannelDuplexHandler {
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        if (cause instanceof RuntimeException) {
            System.out.println("Handle Business Exception Success.");
        }
    }
}
----

=== 编解码器
编解码器可以分为一次解码器和二次解码器:
一次解码器用于解决TCP拆包/粘包问题，按协议解析后得到字节数据。如果需要对解析后的字节数据做对象模型的转换，便需要用到二次解码器。编码器同理。

一次编解码器: MessageToByteEncoder与ByteToMessageDecoder
二次编解码器: MessageToMessageEncoder与MessageToMessageDecoder

长度域解码器LengthFieldBasedFrameDecoder是解决TCP拆包/粘包问题最常用的解码器。它基本上可以覆盖大部分基于长度拆包场景。LengthFieldBasedFrameDecoder比FixedLengthFrameDecoder和DelimiterBasedFrameDecoder要稍微复杂一点，但是功能比较强大。

Protobuf编解码:
ProtobufDecoder与ProtobufEncoder

实现:
https://github.com/netty/netty/tree/4.1/codec/src/main/java/io/netty/handler/codec

=== writeAndFlush()
[source, java]
.https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPipeline.java
----
    @Override
    public final ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {
        return tail.writeAndFlush(msg, promise);
    }

    @Override
    public final ChannelFuture writeAndFlush(Object msg) {
        return tail.writeAndFlush(msg);
    }
----

tail.writeAndFlush调用了AbstractChannelHandlerContext上的writeAndFlush方法(
final class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler):
[source, java]
.https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/AbstractChannelHandlerContext.java
----
    @Override
    public ChannelFuture write(Object msg) {
        return write(msg, newPromise());
    }

    @Override
    public ChannelFuture write(final Object msg, final ChannelPromise promise) {
        write(msg, false, promise);

        return promise;
    }

    void invokeWrite(Object msg, ChannelPromise promise) {
        if (invokeHandler()) {
            invokeWrite0(msg, promise);
        } else {
            write(msg, promise);
        }
    }

    private void invokeWrite0(Object msg, ChannelPromise promise) {
        try {
            // DON'T CHANGE
            // Duplex handlers implements both out/in interfaces causing a scalability issue
            // see https://bugs.openjdk.org/browse/JDK-8180450
            final ChannelHandler handler = handler();
            final DefaultChannelPipeline.HeadContext headContext = pipeline.head;
            if (handler === headContext) {
                headContext.write(this, msg, promise);
            } else if (handler instanceof ChannelDuplexHandler) {
                ((ChannelDuplexHandler) handler).write(this, msg, promise);
            } else if (handler instanceof ChannelOutboundHandlerAdapter) {
                ((ChannelOutboundHandlerAdapter) handler).write(this, msg, promise);
            } else {
                ((ChannelOutboundHandler) handler).write(this, msg, promise);
            }
        } catch (Throwable t) {
            notifyOutboundHandlerException(t, promise);
        }
    }

    @Override
    public ChannelHandlerContext flush() {
        final AbstractChannelHandlerContext next = findContextOutbound(MASK_FLUSH);
        EventExecutor executor = next.executor();
        if (executor.inEventLoop()) {
            next.invokeFlush();
        } else {
            Tasks tasks = next.invokeTasks;
            if (tasks === null) {
                next.invokeTasks = tasks = new Tasks(next);
            }
            safeExecute(executor, tasks.invokeFlushTask, channel().voidPromise(), null, false);
        }

        return this;
    }

    private void invokeFlush() {
        if (invokeHandler()) {
            invokeFlush0();
        } else {
            flush();
        }
    }

    private void invokeFlush0() {
        try {
            // DON'T CHANGE
            // Duplex handlers implements both out/in interfaces causing a scalability issue
            // see https://bugs.openjdk.org/browse/JDK-8180450
            final ChannelHandler handler = handler();
            final DefaultChannelPipeline.HeadContext headContext = pipeline.head;
            if (handler === headContext) {
                headContext.flush(this);
            } else if (handler instanceof ChannelDuplexHandler) {
                ((ChannelDuplexHandler) handler).flush(this);
            } else if (handler instanceof ChannelOutboundHandlerAdapter) {
                ((ChannelOutboundHandlerAdapter) handler).flush(this);
            } else {
                ((ChannelOutboundHandler) handler).flush(this);
            }
        } catch (Throwable t) {
            invokeExceptionCaught(t);
        }
    }

    @Override
    public ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {
        write(msg, true, promise);
        return promise;
    }

    void invokeWriteAndFlush(Object msg, ChannelPromise promise) {
        if (invokeHandler()) {
            invokeWrite0(msg, promise);
            invokeFlush0();
        } else {
            writeAndFlush(msg, promise);
        }
    }

    private void write(Object msg, boolean flush, ChannelPromise promise) {
        ObjectUtil.checkNotNull(msg, "msg");
        try {
            if (isNotValidPromise(promise, true)) {
                ReferenceCountUtil.release(msg);
                // cancelled
                return;
            }
        } catch (RuntimeException e) {
            ReferenceCountUtil.release(msg);
            throw e;
        }

        final AbstractChannelHandlerContext next = findContextOutbound(flush ?
                (MASK_WRITE | MASK_FLUSH) : MASK_WRITE);
        final Object m = pipeline.touch(msg, next);
        EventExecutor executor = next.executor();
        if (executor.inEventLoop()) {
            if (flush) {
                next.invokeWriteAndFlush(m, promise);
            } else {
                next.invokeWrite(m, promise);
            }
        } else {
            final WriteTask task = WriteTask.newInstance(next, m, promise, flush);
            if (!safeExecute(executor, task, promise, m, !flush)) {
                // We failed to submit the WriteTask. We need to cancel it so we decrement the pending bytes
                // and put it back in the Recycler for re-use later.
                //
                // See https://github.com/netty/netty/issues/8343.
                task.cancel();
            }
        }
    }

    @Override
    public ChannelFuture writeAndFlush(Object msg) {
        return writeAndFlush(msg, newPromise());
    }
----

== 内存管理
=== 堆外内存
=== ByteBuf
网络通信中的数据载体。

Netty的ByteBuf相比于JDK的ByteBuffer性能更高，易用性更好:

    容量可以按需动态扩展，类似于StringBuffer
    读写采用了不同的指针，读写模式可以随意切换，不需要调用flip方法
    通过内置的复合缓冲类型可以实现零拷贝
    支持引用计数
    支持缓存池

ByteBuf有多种实现类，每种都有不同的特性，可以划分为三个不同的维度:
Heap/Direct、Pooled/Unpooled和Unsafe/非 Unsafe。

Heap/Direct就是堆内和堆外内存。
Heap指的是在JVM堆内分配，底层依赖的是字节数据；
Direct则是堆外内存，不受JVM限制，分配方式依赖JDK底层的ByteBuffer。

Pooled/Unpooled表示池化还是非池化内存。
Pooled是从预先分配好的内存中取出，使用完可以放回ByteBuf内存池，等待下一次分配。
Unpooled是直接调用系统API去申请内存，确保能够被JVM GC管理回收。

Unsafe/非Unsafe的区别在于操作方式是否安全。
Unsafe表示每次调用JDK的Unsafe对象操作物理内存，依赖offset + index的方式操作数据。
非Unsafe则不需要依赖JDK的Unsafe对象，直接通过数组下标的方式操作数据。

实现:
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/ByteBuf.java

=== 核心实现
- 在单线程或者多线程的场景下，如何高效地进行内存分配和回收？
- 如何减少内存碎片，提高内存的有效利用率？

netty内存管理借鉴了jemalloc。

==== PoolArena
PoolArena是内存管理的统筹者。

内部有一个PoolChunkList组成的链表，链表是按PoolChunkList所管理的使用率划分。

PoolArena在分配内存时会存在竞争的，PoolArena会通过synchronized来保证线程的安全。
Netty会分配多个PoolArena，让线程尽量使用不同的PoolArena，减少出现竞争的情况。

[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolArena.java
----
abstract class PoolArena<T> implements PoolArenaMetric {
    //...
    enum SizeClass {
        Small,
        Normal
    }

    final PooledByteBufAllocator parent;

    final PoolSubpage<T>[] smallSubpagePools;

    private final PoolChunkList<T> q050;
    private final PoolChunkList<T> q025;
    private final PoolChunkList<T> q000;
    private final PoolChunkList<T> qInit;
    private final PoolChunkList<T> q075;
    private final PoolChunkList<T> q100;
    //...
}
----

qInit: 内存使用率为 0 ~ 25% 的Chunk
q000:  内存使用率为 1 ~ 50% 的Chunk
q025:  内存使用率为 25% ~ 75% 的Chunk
q050:  内存使用率为 50% ~ 100% 的Chunk
q075:  内存使用率为 75% ~ 100% 的Chunk
q100:  内存使用率为 100% 的Chunk

六种类型的PoolChunkList除了qInit，它们之间都形成了双向链表。

qInit用于存储初始分配的PoolChunk，因为在第一次内存分配时，PoolChunkList中并没有可用的PoolChunk，所以需要新创建一个PoolChunk并添加到qInit列表中。qInit中的PoolChunk即使内存被完全释放也不会被回收，避免PoolChunk的重复初始化工作。

q000则用于存放内存使用率为1 ~ 50%的PoolChunk，q000中的PoolChunk内存被完全释放后，PoolChunk从链表中移除，对应分配的内存也会被回收。

在分配大于8K的内存时，其链表的访问顺序是q050->q025->q000->qInit->q075，遍历检查 PoolChunkList中是否有PoolChunk可以用于内存分配:
[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolArena.java
----
private void allocateNormal(PooledByteBuf<T> buf, int reqCapacity, int sizeIdx, PoolThreadCache threadCache) {
    assert lock.isHeldByCurrentThread();
    if (q050.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        q025.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        q000.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        qInit.allocate(buf, reqCapacity, sizeIdx, threadCache) ||
        q075.allocate(buf, reqCapacity, sizeIdx, threadCache)) {
        return;
    }

    // Add a new chunk.
    PoolChunk<T> c = newChunk(sizeClass.pageSize, sizeClass.nPSizes, sizeClass.pageShifts, sizeClass.chunkSize);
    boolean success = c.allocate(buf, reqCapacity, sizeIdx, threadCache);
    assert success;
    qInit.add(c);
}
----
为什么会优先选择q050，而不是从q000开始呢？
这是一个折中的选择，在频繁分配内存的场景下，如果从q000开始，会有大部分的PoolChunk面临频繁的创建和销毁，造成内存分配的性能降低。如果从q050开始，会使PoolChunk的使用率范围保持在中间水平，降低了PoolChunk被回收的概率，从而兼顾了性能。

==== PoolChunkList
[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolChunkList.java
----
final class PoolChunkList<T> implements PoolChunkListMetric {
    private static final Iterator<PoolChunkMetric> EMPTY_METRICS = Collections.<PoolChunkMetric>emptyList().iterator();
    private final PoolArena<T> arena;
    private final PoolChunkList<T> nextList;
    private final int minUsage;
    private final int maxUsage;
    private final int maxCapacity;
    private PoolChunk<T> head;
    private final int freeMinThreshold;
    private final int freeMaxThreshold;

    // This is only update once when create the linked like list of PoolChunkList in PoolArena constructor.
    private PoolChunkList<T> prevList;
    //...
}
----
PoolChunkList内部有一个PoolChunk组成的链表。通常一个PoolChunkList中的所有PoolChunk使用率(已分配内存/ChunkSize)都在相同的范围内。
每个PoolChunkList有自己的最小使用率或者最大使用率的范围，PoolChunkList与PoolChunkList之间又会形成链表，并且使用率范围小的PoolChunkList会在链表中更加靠前。
随着PoolChunk的内存分配和使用，其使用率发生变化后，PoolChunk会在PoolChunkList的链表中，前后调整，移动到合适范围的PoolChunkList内。
这样做的好处是，使用率的小的PoolChunk可以先被用于内存分配，从而维持PoolChunk的利用率都在一个较高的水平，避免内存浪费。

==== PoolChunk
PoolChunk可以理解为Page的集合。
Netty会使用伙伴算法将PoolChunk分配成2048个Page，最终形成一颗满二叉树，二叉树中所有子节点的内存都属于其父节点管理.
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolChunk.java

==== Page
Page只是一种抽象的概念，实际在Netty中Page所指的是PoolChunk所管理的子内存块，每个子内存块采用PoolSubpage表示。
PoolChunk所能管理的最小内存叫做Page，大小由PageSize(默认为8K)，即一次向PoolChunk申请的内存都要以Page为单位(一个或多个Page)。
当需要由PoolChunk分配内存时，PoolChunk会查看通过内部记录的信息找出满足此次内存分配的Page的位置，分配给使用者。

==== PoolSubpage
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolSubpage.java

==== PoolThreadCache
线程本地缓存，减少内存分配时的竞争
Netty中负责线程分配的组件有两个: PoolArena和PoolThreadCache。PoolArena是多个线程共享的，每个线程会固定绑定一个 PoolArena，PoolThreadCache则是每个线程私有的缓存空间。

https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolThreadCache.java

回收:
[source, java]
.https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PoolThreadCache.java
----
void trim() {
    trim(smallSubPageDirectCaches);
    trim(normalDirectCaches);
    trim(smallSubPageHeapCaches);
    trim(normalHeapCaches);
}
----

==== 申请内存过程
https://github.com/netty/netty/blob/4.1/buffer/src/main/java/io/netty/buffer/PooledByteBufAllocator.java

=== 轻量级对象池
==== 概念
Netty为了减少频繁new对象的性能损耗，引进了一个通用的对象池，即Recycler。

https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/Recycler.java

==== Stack
Stack是整个对象池的顶层数据结构，描述了整个对象池的构造，用于存储当前本线程回收的对象。
在多线程的场景下，Netty为了避免锁竞争问题，每个线程都会持有各自的对象池，内部通过FastThreadLocal来实现每个线程的私有化。FastThreadLocal可以理解为Java里的ThreadLocal。

==== WeakOrderQueue
WeakOrderQueue存储其它线程回收到当前线程所分配的对象，并且在合适的时机，Stack会从异线程的WeakOrderQueue中收割对象。例如，ThreadB回收到ThreadA所分配的内存时，就会被放到ThreadA的WeakOrderQueue当中。

==== Link
每个WeakOrderQueue中都包含一个Link链表，回收对象都会被存在Link链表中的节点上，每个Link节点默认存储16个对象，当每个Link节点存储满了会创建新的Link节点放入链表尾部。

==== DefaultHandle
DefaultHandle实例中保存了实际回收的对象，Stack和WeakOrderQueue都使用DefaultHandle存储回收的对象。
在Stack中包含一个elements数组，该数组保存的是DefaultHandle实例。
DefaultHandle中每个Link节点所存储的16个对象也是使用DefaultHandle表示的。

==== 应用
find . -name "*.java" | xargs grep -ns "ObjectPool.newPool"

https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/internal/ObjectPool.java

=== 零拷贝
Netty除了支持操作系统级别的零拷贝，更多提供了面向用户态的零拷贝特性，主要体现在:
堆外内存、CompositeByteBuf、Unpooled.wrappedBuffer、ByteBuf.slice 以及FileRegion。
以操作系统的角度来看，零拷贝是一个广义的概念，只要能够减少不必要的CPU拷贝，都可以理解为是零拷贝。

=== 参考
https://zhuanlan.zhihu.com/p/422289486
https://zhuanlan.zhihu.com/p/259819465

== 流程

=== 服务器启动流程

=== 网络请求处理流程

== 优化
=== FastThreadLocal
https://netty.io/4.0/api/io/netty/util/concurrent/FastThreadLocal.html
https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/concurrent/FastThreadLocal.java

- 查找高效
FastThreadLocal在定位数据的时候可以直接根据数组下标index获取，时间复杂度 O(1)。而JDK原生的ThreadLocal在数据较多时哈希表很容易发生Hash冲突，线性探测法在解决Hash冲突时需要不停地向下寻找，效率较低。此外，FastThreadLocal相比ThreadLocal数据扩容更加简单高效，FastThreadLocal以index为基准向上取整到2的次幂作为扩容后容量，然后把原数据拷贝到新数组。而ThreadLocal由于采用的哈希表，在扩容后需要再做一轮rehash。

- 安全性更高
JDK原生的ThreadLocal使用不当可能造成内存泄漏，只能等待线程销毁。在使用线程池的场景下，ThreadLocal只能通过主动检测的方式防止内存泄漏，从而造成了一定的开销。然而FastThreadLocal不仅提供了remove()主动清除对象的方法，而且在线程池场景中Netty还封装了FastThreadLocalRunnable，FastThreadLocalRunnable最后会执行FastThreadLocal.removeAll()将Set集合中所有的FastThreadLocal对象都清理掉。

=== HashedWheelTimer
https://netty.io/4.0/api/io/netty/util/HashedWheelTimer.html
https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/HashedWheelTimer.java

HashedWheelTimer存在的问题:
如果长时间没有到期任务，那么会存在时间轮空推进的现象
只适用于处理耗时较短的任务，由于Worker是单线程的，如果一个任务执行的时间过长，会造成Worker线程阻塞
相比传统定时器的实现方式，内存占用较大

=== Mpsc Queue

=== 线程绑定
绑定线程到某个固定的CPU。

https://github.com/OpenHFT/Java-Thread-Affinity
该类库可以和Netty轻松集成，常用的方式是创建一个AffinityThreadFactory，然后传递给EventLoopGroup，AffinityThreadFactory负责创建Worker线程并完成绑核:

    EventLoopGroup bossGroup = new NioEventLoopGroup(1);
    ThreadFactory threadFactory = new AffinityThreadFactory("worker", AffinityStrategies.DIFFERENT_CORE);
    EventLoopGroup workerGroup = new NioEventLoopGroup(4, threadFactory);
    ServerBootstrap serverBootstrap = new ServerBootstrap().group(bossGroup, workerGroup);

=== 参考
https://www.infoq.cn/article/netty-million-level-push-service-design-points

== 参考
https://netty.io/
https://netty.io/wiki/index.html
https://github.com/netty/netty
