:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:hardbreaks-option:

== 一致性

=== 一致性模型
==== 强一致性模型
===== 线性一致性(Linearizable Consistency)/严格一致性(Strict Consistency)/原子一致性(Atomic Consistency)

    条件:
        1. 任何一次读都能读取到某个数据最近的一次写的数据。
        2. 所有进程看到的操作顺序都跟全局时钟下的顺序一致。

    应用场景:
        分布性锁、Leader 选举、唯一性约束等

- 可以从读写操作的维度来描述:
* 对于写操作来说:
    ** 任意两个写操作 x1 和 x2：如果写 x1 操作和写 x2 操作有重叠，那么可能 x1 覆盖 x2，也可能 x2 覆盖 x1；
    ** 如果写 x1 操作在写 x2 开始前完成，那么 x2 一定覆盖 x1。
* 对于读操作来说:
    ** 写操作完成后，所有的客户端都能立即观察到
    ** 对于多个客户端来说，必须读取到一样的顺序。

线性一致性保证了所有的读取都可以读到最新写入的值，即一旦新的值被写入或读取，所有后续的读都会看到写入的值，直到它被再次覆盖。在线性一致性模型中不论是数据的覆盖顺序还是读取顺序，都是按时间线从旧值向新值移动，而不会出现旧值反转的情况。

有一种说法是“在分布式系统中不可能实现”，通常源于对 “全局完全一致的物理时钟” 的假设。确实，由于网络延迟的不确定性和狭义相对论效应，无法获得完美同步的、无误差的全局物理时钟。

然而，线性一致性并不要求系统拥有完美的物理时钟！它要求的是外部观察者（客户端）感知到的执行顺序，必须与一个虚拟的、单调递增的时间线一致，这个时间线尊重真实时间的先后关系（Real-time Order）。实现线性一致性时，不需要在所有服务器上同步物理时钟，而是通过算法和协议来构建这个“虚拟时间线”。

===== 顺序一致性(Sequential Consistency)

    顺序一致性是Lamport在解决多处理器系统共享存储器时首次提出来的
    条件:
    1. 任何一次读写操作都是按照某种特定的顺序。
    2. 所有进程看到的读写操作顺序都保持一致。

==== 弱一致性模型
===== 因果一致性(Causal Consistency)
是一种弱化的顺序一致性模型
因果关系: 如果事件B是由事件A引起的或者受事件A的影响，那么这两个事件就具有因果关系。

举个分布式数据库的示例，假设进程P1对数据项x进行了写操作，然后进程P2先读取了x，然后对y进行了写操作，
那么对x的读操作和对 y的写操作就具有潜在的因果关系，因为y的计算可能依赖于P2读取到x的值(也就是P1写的值)。
另一方面，如果两个进程同时对两个不同的数据项进行写操作，那么这两个事件就不具备因果关系。

条件:
1. 所有进程必须以相同的顺序看到具有因果关系的读写操作。
2. 不同进程可以以不同的顺序看到并发的读写操作。

Q: 为什么说因果一致性是一种弱化的顺序一致性模型?
A: 顺序一致性虽然不保证事件发生的顺序与实际发生的保持一致，但是能保证所有进程看到的读写操作顺序是一样的。
而因果一致性更进一步弱化了顺序一致性中对读写操作顺序的约束，仅保证有因果关系的读写操作有序，没有因果关系的读写操作(并发事件)则不做保证。也就是说如果是无因果关系的数据操作不同进程看到的值是有可能是不一样，而有因果关系的数据操作不同进程看到的值保证是一样的。

===== 最终一致性(Eventual Consistency)
是更加弱化的一致性模型
因果一致性起码还保证了有因果关系的数据不同进程读取到的值保证是一样的，而最终一致性只保证所有副本的数据最终在某个时刻会保持一致。

1. "最终"到底是多久？通常来说，实际运行的系统需要能够保证提供一个有下限的时间范围。
2. 多副本之间对数据更新采用什么样的策略？一段时间内可能数据可能多次更新，到底以哪个数据为准？一个常用的策略以时间戳最新的数据为准。

由于最终一致性对数据一致性的要求比较低，在对性能要求高的场景中是经常使用的一致性模型。

===== 以客户端为中心的一致性(Client-centric Consistency)
前面讨论的一致性模型都是针对数据存储的多副本之间如何做到一致性，考虑这么一种场景: 在最终一致性的模型中，如果客户端在数据不同步的时间窗口内访问不同的副本的同一个数据，会出现读取同一个数据却得到不同的值的情况。为了解决这个问题，提出了以客户端为中心的一致性模型。

以客户端为中心的一致性为单一客户端提供一致性保证，保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证。

举个例子: 客户端A在副本M上读取x的最新值为1，假设副本M挂了，客户端A连接到副本N上，此时副本N上面的x值为旧版本的0，那么一致性模型会保证客户端A读取到的x的值为1，而不是旧版本的0。一种可行的方案就是给数据x加版本标记，同时客户端A会缓存x的值，通过比较版本来识别数据的新旧，保证客户端不会读取到旧的值。

以客户端为中心的一致性包含了四种子模型:

    1. 单调读一致性(Monotonic-read Consistency)

        如果一个进程读取数据项x的值，那么该进程对于x后续的所有读操作要么读取到第一次读取的值要么读取到更新的值。
        即保证客户端不会读取到旧值。

    2. 单调写一致性(Monotonic-write Consistency)

        一个进程对数据项x的写操作必须在该进程对x执行任何后续写操作之前完成。
        即保证客户端的写操作是串行的。

    3. 读写一致性(Read-your-writes Consistency)

        一个进程对数据项x执行一次写操作的结果总是会被该进程对x执行的后续读操作看见。
        即保证客户端能读到自己最新写入的值。

    4. 写读一致性(Writes-follow-reads Consistency)

        同一个进程对数据项x执行的读操作之后的写操作，保证发生在与x读取值相同或比之更新的值上。
        即保证客户端对一个数据项的写操作是基于该客户端最新读取的值。

参考: https://en.wikipedia.org/wiki/Consistency_model
参考: https://developer.aliyun.com/article/693187

=== 一致性与共识

    一致性(Consistency)通常指指分布式系统中多个副本对外呈现的数据的状态。
    共识(Consensus)则描述了分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。

    一致性描述的是结果状态，共识则是一种手段。
    达成某种共识并不意味着就保障了(强)一致性。只能说共识机制，能够实现某种程度上的一致性。

    一致性的实现，核心过程往往需要通过共识算法来达成。
    但整个系统的一致性不仅仅取决于共识算法，例如可能还依赖客户端策略。

    共识的应用:
    逻辑时间的共识，用于决定时间发生的顺序
    互斥性的共识，用于决定谁正拥有访问的资源
    协调者的共识，用于决定谁是当前的leader

=== 共识算法
==== 2PC
2PC: Two-Phase Commit

===== 过程
===== 特点
- 优点: 原理简单，实现方便
- 缺点: 阻塞，单点，数据不一致
- 应用: 数据库，JTA(Java Transaction API)等，Google Percolator

Q: 如何理解阻塞?
A: 参与者需要等待两个阶段都结束了，才会释放资源锁，这也是2PC性能较差的原因。

Q: 如何理解单点?
A: 事务管理器(协调者)存在单点。

Q: 如何理解数据不一致?
A: 协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，导致只有一部分参与者接受到了commit请求执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个系统便出现了数据不一致性的现象。

===== 解决方案
- XA方案

    2PC的传统方案是在数据库层面实现的，MySQL，Oracle等都支持2PC协议。
    为了统一标准，国际开放标准组织Open Group定义了分布式事务处理模型DTP(Distributed Transaction Processing Reference Model)。
    DTP模型定义TM(事务管理器)和RM(资源管理器，即参与者)之间的接口规范叫XA，可以简单的理解为数据库提供的2PC接口协议。
    基于数据库的XA协议来实现2PC称为XA方案。

- Seata方案

    http://seata.io/zh-cn/docs/dev/mode/xa-mode.html

- MySQL 2PC
http://mysql.taobao.org/monthly/2020/05/07/
https://dev.mysql.com/doc/refman/8.0/en/xa.html

- Google Percolator
http://mysql.taobao.org/monthly/2018/11/02/

- TiDB与Percolator
https://pingcap.com/zh/blog/percolator-and-txn

参考:
https://zhuanlan.zhihu.com/p/35616810
https://database.51cto.com/art/202101/640577.htm
https://www.cnblogs.com/hustcat/p/3577584.html

==== 3PC
3PC: Three-Phase Commit
优点: 降低了阻塞问题
缺点: 同样存在很多问题

==== PBFT
===== 概念
Practical Byzantine Fault Tolerance，即实用拜占庭容错算法。
分布式系统中有两种Fault: Crash Fault与Byzantine Fault

- 核心目标
在分布式系统中，多个节点需要就某个值或某个状态达成一致。但系统中可能存在一些“恶意节点”（即“拜占庭节点”），它们会任意行事：可能不发送消息、发送错误消息、对不同节点发送不同消息，企图破坏系统的一致性。
PBFT 的目标就是: 在存在不超过一定数量的拜占庭节点时，系统依然能够达成共识，保证所有诚实节点行动一致。

- 关键假设与前提条件
PBFT 不是万能的，它在一个特定的模型下工作：
** 节点类型: 系统中的节点分为两类：
    *** 诚实节点: 严格遵守协议规则的节点。
    *** 拜占庭节点: 行为任意的恶意节点，数量为 f。
** 容错能力: PBFT 可以容忍最多 f 个拜占庭节点。要保证系统正常运作，总的节点数 N 必须满足: N ≥ 3f + 1。
    *** 为什么是 3f+1？
        **** 要达成一致，诚实节点的数量必须超过总节点数的一半（N - f > f），否则恶意节点可以联合起来形成多数派，欺骗诚实节点。这推导出 N > 2f。
        **** 同时，在投票过程中，即使有 f 个节点无响应，系统仍需收到足够多的响应来继续前进。综合起来，最少的节点数就是 3f + 1。
** 网络环境: 假设是部分同步网络。消息最终会被送达，但可能会有延迟。
** 密码学安全: 使用非对称加密技术（如数字签名）进行身份认证，确保消息不会被篡改和伪造。

- 优缺点
** 优点:
*** 高性能: 与传统的工作量证明（PoW）等算法相比，它不需要消耗大量算力，交易确认速度快，吞吐量高。
*** 最终性: 一旦达成共识，结果就是最终的，不会像 PoW 那样存在回滚的可能。
*** 理论完备: 在满足 N ≥ 3f + 1 的条件下，能严格保证安全性和活性。
** 缺点:
*** 扩展性差: 由于需要所有节点之间进行多轮广播通信，通信复杂度为 O(N²)。当节点数量（N）非常大时（例如成千上万个），网络开销会变得巨大，性能急剧下降。这限制了它不适合完全开放、节点数量巨大的公链系统。
*** 复杂度高: 协议本身比较复杂，实现难度大，尤其是视图更换和日志垃圾回收等细节。

===== 核心工作流程（三阶段协议）

====== 基础设定

[grid="rows"，frame=all，options="header"]
|===
| **参数** | **说明**
| 节点总数 (N) | N ≥ 3f + 1
| 容错数量 (f) | 最多容忍 f 个拜占庭节点
| 节点角色 | 1个主节点 + (N-1)个备份节点
| 法定人数 (Quorum) | 2f + 1 个节点
|===

====== 1. 请求 (Request)

*客户端 → 主节点*

[source]
----
<REQUEST，o，t，c>_σ_C
o: 操作内容
t: 时间戳  
c: 客户端ID
σ_C: 客户端签名
----

***

====== 2. 预准备阶段 (Pre-Prepare)

*主节点 → 所有备份节点*

[source]
----
<PRE-PREPARE，v，n，d，m>_σ_P
v: 视图编号
n: 序列号
d: 请求m的哈希
m: 原始请求
σ_P: 主节点签名
----

**备份节点验证：**

* ✓ 签名有效
* ✓ 视图号正确  
* ✓ 序列号合法
* ✓ 首次见到 (v，n)

***

====== 3. 准备阶段 (Prepare)

*所有节点 ↔ 所有节点*

[source]
----
<PREPARE，v，n，d，i>_σ_i
i: 节点ID
σ_i: 节点签名
----

**进入 Prepared 状态的条件：**

收集到 **2f** 个匹配的 PREPARE 消息
+ (包括自己共 2f+1 个)

**目标：** 确保所有诚实节点对 *(v，n，d)* 达成一致

***

====== 4. 提交阶段 (Commit)  

*所有节点 ↔ 所有节点*

[source]
----
<COMMIT，v，n，d，i>_σ_i
----

**进入 Committed 状态的条件：**

收集到 **2f+1** 个有效的 COMMIT 消息

**目标：** 通知所有节点请求已最终确认

***

====== 5. 执行与回复

**执行：**
. 按序列号 n 顺序执行请求
. 更新本地状态

**回复客户端：**
. 节点发送：`<REPLY，v，t，c，i，r>_σ_i`
. 客户端等待 **f+1** 个相同结果

===== 关键检查点

.Checkpoints
[cols="1,3"，options="header"]
|===
| 阶段 | 成功条件
| Pre-Prepare | 消息验证通过
| Prepare | 收到 2f 个匹配的 PREPARE 消息
| Commit | 收到 2f+1 个有效的 COMMIT 消息  
| Client | 收到 f+1 个相同的 REPLY
|===

.Protocol Safety Guarantees
[cols="1,1,3"，options="header"]
|===
| 阶段 | 保证 | 目的
| Pre-Prepare | 请求排序 | 主节点分配序列号
| Prepare | 请求一致性 | 所有诚实节点认同同一请求
| Commit | 请求最终性 | 确保请求会被执行
|===

===== 视图更换(View Change)协议
如果主节点是恶意的或者宕机了，它可能不发送预准备消息，或者给不同请求分配相同的序列号，系统就会卡住。

PBFT 有一个视图更换机制来解决这个问题：

1. 如果备份节点在超时时间内没有收到主节点的合法消息，它可以怀疑主节点有问题。

2. 该节点会广播一条 视图更换请求，提议进入下一个视图（v+1）。

3. 当新的主节点（通常是按顺序的下一个节点，如 (v+1) mod N）收到 2f+1 个有效的视图更换请求时，它就会成为新的主节点，并启动新的视图。

4. 新的主节点会收集之前未完成的请求和状态，并通知其他节点同步到最新状态，然后继续处理请求。

这个机制确保了系统的活性，即使主节点失效，系统也能最终恢复。

===== 参考
https://cloud.tencent.com/developer/news/202455

==== Gossip
Gossip protocol也叫Epidemic Protocol(流行病协议)

Gossip过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，直至最终网络中所有的节点都收到了消息。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个最终一致性协议。

使用Gossip的典型系统:
bitcoin(bitcoin除了gossip还依赖pow共识)

- 优点:

    扩展性性好
        网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。
    容错性强
        网络中任何节点的宕机和重启都不会影响Gossip消息的传播，Gossip协议具有天然的分布式系统容错特性。
    去中心化
        Gossip协议不要求任何中心节点，所有节点都可以是对等的
        任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。
    一致性收敛
        Gossip协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播
        因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了logN
    简单
        实现Gossip十分简单

- 缺点:

    消息延迟
        节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网，不可避免的造成消息延迟。
    消息冗余
        节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤
        因此不可避免地引起同一节点多次接收同一消息，增加消息处理的压力。
        一次通信会对网路带宽、CUP资源造成很大的负载，而这些负载又受限于通信频率，该频率又影响着算法收敛的速度。
    拜占庭问题
        如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出问题。

参考: https://zhuanlan.zhihu.com/p/41228196

==== Paxos

参考: https://www.zhihu.com/question/52337912

==== Raft
Raft和Paxos最大的不同之处就在于Raft的强领导特性: Raft使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上，这样使得算法更加容易理解。例如，在Paxos中，领导人选举和基本的一致性协议是正交的: 领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制: Paxos同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。

像Raft一样，VR和ZooKeeper也是基于领导人的，因此也拥有一些Raft的优点。但是，Raft比VR和ZooKeeper拥有更少的机制因为Raft尽可能的减少了非领导人的功能。例如，Raft中日志条目都遵循着从领导人发送给其他人这一个方向: 附加条目RPC是向外发送的。在VR中，日志条目的流动是双向的(领导人可以在选举过程中接收日志)；这就导致了额外的机制和复杂性。根据ZooKeeper公开的资料看，它的日志条目也是双向传输的，但是它的实现更像Raft。

Q: Raft的实现?
Q: Raft实现的正确性?

参考: http://thesecretlivesofdata.com/raft/
参考: https://raft.github.io/raft.pdf
参考: https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md

==== Quorum NWR
===== NWR含义
三个参数：N, W, R
协议由三个可配置的参数定义：

    N (Number of Replicas) - 副本总数
    含义：一份数据在分布式集群中总共保存了多少个副本。
    目的：决定了系统的耐久性。N越大，数据能容忍的节点故障就越多。
    示例：N=3 表示每份数据都有3个副本存储在不同的节点上。

    W (Write Quorum Size) - 写法定票数
    含义：一次写操作必须成功完成的最少副本数，才算写入成功。
    目的：决定了写操作的一致性强度。W越大，写操作越可靠，但延迟也可能越高。

    R (Read Quorum Size) - 读法定票数
    含义：一次读操作必须成功查询的最少副本数，才能确定最终返回的数据。
    目的：决定了读操作的一致性强度。R越大，读到的数据越可能是最新的，但延迟也可能越高。

NWR:

    N: 副本数，也称复制因子(Replication Factor)
        表示集群中同一份数据有多少个副本
    W: 一致性级别(Write Consistency Level)
        表示成功完成W个副本更新，才算完成写操作
    R: 读一致性级别(Read Consistency Level)
        表示读取一个数据对象时需要读R个副本，返回R个副本中最新的那份数据

===== Quorum 机制
Quorum: 法定人数

NWR可以组合N、W、R参数，提供了按需选择一致性级别的灵活度:

    当 W + R > N 时:
        根据抽屉原理，在进行读操作时R个副本返回的结果中一定包含最新的数据。
        然后再利用时间戳、版本号等手段即可确定出最新的数据。
        换言之在满足该条件的参数组合下，可以实现数据的强一致性。

        因为写入的副本集合（大小为W）和读取的副本集合（大小为R）必定至少有一个重叠的副本。
        这个重叠的副本包含了最后一次成功写入的最新数据，因此读操作通过比较时间戳或版本号，总能从这个重叠副本中获取到最新的值。

    当 W + R <= N 时:
        无法实现强一致性，只能保障最终一致性。即系统可能会获取旧数据。

    一些特殊情况:
        W = N且R = 1，即WARO(Write All Read One)
        W = N: 读性能比较好
        R = N: 写性能比较好
        W = (N + 1) / 2、R = (N + 1) / 2，容错能力比较好，能容忍少数节点(即(N - 1) / 2)的故障。

===== vs-Raft

===== 参考
https://zhuanlan.zhihu.com/p/469565533

==== ZAB
ZAB: zookeeper atomic broadcast(zookeeper原子广播协议)

==== POW
Proof-of-Work 工作量证明
比特币(bitcoin)就使用了工作量证明的分布式一致性算法。

工作量证明的关键: 分布式系统中的节点必须解决一个不易解决但容易验证的问题。

注意:
拜占庭容错算法(例如PoW、PBFT)，能容忍一定比例的作恶行为，主要用于相对开放的场景，如公链、联盟链。
非拜占庭容错算法(例如Raft)无法对作恶行为进行容错，主要用于封闭、绝对可信的场景中，如私链。

==== 正确性
https://lamport.azurewebsites.net/tla/tla.html