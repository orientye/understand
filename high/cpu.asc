= CPU
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:homepage: http://orientye.com

<<<

== 概览
https://www.lighterra.com/papers/modernmicroprocessors/

== 概念
- CPU频率
CPU频率指的是CPU内核工作时的时钟频率，也就是CPU每秒内能产生多少个同步脉冲信号，单位是赫兹（Hz），通常以兆赫（MHz）或吉赫（GHz）为单位。它表示CPU的运行速度，频率越高，CPU每秒处理的脉冲数量就越多，通常意味着更快的运算能力，但整体性能也受到其他因素（如架构、缓存、核心数）的影响。

== 流水线(pipelining)
指令是如何被执行的？首先，它被取出，然后被解码，接着由相应的功能单元执行，最后结果被写入到相应的地方。
现代处理器在一个流水线中将这些阶段重叠，就像装配线一样。当一条指令在执行时，下一条指令正在被解码，而后面的那条指令正在被取出。

每个流水线阶段都包含组合逻辑电路，并可能访问寄存器组和/或某种形式的高速缓存存储器。流水线阶段之间通过锁存器进行分隔。统一的时钟信号对各级锁存器进行同步控制，使得所有锁存器能够同时捕获各流水线阶段产生的处理结果。实际上，正是时钟信号驱动着指令在流水线中逐级传递。
在每个时钟周期开始时，承载着部分处理完成指令的数据和控制信息会被暂存于流水线锁存器中，这些信息将作为下一级流水线阶段逻辑电路的输入。在时钟周期运行期间，电信号会通过该阶段的组合逻辑电路进行传播，最终生成的输出结果将在周期结束时被下一级流水线锁存器精准捕获...

由于每条指令的执行结果在完成执行阶段后即可获取，后续指令理应能够立即使用该结果值，而无需等待该结果在写回阶段被提交到目标寄存器。为实现这一机制，需增设称为"旁路"的数据前向通路——这些通路沿着流水线逆向延伸。

流水线深度(pipeline depth):
Modern x86 CPU pipeline depth varies, but common architectures like Intel's Core series have around a 14-stage pipeline.
https://softwareengineering.stackexchange.com/questions/210818/how-long-is-a-typical-modern-microprocessor-pipeline

Since the clock speed is limited by (among other things) the length of the longest, slowest stage in the pipeline, the logic gates that make up each stage can be subdivided, especially the longer ones, converting the pipeline into a deeper super-pipeline with a larger number of shorter stages. Then the whole processor can be run at a higher clock speed! Of course, each instruction will now take more cycles to complete (latency), but the processor will still be completing 1 instruction per cycle (throughput), and there will be more cycles per second, so the processor will complete more instructions per second (actual performance)...

Today, modern processors strive to keep the number of gate delays down to just a handful for each pipeline stage, about 12-25 gates deep (not total!) plus another 3-5 for the latch itself, and most have quite deep pipelines...

== 多发射与超标量(Multiple Issue - Superscalar)
发射宽度(issue width)

== 超长指令集/超长指令字(VLIW即Very Long Instruction Word)
在一条超长的指令中，包含了多个可以并行执行的操作（例如算术运算、内存访问、分支等）。 处理器在单个时钟周期内，取出这条长指令，并将其中的多个操作分发到多个独立的功能单元（如ALU、FPU、加载/存储单元）中同时执行。

核心思想: 将并行性从硬件转移到编译器。

VLIW的工作原理:

    编译时：编译器对高级语言代码进行极致优化。
        它进行指令级并行分析，找出没有数据依赖关系的操作。
        它将这些可以并行执行的操作组合在一起，形成一条VLIW指令。
        一条典型的VLIW指令可能看起来像这样：
        [ 操作码: ADD R1, R2, R3 | 操作码: LOAD R4, [R5] | 操作码: MUL R6, R7, R8 | 无操作 NOP ]
        这条指令告诉处理器，在同一周期内，ALU1执行加法，加载/存储单元执行内存读取，ALU2执行乘法。
        如果找不到足够的操作来填满所有槽位，编译器必须插入NOP（无操作）指令，这会浪费指令空间。
    执行时：
        取指：处理器从指令缓存中取出一条完整的VLIW长指令。
        译码：译码逻辑非常简单，几乎不需要动态分析，直接将指令的不同字段分发到对应的功能单元。
        执行：所有功能单元同时开始执行各自的操作。
        写回：将结果写回到寄存器堆。

VLIW的优势:
高性能潜力：通过编译器静态调度，理论上可以在一个周期内完成大量工作，避免了硬件动态调度的开销。
硬件简单，功耗低：移除了复杂的乱序执行逻辑、分支预测器和硬件调度器，使得芯片面积更小，功耗更低。这在嵌入式系统和移动设备领域非常有吸引力。
设计更可控：将复杂性转移到软件，使得硬件设计周期更短，验证更容易。

VLIW的挑战与劣势:
尽管理念先进，但VLIW在通用计算领域并未成为主流，主要是因为以下几个致命弱点：
编译器依赖性极强：VLIW的性能完全依赖于编译器的智能程度。一个“笨”的编译器会产生大量NOP，导致代码臃肿，性能低下。
代码密度问题：由于需要填充NOP，以及指令本身很长，导致编译后的二进制文件体积庞大，对指令缓存不友好。
二进制兼容性差：这是VLIW的“阿喀琉斯之踵”。
为某一代VLIW处理器（例如有4个ALU）编译的二进制代码，无法在功能单元数量不同的下一代处理器（例如有6个ALU）上高效运行。因为指令包中的操作槽位和延迟都硬编码在二进制里了。
这与x86/ARM架构强大的向后兼容性形成鲜明对比。
对缓存不命中敏感：在乱序执行CPU中，如果发生缓存不命中，CPU可以聪明地去执行后面不相关的指令来“填坑”。而在VLIW中，指令顺序是编译器静态安排的，一旦某个操作（如内存加载）停滞，整个指令包（以及后续依赖的包）都可能被阻塞，硬件难以动态绕过。
难以处理运行时不确定性：如指针别名、动态分支等，在编译时很难100%确定，这限制了编译器挖掘并行性的能力。

VLIW的应用场景:
尽管在通用CPU领域受挫（最著名的尝试是Intel Itanium/IA-64，但其并未达到预期成功），VLIW在特定领域依然大放异彩：
数字信号处理器：这是VLIW最成功的领域。TI（德州仪器）的C6000系列DSP、CEVA的DSP核心等都广泛采用VLIW架构。因为DSP处理的算法（如FIR滤波器、FFT）通常数据并行性高，流程规整，非常适合编译器进行静态调度。
图形处理器：现代GPU的着色器核心在某些方面与VLIW的思想有相似之处，它们擅长对大量并行线程（SIMD/SIMT）进行批处理。
嵌入式媒体处理器：一些专门用于视频编解码的芯片会采用VLIW架构来高效处理高度并行的媒体数据流。

== 参考
《Performance Analysis and Tuning on Modern CPUS》2nd: https://github.com/dendibakh/perf-book