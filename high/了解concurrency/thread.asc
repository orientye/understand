== thread

=== 概览
    
    管理
          creation, termination, cancellation
          join detach
          once
          thread-specific data/thread-local storage
          thread-pool
          sleep
          schedule policy and priority

    共享状态（互斥）
          volatile
          atomic
          memory order/fence/barrier
          mutex, spin lock, rw lock, semaphore… 
      
    同步状态（同步）
          condition_variable, wait/notify, future/promise

    其它
         thread impletation
         thread stack
         threads ans signal
         threads ans process control


=== join and detach

- Join

    If a thread is not detached , then we must join with it using pthread_join(). If we fail to do this, then, when the thread terminates, it produces the thread equivalent of a zombie process. Aside from wasting system resources, if enough thread zombies accumulate, we won’t be able to create additional threads. 

- Detach

    Detaching a thread doesn’t make it immune to a call to exit() in another thread or a return in the main thread. In such an event, all threads in the process are imme-diately terminated, regardless of whether they are joinable or detached. To put things another way, pthread_detach() simply controls what happens after a thread terminates, not how or when it terminates. 

=== specific data and local storage

Thread-Specific Data  vs. Thread-Local Storage

The pthread_key_create and friends are much older, and thus supported on more systems.

The __thread is a relative newcomer, is generally much more convenient to use, and (according to Wikipedia) is supported on most POSIX systems that still matter: Solaris Studio C/C++, IBM XL C/C++, GNU C, Clang and Intel C++ Compiler (Linux systems).

The __thread also has a significant advantage that it is usable from signal handlers (with the exception of using __thread from dlopened shared library, see this bug), because its use does not involve malloc (with the same exception).

参考: https://stackoverflow.com/questions/21015738/thread-specific-data-vs-thread-local-storage 

=== thread-thread pool

==== Task

- 参数
- 返回值
- 队列

==== 调度

==== 关闭

==== 异常

==== 自动扩容缩容

参考：https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html

=== sleep

除非测试，否则尽量不用sleep

使用select/poll/epoll_wait或者条件变量等方式:

https://stackoverflow.com/questions/264350/is-there-an-alternative-for-sleep-in-c


=== volatile

==== volatile(c/c++)
- 作用

    volatile was specifically intended to be used when interfacing with memory-mapped hardware, signal handlers and the setjmp machine code instruction. This makes volatile directly applicable to systems-level programming rather than normal applications-level programming.

    • in C, and consequently C++, the volatile keyword was intended to
    • allow access to memory-mapped I/O devices
    • allow uses of variables between setjmp and longjmp
    • allow uses of sig_atomic_t variables in signal handlers.

- 原子性、可见性、有序性

    Q: c/c++的volatile呢？
    Q: java的volatile呢？

- NOTE

    volatile is (nearly) useless for platform-agnostic, multithreaded application programming. It does not provide any synchronization, it does not create memory fences, nor does it ensure the order of execution of operations. It does not make operations atomic. It does not make your code magically thread safe. volatile may be the single-most misunderstood facility in all of C++. 

    Bjarne Stroustrup says as much in TCPPPL4E:
    • Do not use volatile except in low-level code that deals directly with hardware.
    • Do not assume volatile has special meaning in the memory model. It does not. It is not -- as in some later languages -- a synchronization mechanism. To get synchronization, use atomic, a mutex, or a condition_variable

- 本质

    prevents the compiler from performing optimization on code involving volatile objects, thus ensuring that each volatile variable assignment and read has a corresponding memory access

- 参考

    https://en.wikipedia.org/wiki/Volatile_(computer_programming)
    https://stackoverflow.com/questions/4557979/when-to-use-volatile-with-multi-threading

==== volatile(java)

https://github.com/openjdk/jdk/blob/master/src/hotspot/os_cpu/linux_x86/orderAccess_linux_x86.hpp

// A compiler barrier, forcing the C++ compiler to invalidate all memory assumptions
static inline void compiler_barrier() {
  __asm__ volatile ("" : : : "memory");
}

inline void OrderAccess::loadload()   { compiler_barrier(); }
inline void OrderAccess::storestore() { compiler_barrier(); }
inline void OrderAccess::loadstore()  { compiler_barrier(); }
inline void OrderAccess::storeload()  { fence();            }

inline void OrderAccess::acquire()    { compiler_barrier(); }
inline void OrderAccess::release()    { compiler_barrier(); }

inline void OrderAccess::fence() {
   // always use locked addl since mfence is sometimes expensive
#ifdef AMD64
  __asm__ volatile ("lock; addl $0,0(%%rsp)" : : : "cc", "memory");
#else
  __asm__ volatile ("lock; addl $0,0(%%esp)" : : : "cc", "memory");
#endif
  compiler_barrier();
}

inline void OrderAccess::cross_modify_fence() {
  int idx = 0;
#ifdef AMD64
  __asm__ volatile ("cpuid " : "+a" (idx) : : "ebx", "ecx", "edx", "memory");
#else
  // On some x86 systems EBX is a reserved register that cannot be
  // clobbered, so we must protect it around the CPUID.
  __asm__ volatile ("xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx " : "+a" (idx) : : "esi", "ecx", "edx", "memory");
#endif
}

=== atomic
--------------------------- hardware/cpu -------------------
●  cpu视角

---------------------------  os/kernel -----------------------
●  kernel视角

--------------------------- lan/compiler--------------------
●  基于CAS的实现
●  c++ atomic（c++ atomic超出了原子操作的范畴，不在此节讨论，参考memory fence）
●  java 原子操作

注意：这里的atomic指原子操作，不等同于C++里的atomic

atomic-cpu视角

X86:

1 处理器自动保证基本内存操作的原子性
首先处理器会自动保证基本的内存操作的原子性。处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，
其他处理器不能访问这个字节的内存地址。奔腾 6 和最新的处理器能自动保证单处理器对同一个缓存行里进行 16/32/64 位的操作是原子的，但是复杂
的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。但是处理器提供总线锁定和缓存锁定两个机制来保证复杂
内存操作的原子性。

2 使用总线锁保证原子性
第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写（i++ 就是经典的读改写操作）操作，那么共享变量就会被多个处理
器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致。
原因是有可能多个处理器同时从各自的缓存中读取变量 i，分别进行加一操作，然后分别写入系统内存当中。那么想要保证读改写共享变量的操作是原子
的，就必须保证 CPU1 读改写共享变量的时候，CPU2 不能操作缓存了该共享变量内存地址的缓存。

处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个 LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的
请求将被阻塞住, 那么该处理器可以独占使用共享内存。

3 使用缓存锁保证原子性
第二个机制是通过缓存锁定保证原子性。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把 CPU 和内存之间通信锁住了，这
使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行
优化。

频繁使用的内存会缓存在处理器的 L1，L2 和 L3 高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁，在奔腾 6 和最
近的处理器中可以使用“缓存锁定”的方式来实现复杂的原子性。所谓“缓存锁定”就是如果缓存在处理器缓存行中内存区域在 LOCK 操作期间被锁定，
当它执行锁操作回写内存时，处理器不在总线上声言 LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为
缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时会起缓存行无效，当 CPU1 修改
缓存行中的 i 时使用缓存锁定，那么 CPU2 就不能同时缓存了 i 的缓存行。

但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处
理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于 Intel 486 和奔腾处理器, 就算锁定的内存区域在处理器的缓存行中也会调用总
线锁定。

HOW :  Lock前缀指令

参考： https://www.infoq.cn/article/atomic-operation

atomic-cpu视角

X86:

https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-
developer-vol-3a-part-1-manual.html

chapter8 multi-processor management  8.1

Lock 前缀指令会引起处理器缓存回写到内存。Lock 前缀指令导致在执行指令期间，声言处理器的 LOCK# 信号。在多处理器
环境中，LOCK# 信号确保在声言该信号期间，处理器可以独占使用任何共享内存。（因为它会锁住总线，导致其他 CPU 不能
访问总线，不能访问总线就意味着不能访问系统内存），但是在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕
竟锁总线开销比较大。在 8.1.4 章节有详细说明锁定操作对处理器缓存的影响，对于 Intel486 和 Pentium 处理器，在锁操作
时，总是在总线上声言 LOCK#信号。但在 P6 和最近的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言 
LOCK#信号。相反地，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称
为“缓存锁定”，缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据。

一个处理器的缓存回写到内存会导致其他处理器的缓存无效。IA-32 处理器和 Intel 64 处理器使用 MESI（修改，独占，共享，
无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32 和 Intel 64 处理器
能嗅探其他处理器访问系统内存和它们的内部缓存。它们使用嗅探技术保证它的内部缓存，系统内存和其他处理器的缓存的数
据在总线上保持一致。

参考：https://www.infoq.cn/article/ftf-java-volatile

atomic-kernel视角

x86:
https://github.com/torvalds/linux/blob/master/arch/x86/include/asm/atomic.h

atomic-compilier视角

● 基于CAS的实现

● 基于锁的实现

atomic-基于CAS的实现原理

实现：
●  Compare-And-Swap (CAS) on x86 
●  Load-Link/Store-Conditional (LL/SC) on Alpha, PowerPC, MIPS and ARM.

Implementations

Compare-and-swap (and compare-and-swap-double) has been an integral part of the IBM 370 (and all successor) architectures since 1970. In the 
x86 (since 80486) and Itanium architectures this is implemented as the compare and exchange (CMPXCHG) instruction (on a multiprocessor the 
LOCK prefix must be used).

As of 2013, most multiprocessor architectures support CAS in hardware, and the compare-and-swap operation is the most popular synchronization 
primitive for implementing both lock-based and non-blocking concurrent data structures.

The atomic counter and atomic bitmask operations in the Linux kernel typically use a compare-and-swap instruction in their implementation. The 
SPARC-V8 and PA-RISC architectures are two of the very few recent architectures that do not support CAS in hardware; the Linux port to these 
architectures uses a spinlock.

atomic-CAS的问题

ABA 问题
因为 CAS 需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是 
A，变成了 B，又变成了 A，那么使用 CAS 进行检查时会发现它的值没有发生变化，但是实际上却变化了。
ABA 问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么 
A－B－A 就会变成 1A-2B－3A。
从 Java1.5 开始 JDK 的 atomic 包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。这个类
的 compareAndSet 方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如
果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。

 public boolean compareAndSet
         (V      expectedReference,// 预期引用
          V      newReference,// 更新后的引用
         int    expectedStamp, // 预期标志
         int    newStamp) // 更新后的标志

循环时间长开销大
自旋 CAS 如果长时间不成功，会给 CPU 带来非常大的执行开销。如果 JVM 能支持处理器提供的 pause 指
令那么效率会有一定的提升，pause 指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）, 使 
CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可
以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU 
pipeline flush），从而提高 CPU 的执行效率。

只能保证一个共享变量的原子操作
当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，
循环 CAS 就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合
并成一个共享变量来操作。比如有两个共享变量 i＝2,j=a，合并一下 ij=2a，然后用 CAS 来操作 ij。从 
Java1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个
对象里来进行 CAS 操作。

参考：https://www.infoq.cn/article/atomic-operation

atomic-C++CAS

weak VS. strong

keyword:   spurious failure,  performance,  !loop

https://stackoverflow.com/questions/25199838/understanding-stdatomiccompare-exchange-weak-in-c11

https://stackoverflow.com/questions/17914630/when-should-stdatomic-compare-exchange-strong-be-use

https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange

注意：
Note that although you can use std::atomic<float> or std::atomic<double>,
because the built-in floating point types do satisfy the criteria for use with memcpy and memcmp, the behavior may be surprising in 
the case of compare_exchange_strong. The operation may fail even though the old stored value was equal in value to the comparand, 
if the stored value had a different representation. Note that there are no atomic arithmetic operations on floating-point values. You’ll 
get similar behavior with compare_exchange_
strong if you use std::atomic<> with a user-defined type that has an equality-comparison operator defined, and that operator differs 
from the comparison using memcmp—the operation may fail because the otherwise-equal values have a different representation.

Q: vs java CAS  (one tip: AtomicStampedReference(V initialRef, int initialStamp))

atomic-java

java.util.concurrent.atomic

Q: vs c++ atomic

=== memory order

●  reorder问题 
●  可见性问题

--------------------------- hardware/cpu -------------------
●  cpu视角 

---------------------------  os/kernel -----------------------
●  kernel视角

-------------------------- lan/compiler --------------------
●  memory order - c++
●  memory order - java

--------------------------- application --------------------
●  应用实例

Q:  memory order vs. cache coherence
https://course.ece.cmu.edu/~ece847c/S15/lib/exe/fetch.php?media=part2_2_sorin12.pdf

memory order-reorder问题

● compiler reorder(compile time)
● cpu reorder(runtime)

● eg.   Initially, memory locations x and f both hold the value 0
• Processor #1:
    while (f == 0); 
    print x; 
• Processor #2:
    x = 42; 
    f = 1;

Q:  如何阻止compiler reorder?

complier: asm volatile("" ::: "memory");

Q:  如何阻止cpu reorder?

Q:  单线程会发生指令重排吗(同理，运行在单个CPU core上的多线程)
A:  会，但不会影响结果

Q:  什么样的肯定不会重排？

memory order-可见性问题

memory order-cpu视角

x86:

https://www.intel.com/content/www/us/en/architecture-and-technology/64-
ia-32-architectures-software-developer-vol-3a-part-1-manual.html

chapter8 multi-processor management  8.2

• SFENCE — Serializes all store (write) operations that occurred prior to the SFENCE instruction in the program
instruction stream, but does not affect load operations.

• LFENCE — Serializes all load (read) operations that occurred prior to the LFENCE instruction in the program
instruction stream, but does not affect store operations.

• MFENCE — Serializes all store and load operations that occurred prior to the MFENCE instruction in the
program instruction stream.

memory order-kernel视角

x86:
https://github.com/torvalds/linux/blob/master/arch/x86/include/asm/barrier.h

memory order

Acquire semantics is a property that can only apply to operations that read from 
shared memory, whether they are read-modify-write operations or plain loads. The 
operation is then considered a read-acquire. Acquire semantics prevent memory 
reordering of the read-acquire with any read or write operation that follows it in 
program order.

Release semantics is a property that can only apply to operations that write to shared 
memory, whether they are read-modify-write operations or plain stores. The operation 
is then considered a write-release. Release semantics prevent memory reordering of 
the write-release with any read or write operation that precedes it in program order.

Fence Semantics
A fence semantics combines both acquire and release semantics behavior.

memory order-c++

https://en.cppreference.com/w/cpp/atomic/memory_order:

● Sequenced-before

● Carries dependency

● Modification order

● Release sequence

● Dependency-ordered before

● Inter-thread happens-before

● Happens-before

● Visible side-effects

● Consume operation

● Acquire opertation

● Release operation

● Synchronizes-with

memory order-c++

● 本质上两个问题：

      【1】                       不可以重排到                          的前面/后面

      【2】                        对谁                          可见                       

memory order-c++

memory order-c++ - relaxed

memory order–c++ - release acquire

memory order–c++ - release acquire

memory order–c++ - release acquire

memory order–c++ - release consume

memory order–c++ - release consume

memory order–c++ - sequentially consistent

Q: 上述例子如果换成acquire/release，结果？
Q: seq_cst做了什么？ 什么情况下使用？

memory order–c++ - implement

以memory_order_seq_cst  为例:
Ubuntu(g++, VM, Intel i5 4 core):    汇编指令mfence
Mac OSX(g++,  Intel i5 2 core):        汇编指令xchgl(= lock xchgl)
Win10(VS2015, Intel i5 4 core):        _ReadWriteBarrier, _InterlockedExchange

is_lock_free

memory order–c++-
std::atomic_thread_fence

extern "C" void atomic_thread_fence( std::memory_order order ) noexcept;

Establishes memory synchronization ordering of non-atomic and relaxed 
atomic accesses, as instructed by order, without an associated atomic 
operation.

memory order–java

http://gee.cs.oswego.edu/dl/jmm/cookbook.html

《深入理解java虚拟机》-第12章 Java内存模型与线程

memory order-应用示例

https://github.com/facebook/rocksdb/blob/master/memtable/skiplist.h

https://github.com/apache/incubator-brpc/blob/master/src/bthread/work_stealing_queue.h

vs. atomic
• 参考：https://stackoverflow.com/questions/15056237/which-is-more-efficient-basic-mutex-lock-or-atomic-integer


=== mutex

impl:
linux kernel: 
https://github.com/torvalds/linux/blob/master/include/linux/mutex.h
https://github.com/torvalds/linux/blob/master/kernel/locking/mutex.c
https://0xax.gitbooks.io/linux-insides/content/SyncPrim/linux-sync-4.html

glibc: https://github.com/bminor/glibc/blob/master/nptl/pthread_mutex_lock.c

gcc(libstdc++-v3):https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/bits/std_mutex.h
clang(libc++): https://github.com/llvm/llvm-project/blob/master/libcxx/src/mutex.cpp跟平台相关，例如linux通过pthread

performance

=== spinlock

vs. mutex

impl
linux kernel: 
https://github.com/torvalds/linux/blob/master/include/linux/spinlock.h
#define _spin_lock_irq(lock) \
do { \

local_irq_disable(); \
preempt_disable(); \
_raw_spin_lock(lock); \
__acquire(lock); \

} while (0)
#define local_irq_disable() 

__asm__ __volatile__("cli": : :"memory") 

内核spinlock，不能(隐式)调用sleep。
Q: 为什么要关中断？关抢占？
Q: 为什么上锁后不能显式/隐式调用sleep?

glibc:
https://github.com/bminor/glibc/blob/master/mach/lock-intern.h

应用层一般借助CAS实现
src/core/ngx_spinlock.c

=== rwlock

vs. mutex

impl
linux kernel: 
https://github.com/torvalds/linux/blob/master/include/linux/rwlock.h

glibc: https://github.com/bminor/glibc/tree/master/nptl

clang: https://github.com/llvm/llvm-project/blob/master/llvm/lib/Support/RWMutex.cpp内部还是pthread

Q: vs. RCU
The read-copy-update (RCU) algorithm is one solution to the readers–writers problem. RCU is wait-free for readers. 

Q: vs Copy On Write(cow) 

The Linux kernel implements a special solution for few writers called seqlock.

java：
http://tutorials.jenkov.com/java-concurrency/read-write-locks.html

Q:  写饥饿问题如何避免？
Q:  StampedLock(java8) vs. ReentrantReadWriteLock

=== semaphore

vs. mutex

impl
linux kernel: 
https://github.com/torvalds/linux/blob/master/include/linux/semaphore.h
https://github.com/torvalds/linux/blob/master/kernel/locking/semaphore.c

glibc: https://github.com/bminor/glibc/tree/master/nptl

https://people.eecs.berkeley.edu/~kubitron/courses/cs162-F06/hand-outs/synch.html

condition variable

why

wait:
Q: wait作了什么
Q: 谁会导致wait返回
Q: 什么是虚假唤醒(spurious wakeup)

signal:
Q: signal作了什么
Q: signal与unlock的顺序, 什么是wait morphing：SUSv3http://en.wikipedia.org/wiki/Single_UNIX_Specification的
规范中(pthread)，指明了这两种顺序不管采用哪种，其实现效果都是一样的

使用注意事项:
https://github.com/chenshuo/recipes/blob/master/thread/test/Waiter_test.cc

=== condition variable

impl
glibc: https://github.com/lattera/glibc/blob/master/nptl/pthread_cond_common.c

使用注意事项:
https://github.com/chenshuo/recipes/blob/master/thread/test/Waiter_test.cc

=== thread-impletation

●  Linux Threads(已废弃)， NTPL(now)
https://en.wikipedia.org/wiki/LinuxThreads
https://en.wikipedia.org/wiki/Native_POSIX_Thread_Library

●  Thread Implementation Models
      how threads are mapped onto kernel scheduling entities (KSEs)

      Many-to-one (M:1) implementations (user-level threads)
      One-to-one (1:1) implementations (kernel-level threads)
          Many-to-many (M:N) implementations (two-level model)

      Both of the Linux threading implementations—LinuxThreads and 
NPTL—employ the 1:1 model.

●  thread-stack

                 Four threads executing in a process (Linux/x86-32)

Q: 线程堆栈默认大小？
per-thread-size默认大小: 8MB(x86-32) ？ 32MB(IA64)？
https://stackoverflow.com/questions/2340093/how-is-stack-size-of-linux-process-related-to-pthread-fork-and-exec   

=== thread-signal

●  OS

●  应用程序

=== thread-process control

●  threads and exec()
When any thread calls one of the exec() functions, the calling program is completely replaced. All 
threads, except the one that called exec(), vanish immediately. None ofthe threads executes destructors 
for thread-specific data or calls cleanup handlers. All of the (process-private) mutexes and condition 
variables belonging to the process also disappear. After an exec(), the thread ID of the remaining 
thread is unspecified.

●  threads and fork()
When a multithreaded process calls fork(), only the calling thread is replicated in the child process. 
(The ID of the thread in the child is the same as the ID of the thread that called fork() in the parent.) All 
of the other threads vanish in the child; no thread-specific data destructors or cleanup handlers are 
executed for those threads. 
●  threads and exit()
If any thread calls exit() or, equivalently, the main thread does a return, all threads immediately vanish; 
no thread-specific data destructors or cleanup handlers are executed.