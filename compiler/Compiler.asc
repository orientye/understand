= 编译器
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 6
:homepage: http://orientye.com
<<<

== 概念
• DSL(domain specific language)

    如SQL, Google Protocol Buffers等
    https://en.wikipedia.org/wiki/Domain-specific_language

• 自然语言与编程语言的区别

    https://groups.google.com/g/semean/c/hFhkcGHaU4M

• 主要数据结构

    Tokens
    The Syntax Tree, Abstract Syntax Tree(AST)
    The Symbol Table: 标识符，变量，函数等
    The Literal Table: 常量，字符串等
    Intermediate Code
    Temporary Files

• 自举(bootstraping)

    编译器用自身语言编写(self-compiling)，这是语言迈向成熟的标志。
    在起始阶段，通常还是要借助别的语言来编写。
    https://en.wikipedia.org/wiki/Bootstrapping_(compilers)

• JIT(即时编译)与AOT(提前编译)

    https://www.quora.com/What-is-the-difference-between-JIT-compiler-and-AOT-compiler

• 动态类型与静态类型

    动态类型运行时检查
    静态类型编译期检查
    C# 4 dynamic
    https://stackoverflow.com/questions/1517582/

• 强类型与弱类型

    强类型语言中，变量的类型一旦声明就不能改变
    弱类型语言中，变量类型在运行期时可以改变

    有很少（合理）隐式类型转化的是强类型语言
    有较多（过分）隐式类型转化的是弱类型语言
    python属于动态强类型语言
        https://stackoverflow.com/questions/11328920/is-python-strongly-typed
        https://wiki.python.org/moin/Why%20is%20Python%20a%20dynamic%20language%20and%20also%20a%20strongly%20typed%20language
    c/c++属于静态弱类型语言
    java/c#属于静态强类型语言
    javascript属于动态弱类型语言

• 作用域(scope)与生存期(extent)

    程序集/命名空间/类/函数/块
    生存期与作用域紧密相关，是变量可以访问的时间段，也就是从给它分配内存，到收回内存之间的时间。

• 引用计数与GC

    引用计数实现比较简单，Perl、PHP、Python一开始都是使用的引用计数；
    但引用计数缺乏全局对象图信息，处理循环引用等情况比较麻烦，需要引入类似"弱引用"等方法来解决。

• argument与parameter

    argument: 实际参数
    parameter: 形式参数

• 左值与右值

    https://learn.microsoft.com/zh-cn/cpp/c-language/l-value-and-r-value-expressions

• 闭包

    inner()访问了外部函数中声明的局部变量

• Lambda与匿名函数

    主要是为了方便(真的吗? 例如捕获?)

• 类与原型

    类更简洁明晰，对用户更友好
    原型(例如javascript)更灵活，强大

• REPL

    read–eval–print loop
    https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop

• 虚拟机

    基于堆栈，例如JVM
    基于寄存器(不一定是真正的物理寄存器)，例如Lua

• 尾调用与尾递归

    https://en.wikipedia.org/wiki/Tail_call

== 词法分析

=== 概念与作用
▪ 词法分析(lexical analysis)也称扫描(scanning)

▪ 词法分析将输入的程序按照构词规则分解成一系列token(词法单元)符号，即字符流->记号流

▪ token通常分为以下类型

    (1) 关键字 是由程序语言定义的具有固定意义的标识符
        从词法分析的角度看，关键字是标识符的一部分
    (2) 标识符 用来表示各种名字，如变量名、函数名等
    (3) 常数 常数的类型一般有整型、实型、布尔型、字符串类型等
    (4) 运算符 如+、-、*、/等
    (5) 界符 如逗号、分号、括号等

▪ 输出
通常表示为二元式列表：[<token-name, attribute-value>]

▪ 词法分析是编译过程中的一个阶段，可以在语法分析前进行独自作为一遍，也可以和语法分析结合在一起作为一遍。

▪ 词法分析中的二义性
通常的做法是如果两个模式都可以匹配，匹配在程序中更早出现的模式。例如关键字优先于标识符。flex也是这么处理的。

=== 手工构造与自动构造
- 手工构造词法分析器

    特点:
        复杂且容易出错
        灵活可控
            例如GCC, LLVM等
    思想: 状态转换与有限自动机(FA)

- 自动生成词法分析器

    特点:
        快速实现，代码量少
        难以控制细节
    思想: 将正则表达式转换成NFA，然后把NFA转换成DFA

=== 词法单元的识别
状态转换图(transition diagram)
保留字和标识符的识别
基于状态转换图的词法分析器的结构

=== 正则表达式
==== 概念
正则表达式(regular expression)是一种用来描述词素模式的重要表示方法。
虽然正则表达式不能表达出所有可能的模式，但是可以高效地描述在处理词法单元时要用到的模式类型。

如果M和N是正则表达式，则以下也是正则表达式:
(1) 选择: M | N = {M, N} 可能属于M也可能属于N，但只能是其中之一
(2) 连接: MN = {mn | m ∈ M, n ∈ N} 既属于M，也属于N
(3) 闭包: M* = {ε, M, MM, MMM, ...} (Kleene闭包)
闭包分Kleene闭包和正闭包。Kleene闭包是指某一集合的符号重复0～∞多次，可能是空串；正闭包不包含重复0次，即不包含空串。

其它正则表达式可以看作是基本规则的某种语法糖:
例如a?表示零个或一个a, 可以表示为ε|a

==== 用法
https://github.com/cdoco/learn-regex-zh
https://en.wikipedia.org/wiki/Regular_expression

==== 示例
• 保留字(reserved)

    reserved = if|while|do|...

• 标识符(identifier)

    letter = [a-zA-Z]
    digit = [0-9]
    identifier = letter(letter|digit)*

• 数字(number)

    nat = [0-9]+
    signedNat = (+|-)? nat
    number = signedNat("." nat)?(E signedNat)?

• 注释(comment)

    词法分析会忽略注释，但需要识别注释。

    单个界符(delimiter)的注释用正则表达式比较简单，例如:
        {this is a Pascal commment} 其正则表达式: {(~})*}
        -- this is Ada comment 其正则表达式: --(~newline)*
    多个界符的则困难些，例如:
        /* this is a C commment */
        此时即便用正则表达式写对了，也很难具有好的可读性
        因此这种情况下一般会封装一个函数使用状态机

• Ambiguity, White Space and Lookahead

Q: 正则表达式如何匹配3的倍数？
https://www.zhihu.com/question/24824487

56 行代码用 Python 实现一个 Flex/Lex:
https://zhuanlan.zhihu.com/p/663995549

=== 有限自动机
- FA

    Finite Automaton 有限自动机/有穷自动机

- DFA

    Deterministic Finite Automaton 确定的有穷自动机
    在任何一个状态，基于输入的字符串，都能做一个确定的转换。

    优点: 可以避免回溯问题，运行性能较高
    缺点: 不容易直接设计出来，需要通过一系列方案，基于NFA的转换而得到，并且需要占用额外的空间
    适用情况: 状态较为复杂，或者对时间复杂度要求较为严苛的工业级词法分析器

    Q: 如何用DFA描述identifier, 即identifier = letter(letter|digit)*
    Q: 如何用DFA描述number

- NFA

    Nondeterministic Finite Automaton 非确定的有穷自动机
    存在某些状态，针对某些输入，不能做一个确定的转换。
    可以细分成两种情况:
        对于一个输入，它有两个状态可以转换；
        存在ε转换，也就是没有任何输入的情况下，也可以从一个状态迁移到另一个状态。

    优点: 在设计上更简单直观
    缺点: 无法避免回溯问题，在某些极端的情况下可能会造成编译器运行的性能低下
    适用情况: 状态较为简单，且不存在回溯的场景

- NFA与DFA

    NFA与DFA在表达力上是等价的。任何DFA都是某个NFA的一个特例。

- DFA的实现

    有向图(边和节点可能都带有一些信息)

- 有限自动机与状态转换图

    有穷自动机可以看作是状态转换图的形式化表示。
    与状态转换图不同，有穷自动机既可以在输入字符上执行转换，也可以在空输入上执行转换。

- 自动机与文法

    有限自动机是比较简单的一种自动机，对应于正则文法，也叫做3型文法。
    再强大的是下推自动机，对应于上下文无关文法，也叫做2型文法。
    更强大的是线性有界自动机，对应于上下文相关文法，也叫1型文法。
    图灵机的范围更大，对应0型文法。任何能用产生式写出来的文法规则，都属于0型文法。
    参考: https://en.wikipedia.org/wiki/Chomsky_hierarchy

- 例子
https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%A1%AE%E5%AE%9A%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E8%87%AA%E5%8A%A8%E6%9C%BA#%E4%BE%8B%E5%AD%90

=== 正则表达式与有限自动机
可以将任意一个正则表达式转换为一个大小基本相同的NFA；
任何NFA可以转换为一个代表相同模式的DFA，虽然最坏的情况下自动机的大小会以指数级增长，但在常见的程序设计语言中尚未碰到这些情况。

RE->NFA->DFA:
1 RE->NFA: 使用Thompson构造法构建一个识别该RE的NFA
2 NFA->DFA: 使用子集构造法导出一个能够模拟该NFA的DFA
3 最小化DFA: 使用Hopcroft算法来识别该DFA中等价的状态，来构建一个最小DFA

可以将任意一个NFA或DFA会转化为一个正则表达式。

参考:
https://time.geekbang.org/column/article/137286

=== 词法分析器生成工具

=== 基于DFA的模式匹配器的优化

== 语法分析

=== 概念
==== 语法分析(syntax analysis)
也称解析(parsing)
输出: 语法树(syntax tree)
语法分析是把程序的结构识别出来，并形成一棵便于由计算机处理的抽象语法树。

抽象语法树:
https://en.wikipedia.org/wiki/Abstract_syntax_tree
AST是通常是一颗多叉树
https://stackoverflow.com/questions/65729833/is-an-abstract-syntax-tree-always-a-binary-tree

==== 终结符与非终结符
https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols

Terminal symbols are symbols that may appear in the outputs of the production rules of a formal grammar and which cannot be changed using the rules of the grammar.

Nonterminal symbols are those symbols that can be replaced. They may also be called simply syntactic variables.

    <digit> ::= '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'
    <integer> ::= ['-'] <digit> {<digit>}
    In this example:
        symbols (-,0,1,2,3,4,5,6,7,8,9) are terminal symbols
        <digit> and <integer> are nonterminal symbols

==== 产生式
production rule
https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols#Production_rules
https://en.wikipedia.org/wiki/Production_(computer_science)

==== 上下文无关文法(Context-Free Grammar, CFG)
上下文无关文法就是说这个文法中所有的产生式左边只有一个非终结符。
CFG每个文法规则左边都只有一个非终结符，而右边可以由非终结符和终结符组成，意思是，只要出现左边的非终结符，都可以等价替换成右边的那一串表达式。
CFG规定左边有且只有一个非终结符，意味着这个非终结符出现在哪，前后都有啥并不重要，只要出现它都可以等价替换右边的式子，因此叫做上下文无关。上下文指的是左边这个非终结符的上下文。
如何理解上下文无关文法？
上下文无关的意思是，无论在任何情况下，文法的推导规则都是一样的。
例如，在变量声明语句中可能要用到一个算术表达式来做变量初始化，在其他地方可能也会用到算术表达式。但不管在什么地方，算术表达式的语法都一样，都允许用加法和乘法，计算优先级也不变。
大多数计算机语言，都能用上下文无关文法来表达其语法。
https://www.zhihu.com/question/21833944
https://en.wikipedia.org/wiki/Context-free_grammar

==== 上下文有关文法(Context-Sensitive Grammar, CSG)
也是一种形式文法，其中任何产生式规则的左手端和右手端都可以被终结符和非终结符构成的上下文所围绕。
https://en.wikipedia.org/wiki/Context-sensitive_grammar

==== PEG文法
解析表达文法(Parsing Expression Grammar, PEG)
因为PEG更加严格更加强大，PEG可以成为很好的正则表达式的替代品。例如，一个正则表达式本身是无法匹配嵌套的括号对，因为正则表达式不是递归的，但是PEG却能做到这点。
许多CFG固有的存在二义性，即使它们原本要描述的东西并不具有二义性。C, C++, Java里面著名的悬空else问题就是一个例子。这个问题通常都是应用文法之外的一个规则解决。而在PEG里面，因为使用了优先权，所以根本不存在这种问题。
https://en.wikipedia.org/wiki/Parsing_expression_grammar
https://stackoverflow.com/questions/59157302/
https://leafo.net/guides/parsing-expression-grammars.html
https://www.inf.puc-rio.br/~roberto/docs/peg.pdf
https://www.inf.puc-rio.br/%7Eroberto/docs/ry10-01.pdf
http://cuberl.com/2020/06/08/peg/

==== 线性文法(linear grammar)
has at most one nonterminal in the right-hand side of each of its productions.
https://en.wikipedia.org/wiki/Linear_grammar

==== 正则文法(regular grammar)
称为3型文法。这种文法分为两种类型: 第一类要求生成式的形式必须是A→ωB或A→ω，其中A，B都是变元，ω是终结符串，也称为右线性文法。第二类正则文法称为左线性文法，它要求生成式必须是A→Bω，或A→ω的形式。

==== 逆波兰表达式(Reverse Polish Notation)
也称后缀表达式，前缀表达式即波兰表达式，正常的计算规则(需要带括号)则是中缀表达式。
波兰表达式和逆波兰表达式有个好处，去掉括号后表达式无歧义。

波兰表达式即前缀表达式，实际是抽象语法树的表示方式，比如中缀 (1 + 2) * (3 + 4) 编译时转成的抽象语法树为:

            *
          /    \
         +      +
        / \    / \
       1   2  3   4

逆波兰表示式，可用栈进行计算: 遇到数字就将数字压栈，遇到操作符，就将栈顶的两个元素取出计算，将计算结果再压入栈。

对于编译器开发者来说，逆波兰式还有另外两个优点:
一个优点是如果使用自底向上的语法分析器（如bison），它非常容易产生逆波兰式。如果在识别动作代码的规则里发出 (emit) 相应操作符或操作数的动作代码，代码将是基于逆波兰式的顺序。
另外一个优点是它很容易把逆波兰式记号的字符串转化为抽象语法树，倒过来也一样成立。

参考: https://www.zhihu.com/question/41103160

==== 方法分类
处理文法的语法分析器大致可以分为三种类型: 通用型，自顶向下型，自底向上型。
像Cocke-Younger-Kasami算法和Earley算法可以对任意文法进行语法分析，但这些通用方法效率比较低，不能用于编译器产品。
自顶向下(top-down): LL(1)
自底向上(bottom-up): LR(0), SLR(1), LALR(1), LR(1)
https://blog.csdn.net/misskanagi/article/details/29852901
https://softwareengineering.stackexchange.com/questions/19541/what-are-the-main-advantages-and-disadvantages-of-ll-and-lr-parsing

==== ε(epsilon)
表示没有/空产生式: the empty production rule, matching only the empty string

==== BNF(Backus-Naur form)与EBNF(Extended BNF)
be used to express a context-free grammar.
Any grammar defined in EBNF can also be represented in BNF, though representations in the latter are generally lengthier. E.g., options and repetitions cannot be directly expressed in BNF and require the use of an intermediate rule or alternative production defined to be either nothing or the optional production for option, or either the repeated production of itself, recursively, for repetition. The same constructs can still be used in EBNF.
BNF与EBNF的表达力是一样的，但EBNF更简单些。
https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form#Advantages_over_BNF
https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form
https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form

==== Chomsky hierarchy
https://en.wikipedia.org/wiki/Chomsky_hierarchy

==== 优先级
优先级是通过右侧不同产生式的顺序决定的。在标准的上下文无关文法中，产生式的顺序是无关的，但在具体的算法中，会按照确定的顺序来尝试各个产生式。

通过在语法推导中的层次来决定的，优先级越低的，越先尝试推导。

==== 结合性
与左递归还是右递归有关，左递归导致左结合，右递归导致右结合。

==== 二义性
- 什么是二义性问题？
注意: 有些文法在某种算法下(例如LL)没有二义性，但在某种算法下(例如LR)就存在二义性，因此，文法要经常和解析算法配合。

- 如何解决二义性问题？
需要首先找出冲突的起源，然后再来确定语言本身正确与否，如果语言本身没有问题，那么只需要调整语法来正确地描述语言。

=== 上下文无关文法
- 定义
一个上下文无关文法由四个元素组成:
(1) 一个终结符号集合，有时也称为词法单元
(2) 一个非终结符号集合，有时也称为语法变量。每个非终结符号表示一个终结符号串的集合。
(3) 一个产生式集合
(4) 指定一个非终结符号为开始符号

- 例如:
stmt -> if (expr) stmt else stmt
其中:
箭头可以读作"可以具有如下形式"，这样的规则称为产生式(production)；
if和括号这样的词法元素称为终结符号(terminal)；
expr和stmt这样的变量表示终结符号的序列，则称为非终结符号(nonterminal)。

- 两种描述形式:
一种是巴科斯范式(BNF)
一种是巴科斯范式的一种扩展形式(EBNF)，其更利于自动化生成语法分析器。
其中，产生式、终结符、非终结符、开始符号是巴科斯范式的基本组成要素。

- 上下文无关文法与正则文法的区别:
上下文无关文法允许递归调用，而正则文法(regular grammar)不允许
上下文无关文法比正则文法的表达能力更强，正则文法是上下文无关文法的一个子集

- 最左推导与最右推导
最左推导: 每次总是选择最左侧的符号进行替换。

=== 设计文法
- 文法能够描述程序设计语言的大部分(但不是全部)语法
例如，标识符必须先声明后使用，但这个要求不能通过一个上下文无关文法来描述。

- vs. 正则表达式
正则表达式适合用来词法分析/扫描，但它不够强大，无法处理任意深度嵌套的表达式。
任何能够使用正则表达式描述的，都可以使用文法描述。
但正则表达式规则更简单和易于理解，同时，将一个语言的语法结构分为词法和非词法两部分更容易将编译器前端模块化。

=== 自顶向下的语法分析
- 递归下降
对于一个非终结符，要从左到右依次匹配其产生式中的每个项，包括非终结符和终结符。
在匹配产生式右边的非终结符时，要下降一层，继续匹配该非终结符的产生式。
如果一个语法规则有多个可选的产生式，那么只要有一个产生式匹配成功就行。如果一个产生式匹配不成功，那就回退回来，尝试另一个产生式（回退，即回溯）。

- LL语法分析
https://en.wikipedia.org/wiki/LL_parser

- LL(1)文法
向前读入一个词素来决定选择哪个语法推导规则。
LL(1)中的第一个L，是Left-to-right的缩写，代表从左向右处理Token串。第二个L，是Leftmost，即最左推导。最左推导就是它总是先把产生式中最左侧的非终结符展开完毕以后，再去展开下一个。这也就相当于对AST从左子节点开始的深度优先遍历。LL(1)中的1，指的是预读一个Token。

- 优点

    容易手工实现
    解读:
        上级文法嵌套下级文法，上级的算法调用下级。
        表现在生成AST中，上级算法生成上级节点，下级算法生成下级节点。这也正是"下降"的含义。
        递归下降的特点是: 程序结构基本上是跟文法规则同构的，比较直观。
    代码结构与文法对应
    易理解，容易增加错误处理和错误恢复

- 缺点

    限制较多
    需要解决左递归问题

- Examples
antlr4
https://en.wikipedia.org/wiki/Top-down_parsing#Examples
Recursive descent parser: https://en.wikipedia.org/wiki/Recursive_descent_parser
Pratt Parsing(也称Top Down Operator Precedence Parsing):
https://en.wikipedia.org/wiki/Operator-precedence_parser#Pratt_parsing

- 参考
https://en.wikipedia.org/wiki/Top-down_parsing

=== LL算法
==== 实现方式
用 LL 算法解析语法的时候，可以选择两种实现方式:
第一种，采用递归下降算法，通过预读能判断出选择哪个产生式。
第二种，采用表驱动的方式: 需要基于计算出来的 First 和 Follow 集合构造一张预测分析表，根据这个表，查找在遇到 Token 应该走哪条路径。

这两种方式是等价的。

LL(1)算法解决的问题是: 如何通过只向前看1个词法单元来唯一确定与当前句子匹配的产生式。
LL(1)算法对于文法的要求是: 在向前看1一个词法单元下无推导二义性。

==== 左递归问题
===== 什么是左递归问题

    存在某非终结符号A，最终会推导出来的句型(sentential form)里面包含以自己为最左符号(left-symbol)的句型

    直接左递归(immediate left recursion):
        例如A -> Aa | b

    间接左递归(indirect left recursion):
        A -> Ba | C
        B -> Ab | D
        可能产生 A -> Ba -> Aba 这种生成

    multiplicativeExpression
        :   IntLiteral
        |   multiplicativeExpression Star IntLiteral
        ;
    https://en.wikipedia.org/wiki/Left_recursion

    注意，并不是所有的算法都不能处理左递归，对于另外一些算法如LR，处理左递归是没有问题的

    结合性是跟左递归还是右递归有关的，左递归导致左结合，右递归导致右结合。

===== 解决: 转化为循环

    左递归可以通过改写语法规则来避免，
    而改写后的语法又可以表达成简洁的EBNF格式，从而说明可以使用循环代替右递归。

    multiplicativeExpression
        :   IntLiteral
        |   IntLiteral Star multiplicativeExpression
        ;

    https://en.wikipedia.org/wiki/Left_recursion#Removing_left_recursion

===== 分析

    BNF形式:
        add ::= mul | add + mul
        mul ::= pri | mul * pri
        pri ::= Id | Num | (add)

    由加法规则推导乘法规则，保证了AST中的乘法节点一定会在加法节点的下层，也就保证了乘法计算优先于加法计算。

    转化为右递归，但存在右结合的问题:
        add -> mul add'
        add' -> + mul add' | ε
    结合性是跟左递归还是右递归有关的，左递归导致左结合，右递归导致右结合。

    转化为EBNF: add -> mul (+ mul)*，进而转化为循环的形式:
        mul()
        while (next token == '+') {
            mul()
            createAddNode()
        }

==== 回溯问题
解决: 预读后续的一个Token，判断该选择哪个产生式

==== First集合
- 目的

- 计算

- 注意事项
尽量抽取左公因子，这样可以避免 First 集合产生交集。

==== Follow集合
- 目的

- 计算

==== First与Follow的运用

==== 参考
https://zhuanlan.zhihu.com/p/675095121

=== 自底向上的语法分析
- vs. 自顶向下
https://stackoverflow.com/questions/4316385/why-is-bottom-up-parsing-more-common-than-top-down-parsing
https://qntm.org/top

- LR语法分析
https://en.wikipedia.org/wiki/LR_parser
LR 算法是一种自底向上的算法，它能够支持更多的语法，且没有左递归的问题。
第一个字母 L，与 LL 算法的第一个 L 一样，代表从左向右读入程序。第二个字母 R，指的是 RightMost（最右推导），也就是在使用产生式的时候，是从右往左依次展开非终结符。
例如，对于add->add+mul这样一个产生式，是优先把 mul 展开，然后再是 add。

- 优点

    LR语法比LL普适，能够避免左递归问题
    速度快、表达能力强

- 缺点

    不那么直观，不太容易理解
    手工构造LR分析器的工作量非常大，因此一般会使用工具例如yacc/bison

- SLR(1): simple LR(1)
https://en.wikipedia.org/wiki/Simple_LR_parser

- LALR(1): look ahead LR(1)
https://en.wikipedia.org/wiki/LALR_parser

- Examples
bison
https://en.wikipedia.org/wiki/Bottom-up_parsing#Examples
Operator-precedence parser: https://en.wikipedia.org/wiki/Operator-precedence_parser

- 参考
https://en.wikipedia.org/wiki/Bottom-up_parsing

=== LR算法
==== vs. LL算法
https://stackoverflow.com/questions/4092280/what-advantages-do-ll-parsers-have-over-lr-parsers

==== 移进(shift)与规约(reduce)
LR算法通过移进-规约方法逐步构造AST完成语法解析。

冲突:
shift/reduce conflict
reduce/reduce conflict

==== LR(0)

==== SLR(k)
k表示需要在Token队列里预读k个Token。

==== LALR(k)
Look Ahead Left to Right
bison常用的分析技术
https://stackoverflow.com/questions/19663564/what-is-the-difference-between-lalr-and-lr-parsing
https://stackoverflow.com/questions/2676144/what-is-the-difference-between-lr-slr-and-lalr-parsers

==== LR(k)

==== GLR
https://en.wikipedia.org/wiki/GLR_parser
https://stackoverflow.com/questions/2129532/glr-parsing-algorithm-resources

GLR可以认为是允许无限制地往前查看。

bison可以通过包含%glr-parser声明来让bison创建通用LR(Generalized LR, GLR)语法分析器。
当GLR语法分析器遇到冲突时，理论上来说它会分裂出并行的两种可能的分析，每种分析会消耗其对应的记号。如果有更多的冲突，它可以创建一棵部分语法分析的树，在每次冲突时进一步分裂。在分析结束时，要么仅存活一种分析，其它分析由于不能匹配剩余的输入而被放弃，要么在语法确实有歧义的情况下存活多种分析，而这时就需要决定如何处理它们。

=== 二义性文法

=== 语法分析器生成工具

== 语义分析
=== 概念
语义分析(semantic analyzer):
使用语法树和符号表中的信息来检查源程序是否和语言定义的语义一致，消除语义模糊，同时也收集类型信息，并把这些信息放在语法树或符号表中，以便在之后的中间代码生成过程中使用。
语义分析将获得的一些信息（如引用消解信息、类型信息等），附加到AST上，这样的AST叫做带有标注信息的AST（Annotated AST/Decorated AST）。

语义分析也称为类型检查、上下文相关分析，负责检查程序(语法树)的上下文相关的属性。
语义分析的本质，就是针对上下文相关的情况做处理。

在语义分析过程中，会使用两个数据结构:
(1) AST，会把语义分析时获得的一些信息标注在AST上，形成带有标注的AST。
(2) 符号表，用来记录程序中声明的各种标识符，并用于后续各个编译阶段。

符号表(symbol table):
不仅仅是语义分析阶段会用到符号表，其它编译阶段也会用到: 在词法分析阶段，就可以为符号表建立条目；在生成IR、做优化和生成目标代码的时候，也会用到符号表里的信息。

symbol:
There are two fundamental operations for symbol validation: defining symbols and resolving symbols. Defining a symbol means adding it to a scope. Resolving a symbol means figuring out which definition the symbol refers to. In some sense, resolving a symbol means finding the “closest” matching definition.

scope:
If a programming language allows the same identifier to mean different things in different contexts, the symbol table groups symbols into scopes. A scope is just a set of symbols such as a list of parameters for a function or the list of variables and functions in a global scope.

=== 语义规则
大多数语言的语义规则都是采取自然语言描述的，是人为规定的，编译器的实现者必须对语言中的语义规定有全面的理解。

语义规则可以分为两大类:
第一类规则与上下文有关。
第二类规则与类型有关。在做类型分析的时候，会用属性计算。某些与类型有关的处理工作，还需要放到运行期去处理，例如多态的情况下调用一个方法时，到底要采用哪个子类的实现，只有在运行时才会知道。

=== 上下文相关的分析处理
==== 控制流检查
例如return、break和continue等语句，都与程序的控制流有关，它们必须符合控制流方面的规则。

==== 引用消解
引用消解（Reference Resolution），有时也被称作名称消解（Name Resolution）或者标签消解（Label Resolution）。
对变量名称、常量名称、函数名称、类型名称、包名称等的消解，都属于引用消解。
在做引用消解的时候，为了更好地查找变量、类型等定义信息，编译器会使用一个辅助的数据结构：符号表。

==== 闭包分析
要正确地使用闭包，就必须在编译期知道哪些变量是自由变量。这里的自由变量是指在本函数外面定义的变量，但被这个函数中的代码所使用。这样，在运行期，编译器就会用特殊的内存管理机制来管理这些变量。

=== 类型系统
==== 概念
以表达式x = y + 6为例:

- 类型推导(Type Inference)
如果 y 是浮点类型，y + 6 的结果也是浮点型。
如果 y 是字符串类型，且语言允许执行 + 号运算即字符串拼接，则y + 6 的结果也是字符串型。
这个过程，就是类型推导。

- 类型检查(Type Checking)
当右边的值计算完，赋值给 x 的时候，检查左右两边的类型是否匹配的过程，即类型检查。

- 类型转换(Type Conversion)
如果 x 的类型是浮点型，而右边传过来的是整型，则需要进行类型转换。

==== 属性文法
attribute grammer
以上下文无关文法为基础，扩充了:
每个文法符号(终结符或非终结符)有"值"属性
每个产生式有一组属性的语义规则，对属性进行计算和传递

例如，对 value 属性进行计算的属性文法:

    add1 → add1 + mul [ add1.value = add2.value + mul.value ]
    add → mul [ add.value = mul.value ]
    mul1 → mul2 * primary [ mul1.value = mul2.value * primary.value ]
    mul → primary [ mul.value = primary.value ]
    primary → "(" add ")" [ primary.value =  add.value ]
    primary → integer [ primary.value = strToInt(integer.str) ]

==== S属性与I属性
S属性(Synthesized Attribute，综合出来的属性):
从子节点计算而来

I属性(Inherited Attribute，继承到的属性):
由父节点或者兄弟节点计算而来

计算出来的属性，可以标注在AST上，带有标注信息的AST，也被称为Annotated Tree/Decorated Tree/Attributed Tree(虽然叫法有很多，但都是一个含义，即向AST中添加了语义信息)。

注意，这些属性在数据结构上，并不一定是AST节点的属性，只是在概念上，这些属性还是标注在树节点上的。

==== 语法制导的翻译
在语法规则上附加一系列动作（这种语义规则的定义叫做语法制导的定义: Syntax directed definitio即SDD），在解析语法的时候执行这些动作的方式，是一种编译方法，龙书里有一个专门的名字，叫做语法制导的翻译（Syntax Directed Translation，SDT）。
使用语法制导的翻译可以做很多事情，包括计算属性、填充符号表，以及生成IR。

== 中间代码的生成与优化
=== 概念
IR(Intermediate Representation)，中间代码，也称Intermediate Code即IC

中间代码的主要两种形式：抽象语法树与三地址指令。

三地址指令:
https://en.wikipedia.org/wiki/Three-address_code

作用:
生成代码的时候，需要做大量的优化工作。很多优化工作没有必要基于汇编代码来做，而是可以基于IR，使用统一的算法来完成。
其次，很多解释型的语言，可以直接执行IR，例如java，如此一来，编译器生成IR以后就完成任务，没有必要生成最终的汇编代码了。

=== SSA
Static Single Assignment，静态单赋值，它要求一个变量只能被赋值一次。
https://en.wikipedia.org/wiki/Static_single-assignment_form

SSA体现了精确的使用-定义(use-def)关系，并且由于变量的值定义出来以后就不再变化，使得基于SSA更容易运行一些优化算法。

现代语言用于优化的IR，很多都是基于SSA的了，包括Java的JIT编译器、JavaScript的V8编译器、Go语言的gc编译器、以及LLVM工具等。

=== CPS
https://en.wikipedia.org/wiki/Continuation-passing_style
https://matt.might.net/articles/by-example-continuation-passing-style/

== 目标代码的生成与优化
=== 概念
编译器首先为中间表示选择对应目标机器平台上的指令集架构并进行初步翻译。在此基础上，完成寄存器分配并尝试进行目标代码优化等工作。

=== 指令选择
=== 寄存器分配
=== 指令重排序

== 工具
=== antlr
==== 概览
全名: ANother Tool for Language Recognition

- vs. flex-and-bison
https://tomassetti.me/why-you-should-not-use-flex-yacc-and-bison/
https://stackoverflow.com/questions/212900/advantages-of-antlr-versus-say-lex-yacc-bison

==== 文档
https://github.com/antlr/antlr4/blob/master/doc/index.md
左递归: https://github.com/antlr/antlr4/blob/master/doc/left-recursion.md

==== grammars-v4
https://github.com/antlr/grammars-v4/

The grammar is kept application independent and programming language neutral.

规则文件以.g4 结尾，词法规则和语法规则可以放在同一个文件里，也可以分成两个文件这样更清晰。
每个词法规则都是大写字母开头，而语法规则是以小写字母开头的。

==== Parse-Tree Listeners and Visitors
监听器和访问者机制将语法与应用程序代码分离开来，可以不在语法中嵌入操作(代码)的情况下构建语言应用程序。
这种解耦可以很好地封装应用程序，而不是将其分割并分散到语法中。
没有嵌入式动作，就可以在不同的应用中重复使用相同的语法，而无需甚至无需重新编译生成的解析器，还能为同一语法生成不同编程语言的解析器。

org.antlr.v4.runtime.tree.ParseTreeListener
vistor: antlr4 -no-listener -visitor xyz.g4

The biggest difference between the listener and visitor mechanisms is that listener methods are called by the ANTLR-provided walker object, whereas visitor methods must walk their children with explicit visit calls.
Visitors work very well if we need application-specific return values because we get to use the built-in Java return value mechanism. If we prefer not having to explicitly invoke visitor methods to visit children, we can switch to the listener mechanism. Unfortunately, that means giving up the cleanliness of using Java method return values.
ANTLR generates listener event methods that return no values (void return types). To return values to listener methods executing on nodes higher in theparse tree, we can store partial results in a field of our listener. Eg. Simulating Return Values with a Stack

==== Precedence, Left Recursion, and Associativity
- 优先级
    声明顺序

- 结合性
    <assoc=right>

- 左递归
ANTLR v4’s major improvements is that it can now handle direct left recursion.
注意:
尽管ANTLR v4能处理直接左递归，但不能处理间接左递归，例如:

    expr : expo // indirectly invokes expr left recursively via expo
    | ...
    ;
    expo : expr '^'<assoc=right> exp

- 左递归的消除

==== Labeling Rule Alternatives for Precise Event Methods

    e : e MULT e # Mult
    | e ADD e # Add
    | INT # Int
    ;

    Now ANTLR generates a separate listener method for each alternative of e.
    Consequently, we don’t need the op token label anymore.
    For alternative label X, ANTLR generates enterX() and exitX().

    public interface LExprListener extends ParseTreeListener {
        void enterMult(LExprParser.MultContext ctx);
        void exitMult(LExprParser.MultContext ctx);
        void enterAdd(LExprParser.AddContext ctx);
        void exitAdd(LExprParser.AddContext ctx);
        void enterInt(LExprParser.IntContext ctx);
        void exitInt(LExprParser.IntContext ctx);
        ...
    }

==== Sharing Information Among Event Methods
- Traversing Parse Trees with Visitors

- Simulating Return Values with a Stack

- Annotating Parse Trees

    e returns [int value]
        : e '*' e # Mult
        | e '+' e # Add
        | INT # Int
        ;

    ParseTreeProperty

==== Error Reporting and Recovery
BaseErrorListener

java org.antlr.v4.runtime.misc.TestRig $grammar $startrule -diagnostics
PredictionMode.LL_EXACT_AMBIG_DETECTION

DefaultErrorStrategy

==== Attributes and Actions
Grammars without actions are easier to read, aren’t tied to a particular target language, and aren’t tied to a specific application.

embedded actions can be useful for three reasons.
• Simplicity: Sometimes it’s easier just to stick in a few actions and avoid creating a tree listener or visitor.
• Efficiency: In resource-critical applications, we might not want to waste the time or memory needed to build a parse tree.
• Predicated parsing: In rare cases, we can’t parse properly without referencing data collected previously in the input stream. Some grammars need to build up a symbol table and then recognize future input differently, depending on whether an identifier is, say, a type or a method.

==== Altering the Parse with Semantic Predicates

==== 源码
https://github.com/antlr/antlr4

==== 示例
https://github.com/antlr/grammars-v4
https://github.com/antlr/grammars-v4/tree/master/sql
https://github.com/mysql/mysql-workbench/tree/8.0/library/parsers/grammars

==== 工具
ANTLRWorks: https://www.antlr3.org/works/

==== 命令与选项
antlr命令: java org.antlr.v4.Tool命令的别名
grun 命令: java org.antlr.v4.gui.TestRig命令的别名

指定包名:

    antlr.bat -package org.orient LuaLexer.g4 LuaParser.g4
    或者在g4文件里定义:
    @header {
        package org.orient;
    }

==== 参考
《The Definitive ANTLR 4 Reference》
https://www.antlr.org/
http://staff.ustc.edu.cn/~yuzhang/compiler/2021f/lectures/05antlr.pdf

=== flex and bison
==== 简介
lex/yacc发展为flex/bison，POSIX的lex与POSIX的yacc基本上是flex与bison的子集。

flex是一个快速词法分析生成器，它可以将用户用正则表达式写的分词匹配模式构造成一个有限状态自动机(DFA)(一个C函数)。

Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1), IELR(1) or canonical LR(1) parser tables.

bison规范与flex规范一样由三部分组成:
第一部分是定义部分，处理语法分析器的控制信息，建立分析器操作所需要的执行环境。
第二部分包含语法分析器的规则。
第三部分则是C代码，它们会袚逐字拷贝到生成的C程序中去。

==== Q&A
有没有不适合使用flex/lex作为词法分析器的语言?
https://www.zhihu.com/question/

$$, $1, $2, ...等的含义？
$$指冒号左边的值，$1, $2指冒号后第一个值，第二个值...

默认动作？
{$$ = $1}

==== 示例
flex:  https://pandolia.net/tinyc/ch8_flex.html
bison: https://pandolia.net/tinyc/ch13_bison.html

https://github.com/stardust95/TinyCompiler

==== flex
===== 结构
https://westes.github.io/flex/manual/Format.html#Format

flex程序包含三个部分，各部分之间通过仅有%%的行来分割。
第一部分包含声明和选项设置；
第二个部分是一系列的模式和动作；
第三部分则是会被拷贝到生成的词法分析器里面的C代码，通常是一些与动作代码相关的例程。

===== 上下文相关性
左上下文相关:
三种方法: 特殊的行首模式字符、起始状态、显式代码。

右上下文相关:
三种方法: 特殊的行尾模式字符、斜线操作符、yyless()。

===== 参考
https://westes.github.io/flex/manual/

==== bison
===== Parser Algorithm
GLR: https://www.gnu.org/software/bison/manual/bison.html#GLR-Parsers
https://www.gnu.org/software/bison/manual/bison.html#Algorithm

===== 参考
https://www.gnu.org/software/bison/manual/bison.html

==== 源码
flex:  https://github.com/westes/flex
bison: https://github.com/akimd/bison

==== 参考
《flex与bison》中文版 第二版
https://epaperpress.com/lexandyacc/download/flex.pdf
https://www.gnu.org/software/bison/manual/bison.pdf
https://web.iitd.ac.in/~sumeet/flex__bison.pdf

=== AST
java org.antlr.v4.runtime.misc.TestRig $grammar $startrule -tree
java org.antlr.v4.runtime.misc.TestRig $grammar $startrule -gui

clang -ast-dump: https://clang.llvm.org/docs/IntroductionToTheClangAST.html

https://github.com/fkling/astexplorer

Javascript: https://esprima.org/demo/parse.html

=== LLVM
https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/index.html

=== ASM(JAVA字节码工具)
https://asm.ow2.io/

=== lemon
http://www.hwaci.com/sw/lemon/
https://www.sqlite.org/lemon.html
https://github.com/sqlite/sqlite/blob/master/tool/lemon.c

=== jison
https://github.com/zaach/jison
https://gerhobbelt.github.io/jison/docs/
EBNF: https://gist.github.com/zaach/1659274
https://npm-compare.com/chevrotain,jison,nearley,peggy

=== lua-parser
https://github.com/andremm/lua-parser

=== peg相关
https://github.com/roberto-ieru/LPeg
https://github.com/pegjs/pegjs
https://brynne8.github.io/blog/post/lpeg-and-peg-practices.html

=== 小结
词法分析的工具通常以DFA为主
语法分析的工具通常以LR(1), LALR(1)等为主，不过像antlr也在使用LL(*)
参考: https://en.wikipedia.org/wiki/Comparison_of_parser_generators

== 语言特性
=== 垃圾回收

=== 异常

=== 面向对象

=== 函数式语言

=== CPS(Continuation-Passing Style)
https://en.wikipedia.org/wiki/Continuation-passing_style

== project

=== json
https://www.zhihu.com/question/24640264

=== PeachCompiler
C语言编写, 实现C语言子集

https://www.bilibili.com/video/BV1Jy4y1d7nq/
p1-p4: 介绍与工程
p5-p22: 词法分析
p23-p101: 语法分析

https://github.com/nibblebits/PeachCompiler

=== Tiny C Compiler(Fabrice Bellard)
https://bellard.org/tcc/
https://bellard.org/tcc/tcc-doc.html

=== A small C compiler
https://www.sigbus.info/compilerbook
https://github.com/rui314/chibicc

=== lcc
https://github.com/drh/lcc

=== Writing a C Compiler
OCaml语言编写
https://norasandler.com/archive/
https://github.com/nlsandler/nqcc

=== write-a-C-interpreter
https://github.com/rswier/c4
https://github.com/lotabout/write-a-C-interpreter

=== 自己动手用java写编译器
https://study.163.com/course/courseLearn.htm?courseId=1002830012#

=== The TINY Sample Language
C语言编写，实现类Pascal子集
http://www.cs.sjsu.edu/~louden/cmptext/loucomp.zip

=== Cb
java语言编写，C的变化子集
https://www.ituring.com.cn/book/1308 随书下载

=== PL
js编写 一个示例语言
https://lisperator.net/pltut/

=== program transformation
https://en.wikipedia.org/wiki/Program_transformation

==== dynamic2static
https://www.reddit.com/r/Compilers/comments/1cmyuhm/add_static_type_to_a_dynamically_typed_language/

lua2csharp:
https://stackoverflow.com/questions/2184510/easily-porting-lua-code-to-c-sharp

=== 参考
http://scheme2006.cs.uchicago.edu/11-ghuloum.pdf[An Incremental Approach to Compiler Construction]

== 参考
《Compiler Construction Principles and Practice》(编译原理及实践)
《Language Implementation Patterns》
《Engineering a Compiler》2nd (编译器设计)
《Compilers, Principles, Techniques and Tools》2nd (龙书)
《Modern Compiler Implementation in C》(虎书)
《Advanced Compiler Design and Implementation》(鲸书)
《Optimizing compilers for modern architectures - a dependency based approach》(现代体系结构的优化编译器)
编译原理 华保健 https://www.bilibili.com/video/BV1m7411d7iS/
http://web.stanford.edu/class/cs143/
Stanford CS143: https://www.bilibili.com/video/BV17K4y147Bz
Harvard CS153: https://www.bilibili.com/video/BV1cT4y197hW
https://www.cs.cmu.edu/afs/cs/academic/class/15745-s19/www/lectures/
《编译原理之美》 https://time.geekbang.org/column/intro/100034101
    配套代码: https://github.com/RichardGong/PlayWithCompiler/tree/master/playscript-java
《编译原理实战课》 https://time.geekbang.org/column/intro/100052801
《Introduction to the Theory of Computation》3rd (计算理论导引) 正则表达式，自动机，上下文无关文法等相关理论
http://staff.ustc.edu.cn/~bjhua/courses/compiler/2014/
https://compilers.iecc.com/crenshaw/
《crafting-interpreters》
https://github.com/aalhour/awesome-compilers