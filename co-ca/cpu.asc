= CPU
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:homepage: http://orientye.com

<<<

== 概览

=== 概念
- 时钟周期
一个完整的脉冲（从高到低再到高）所花费的时间，就是一个时钟周期。它是 CPU 工作的最基本、最小的时间单位。

- CPU频率
CPU频率指的是CPU内核工作时的时钟频率，也就是CPU每秒内能产生多少个同步脉冲信号，单位是赫兹（Hz），通常以兆赫（MHz）或吉赫（GHz）为单位。它表示CPU的运行速度，频率越高，CPU每秒处理的脉冲数量就越多，通常意味着更快的运算能力，但整体性能也受到其他因素（如架构、缓存、核心数）的影响。

=== 流水线(pipelining)
指令是如何被执行的？首先，它被取出，然后被解码，接着由相应的功能单元执行，最后结果被写入到相应的地方。
现代处理器在一个流水线中将这些阶段重叠，就像装配线一样。当一条指令在执行时，下一条指令正在被解码，而后面的那条指令正在被取出。

每个流水线阶段都包含组合逻辑电路，并可能访问寄存器组和/或某种形式的高速缓存存储器。流水线阶段之间通过锁存器进行分隔。统一的时钟信号对各级锁存器进行同步控制，使得所有锁存器能够同时捕获各流水线阶段产生的处理结果。实际上，正是时钟信号驱动着指令在流水线中逐级传递。
在每个时钟周期开始时，承载着部分处理完成指令的数据和控制信息会被暂存于流水线锁存器中，这些信息将作为下一级流水线阶段逻辑电路的输入。在时钟周期运行期间，电信号会通过该阶段的组合逻辑电路进行传播，最终生成的输出结果将在周期结束时被下一级流水线锁存器精准捕获...

由于每条指令的执行结果在完成执行阶段后即可获取，后续指令理应能够立即使用该结果值，而无需等待该结果在写回阶段被提交到目标寄存器。为实现这一机制，需增设称为"旁路"的数据前向通路——这些通路沿着流水线逆向延伸。

流水线深度(pipeline depth):
Modern x86 CPU pipeline depth varies, but common architectures like Intel's Core series have around a 14-stage pipeline.
https://softwareengineering.stackexchange.com/questions/210818/how-long-is-a-typical-modern-microprocessor-pipeline

Since the clock speed is limited by (among other things) the length of the longest, slowest stage in the pipeline, the logic gates that make up each stage can be subdivided, especially the longer ones, converting the pipeline into a deeper super-pipeline with a larger number of shorter stages. Then the whole processor can be run at a higher clock speed! Of course, each instruction will now take more cycles to complete (latency), but the processor will still be completing 1 instruction per cycle (throughput), and there will be more cycles per second, so the processor will complete more instructions per second (actual performance)...

Today, modern processors strive to keep the number of gate delays down to just a handful for each pipeline stage, about 12-25 gates deep (not total!) plus another 3-5 for the latch itself, and most have quite deep pipelines...

=== 多发射与超标量(Multiple Issue - Superscalar)
发射宽度(issue width)

=== 超长指令集/超长指令字(VLIW)
- 概念
即Very Long Instruction Word: 在一条超长的指令中，包含了多个可以并行执行的操作（例如算术运算、内存访问、分支等）。处理器在单个时钟周期内，取出这条长指令，并将其中的多个操作分发到多个独立的功能单元（如ALU、FPU、加载/存储单元）中同时执行。

- 核心思想
将并行性从硬件转移到编译器。

- VLIW的工作原理

    编译时：编译器对高级语言代码进行极致优化。
        它进行指令级并行分析，找出没有数据依赖关系的操作。
        它将这些可以并行执行的操作组合在一起，形成一条VLIW指令。
        一条典型的VLIW指令可能看起来像这样：
        [ 操作码: ADD R1, R2, R3 | 操作码: LOAD R4, [R5] | 操作码: MUL R6, R7, R8 | 无操作 NOP ]
        这条指令告诉处理器，在同一周期内，ALU1执行加法，加载/存储单元执行内存读取，ALU2执行乘法。
        如果找不到足够的操作来填满所有槽位，编译器必须插入NOP（无操作）指令，这会浪费指令空间。
    执行时：
        取指：处理器从指令缓存中取出一条完整的VLIW长指令。
        译码：译码逻辑非常简单，几乎不需要动态分析，直接将指令的不同字段分发到对应的功能单元。
        执行：所有功能单元同时开始执行各自的操作。
        写回：将结果写回到寄存器堆。

- VLIW的优势
高性能潜力：通过编译器静态调度，理论上可以在一个周期内完成大量工作，避免了硬件动态调度的开销。
硬件简单，功耗低：移除了复杂的乱序执行逻辑、分支预测器和硬件调度器，使得芯片面积更小，功耗更低。这在嵌入式系统和移动设备领域非常有吸引力。
设计更可控：将复杂性转移到软件，使得硬件设计周期更短，验证更容易。

- VLIW的挑战与劣势
尽管理念先进，但VLIW在通用计算领域并未成为主流，主要是因为以下几个致命弱点：
编译器依赖性极强：VLIW的性能完全依赖于编译器的智能程度。一个“笨”的编译器会产生大量NOP，导致代码臃肿，性能低下。
代码密度问题：由于需要填充NOP，以及指令本身很长，导致编译后的二进制文件体积庞大，对指令缓存不友好。
二进制兼容性差：这是VLIW的“阿喀琉斯之踵”。
为某一代VLIW处理器（例如有4个ALU）编译的二进制代码，无法在功能单元数量不同的下一代处理器（例如有6个ALU）上高效运行。因为指令包中的操作槽位和延迟都硬编码在二进制里了。
这与x86/ARM架构强大的向后兼容性形成鲜明对比。
对缓存不命中敏感：在乱序执行CPU中，如果发生缓存不命中，CPU可以聪明地去执行后面不相关的指令来“填坑”。而在VLIW中，指令顺序是编译器静态安排的，一旦某个操作（如内存加载）停滞，整个指令包（以及后续依赖的包）都可能被阻塞，硬件难以动态绕过。
难以处理运行时不确定性：如指针别名、动态分支等，在编译时很难100%确定，这限制了编译器挖掘并行性的能力。

- VLIW的应用场景
尽管在通用CPU领域受挫（最著名的尝试是Intel Itanium/IA-64，但其并未达到预期成功），VLIW在特定领域依然大放异彩：
数字信号处理器：这是VLIW最成功的领域。TI（德州仪器）的C6000系列DSP、CEVA的DSP核心等都广泛采用VLIW架构。因为DSP处理的算法（如FIR滤波器、FFT）通常数据并行性高，流程规整，非常适合编译器进行静态调度。
图形处理器：现代GPU的着色器核心在某些方面与VLIW的思想有相似之处，它们擅长对大量并行线程（SIMD/SIMT）进行批处理。
嵌入式媒体处理器：一些专门用于视频编解码的芯片会采用VLIW架构来高效处理高度并行的媒体数据流。

=== 指令依赖与延迟(Instruction Dependencies & Latencies)
- 指令依赖
现代流水线处理器的核心问题在于指令依赖 - 后续指令需要前一条指令的结果才能继续执行。
依赖会导致延迟（等待时间），从而阻止处理器保持持续忙碌状态。

- 依赖主要有三种类型
    ** 数据依赖
        这是最常见也是最根本的类型。当一条指令需要前一条指令的计算结果时发生。
        示例：b = a * 2; 后跟 c = b + 1;
        第二条指令（c = b + 1）必须等待第一条指令（b = a * 2）计算出b的值。
        处理器必须保持程序顺序，即确保指令看起来是按照原始程序顺序执行的，以维护这种依赖关系。
    ** 名字依赖
        这种依赖不是真正的数据流需求，而是因为指令使用了相同的寄存器或内存位置（即"名字"）。
        它有两种子类型：
        *** 反依赖
            当一条指令读取一个值，后续指令覆盖（写入）该值时发生。
            例如，假设R1是两条指令使用的寄存器：b = R1 * 2; 后跟 R1 = c + 1; 第一条指令需要读取R1的旧值，而第二条指令会写入R1的新值。
            如果第二条指令在第一条指令读取旧值之前就覆盖了R1，就会出错。
        *** 输出依赖
            当两条指令写入同一个位置时发生。例如：R1 = a * 2; 后跟 R1 = c + 1; R1的最终值必须是第二条指令的结果。
            如果第二条指令先完成（例如由于乱序执行），那么R1的最终值将是错误的（来自第一条指令的旧值）。
    ** 控制依赖
        这与程序流程有关，即指令是否执行取决于前一条指令（通常是条件分支）的结果。
        例如，在 if (a == 0) { b = 1; } 中，b = 1 这条指令是否执行取决于 a == 0 比较的结果。

- 延迟
延迟是指令从开始到产生结果所需的时钟周期数。例如，一个典型的浮点加法可能有3个周期的延迟，这意味着使用该加法结果的指令必须等待2个空周期（"气泡"）才能开始。

- 解决
正是这些依赖及其导致的延迟，使得简单流水线无法达到每周期一条指令（IPC=1）的理想速率。为了克服这些限制，现代微处理器采用了复杂的技术，如乱序执行、寄存器重命名（用于消除名字依赖）和分支预测（用于减轻控制依赖）。

=== 分支预测(Branch Prediction)
预测正确：流水线没有任何停顿，性能无损。
预测错误：清空在错误路径上已经取入流水线的指令（这被称为“流水线冲刷”），然后从正确的地址重新开始取指。这会带来一个固定的惩罚周期（例如，浪费10-20个时钟周期）。

=== 通过谓词化消除分支(Eliminating Branches with Predication)
核心思想是：将控制依赖转换为数据依赖。
控制依赖：程序执行哪段代码，取决于一个条件判断的结果（if-else）。这会导致分支指令，CPU需要预测分支的方向，预测错误会带来巨大的性能惩罚。
数据依赖：代码的执行路径是固定的，但通过一个条件值（谓词）来决定指令的结果是否被提交。指令照常执行，但只有满足条件的指令才能影响最终状态。

    例如原始代码:
    if (a > b) {
        x = y + z;
    } else {
        x = y - z;
    }
    谓词化后的代码（无分支）:
    // 1. 计算条件（谓词）
    p = (a > b); // 谓词p为true（1）或false（0）
    q = !p;      // 反谓词
    // 2. 执行所有路径的代码，但用谓词作为“开关”
    temp1 = y + z; // if 路径的计算
    temp2 = y - z; // else 路径的计算
    // 3. 根据谓词，选择正确的结果
    x = p ? temp1 : temp2;

    条件选择指令：x = (p) ? temp1 : temp2; -> 对应 CMOV（Intel）或 CSEL（ARM）指令。
    这条指令没有分支，只是根据条件 p 选择两个寄存器中的一个作为结果。

优点：
消除分支预测错误惩罚。
简化控制流：使代码更线性，有利于指令缓存和静态调度。

缺点：
执行了多余的指令：无论条件如何，if 和 else 路径的指令都被执行了（尽管结果可能被丢弃）。
对副作用敏感：如果某条路径的代码有副作用（例如，if (p) { x++; }），谓词化会导致副作用总是发生，这改变了程序语义，因此不能优化。
寄存器压力可能增大：需要同时保存两个路径的临时结果。

=== 参考
https://www.lighterra.com/papers/modernmicroprocessors/

== Cache
=== 概念
- CPU缓存架构

- cache line(缓存行)

- cache coherency(缓存一致性)

- false sharing(伪共享)

    Q: 什么是伪共享，什么情况下会发生
    Q: 伪共享有什么问题
    Q: 如何避免伪共享
    A: local/align

- cache ping-pong

    False Sharing: 不同核心访问同一缓存行中的不同变量
    Cache Ping-Pong: 不同核心访问同一缓存行中的同一变量

=== Cache Structure
perfbook C2.1

=== Cache-Coherence Protocols
==== 概念
- 背景
在现代多核CPU中，每个核心通常都有自己的私有缓存（如L1、L2缓存）。当一个程序在运行时，它需要的数据可能会被复制到多个核心的缓存中。
这就引发了一个问题：如果其中一个核心修改了自己缓存里的数据，其他核心的缓存副本就会变成“过时”的，从而导致程序运行错误。
MESI协议就是为了解决这个问题而生的。它通过维护缓存行（Cache Line）的状态，并在不同核心之间进行通信，来保证所有缓存中的数据副本都是一致的。

- 机制
每个缓存行都有 M、E、S、I 四种状态，通过核心间的“总线嗅探”和消息传递来触发状态转换。

- 性能
MESI协议减少了对主内存的直接访问。例如，在 M 状态的数据只有在必要时才写回内存；在 S 状态的数据可以直接从其他缓存获取，这比访问内存快得多。

==== MESI States
MESI stands for “modified”, “exclusive”, “shared”, and “invalid”, the four states a given cache line can take on using this protocol. Caches using this protocol therefore maintain a two-bit state “tag” on each cacheline in addition to that line’s physical address and data.

- M - Modified (已修改)
    ** 当前缓存行中的数据已经被修改，与主内存中的数据不一致。
    ** 这个缓存是拥有该数据最新、唯一正确副本的缓存。
    ** 在某个时间点，这个被修改的数据必须被写回主内存。

- E - Exclusive (独占)
    ** 当前缓存行中的数据与主内存中的数据一致。
    ** 只有我这一个缓存拥有该数据的副本。
    ** 如果我要修改它，我可以直接进行，无需通知其他核心，因为我是唯一的拥有者。

- S - Shared (共享)
    ** 当前缓存行中的数据与主内存中的数据一致。
    ** 可能有多个缓存中都存在该数据的相同副本。
    ** 因为副本是共享的，所以我不能直接修改它。如果我想修改，必须先通知其他缓存，让它们的副本失效。

- I - Invalid (无效)
    ** 这个缓存行中的数据是过时的、不可用的。
    ** 它可能是因为其他核心修改了数据，导致本地的副本失效。
    ** 当CPU需要读取这个数据时，它不能使用这个无效的副本，而必须从其他缓存或主内存中重新获取。

==== MESI Protocol Messages
MESI 协议的消息，是缓存之间、缓存与主内存之间进行通信的语言，通过它们来协调状态的变化和数据的传输。
MESI 协议通过一个总线嗅探 机制工作，每个缓存控制器都监听总线上的所有消息。当它看到某个消息与它缓存的数据相关时，就会采取相应的行动。

一共6个消息:

Read(读):
The “read” message contains the physical address of the cache line to be read.

Read Response(读响应):
The “read response” message contains the data requested by an earlier “read” message. This “read response” message might be supplied either by memory or by one of the other caches. For example, if one of the caches has the desired data in “modified” state, that cache must supply the “read response” message.

Invalidate(使无效):
The “invalidate” message contains the physical address of the cache line to be invalidated. All other caches must remove the corresponding data from their caches and respond.

Invalidate Acknowledge(使无效应答):
A CPU receiving an “invalidate” message must respond with an “invalidate acknowledge” message after removing the specified data from its cache.

Read Invalidate(读使无效):
The “read invalidate” message contains the physical address of the cache line to be read, while at the same time directing other caches to remove the data.
Hence, it is a combination of a “read” and an “invalidate”, as indicated by its name. A “read invalidate” message requires both a “read response” and a set of “invalidate acknowledge” messages in reply.

Writeback(写回):
The “writeback” message contains both the address and the data to be written back to memory (and perhaps “snooped” into other CPUs’ caches along the way). This message permits caches to eject lines in the “modified” state as needed to make room for other data.

Q: Where does a writeback message originate from and where does it go to?
回写消息从何而来，去往何处？
回写消息可能源自某个特定的CPU，或者在某些设计中也可能源自某个CPU特定层级的缓存——甚至可能源自多个CPU共享的缓存。关键在于，某个缓存中已没有空间存放特定数据项，因此必须将其他数据驱逐出去以腾出空间。如果存在其他数据副本（位于其他缓存或内存中），那么可以直接丢弃该数据，无需发出回写消息。
然而，如果每个可能被驱逐的数据都已被修改，导致唯一的最新副本就在当前缓存中，那么就必须将这些数据项之一复制到其他地方。这个复制操作就是通过"回写消息"来完成的。
回写消息的目的地必须是能够存储新值的位置。这可能是主内存，但也可能是其他缓存。如果是其他缓存，通常会是同一CPU的更高层级缓存（例如，一级缓存可能回写到二级缓存）。不过，有些硬件设计允许跨CPU的回写操作，因此CPU 0的缓存可能会向CPU 1发送回写消息。这通常发生在CPU 1以某种方式表明了对该数据的兴趣时（例如，最近曾发出过读取请求）。
简而言之，回写消息从系统中空间不足的组件发出，由系统中能够容纳该数据的其他组件接收。

Q: What happens if two CPUs attempt to invalidate the same cache line concurrently?
One of the CPUs gains access to the shared bus first, and that CPU “wins”. The other CPU must invalidate its copy of the cache line and transmit an “invalidate acknowledge” message to the other CPU.
Of course, the losing CPU can be expected to immediately issue a “read invalidate” transaction, so the winning CPU’s victory will be quite ephemeral.

Q: When an “invalidate” message appears in a large multiprocessor, every CPU must give an “invalidate acknowledge” response. Wouldn’t the resulting “storm” of “invalidate acknowledge” responses totally saturate the system bus?
如果大规模多处理器确实以这种方式实现，这种情况确实可能发生。但大型多处理器（特别是NUMA架构机器）往往采用所谓的"基于目录"的缓存一致性协议，正是为了避免此类问题及其他相关问题。

Q: If SMP machines are really using message passing anyway, why bother with SMP at all?
既然现实中SMP机器总是会在使用消息传递，那为何还要大费周章地发展SMP技术？
过去几十年来，关于这一点确实存在一些争论。一种解释是：缓存一致性协议本身相当简洁，因此能够直接通过硬件实现，从而获得软件消息传递无法企及的带宽与延迟优势。另一种观点认为，真正原因在于经济性-大型SMP设备与小型SMP集群的成本差异决定了技术路线。亦有见解指出SMP编程模型比分布式系统更易用。但反对声音会举出HPC集群与MPI的成功案例。这场争论，至今仍在持续。

==== MESI State Diagram
12种状态转换

- (a) M->E
A cache line is written back to memory, but the CPU retains it in its cache and further retains the right to modify it. This transition requires a “writeback” message.
缓存行被写回内存，但CPU仍将其保留在缓存中，并保留修改权限。这一状态转换需要发送“写回”消息。

- (b) E->M
The CPU writes to the cache line that it already had exclusive access to. This transition does not require any messages to be sent or received.
CPU对其已有独占权的缓存行执行写入操作。此状态转换无需发送或接收任何消息。

- (c) M->I
The CPU receives a “read invalidate” message for a cache line that it has modified. The CPU must
invalidate its local copy, then respond with both a “read response” and an “invalidate acknowledge” message, both sending the data to the requesting CPU and indicating that it no longer has a local copy.
当CPU收到针对其已修改缓存行的"读无效"消息时，必须执行以下操作：首先使本地副本失效，随后同时发出"读响应"和"无效确认"消息：将数据传送至请求方CPU，表明自身不再持有本地副本。

- (d) I->M
The CPU does an atomic read-modify-write operation on a data item that was not present in its cache. It transmits a “read invalidate”, receiving the data via a “read response”. The CPU can complete the transition once it has also received a full set of “invalidate acknowledge” responses.
CPU对其缓存中不存在的数据项执行原子性的读写修改操作。它首先发出“读无效”指令，并通过“读响应”接收数据。只有在收到完整的“无效确认”响应集合后，CPU才能完成该状态转换。

- (e) S->M
The CPU does an atomic read-modify-write operation on a data item that was previously read-only in its cache. It must transmit “invalidate” messages, and
must wait for a full set of “invalidate acknowledge” responses before completing the transition.
CPU对其缓存中原本处于只读状态的数据项执行原子性读写修改操作。它必须发送"无效"消息，并且需要收齐所有"无效确认"响应后才能完成状态转换。

- (f) M->S
Some other CPU reads the cache line, and it is supplied from this CPU’s cache, which retains a readonly copy, possibly also writing it back to memory.
This transition is initiated by the reception of a “read” message, and this CPU responds with a “read response” message containing the requested data.
当其他CPU读取该缓存行时，本cpu必须以其local cacheline的数据回应，并以read response回应之前总线上的read请求。这时候该cacheline状态从Modified状态变成shared状态（有可能也会进行写回的动作）。

- (g) E->S
Some other CPU reads a data item in this cache line, and it is supplied either from this CPU’s cache or from memory. In either case, this CPU retains a readonly copy. This transition is initiated by the reception of a “read” message, and this CPU responds with a “read response” message containing the requested data.
当其他CPU读取此缓存行中的数据项时，数据可能由此CPU的缓存提供，也可能由内存提供。无论哪种情况，此CPU都会保留一份只读副本。该状态转换由接收到的"读取"消息触发，此CPU会返回包含所请求数据的"读取响应"消息。
与(f)不同，不存在写回的问题。

- (h) S->E
This CPU realizes that it will soon need to write to some data item in this cache line, and thus transmits an “invalidate” message. The CPU cannot complete the transition until it receives a full set of “invalidate acknowledge” responses, indicating that no other CPU has this cacheline in its cache. In other words, this CPU is the only CPU caching it.
此CPU预判其不久将需要修改该缓存行中的某个数据项，因而主动发出"无效"消息。在收到完整的"无效确认"响应集之前，CPU无法完成状态转换，一旦收到所有的响应，表明其他CPU的缓存中均已不存在该缓存行。换言之，此时该CPU已成为唯一缓存此数据的处理器。

- (i) E->I
Some other CPU does an atomic read-modify-write operation on a data item in a cache line held only in this CPU’s cache, so this CPU invalidates it from its cache. This transition is initiated by the reception of a “read invalidate” message, and this CPU responds with both a “read response” and an “invalidate acknowledge” message.
当其他CPU对仅存在于本CPU缓存中的缓存行内的数据项执行原子性读-修改-写操作时，本CPU会将该缓存行从自身缓存中置为无效。此状态转换由接收到“读-无效”消息触发，本CPU将同时回复“读响应”与“无效确认”消息作为响应。

- (j) I->E
This CPU does a store to a data item in a cache line that was not in its cache, and thus transmits a “read invalidate” message. The CPU cannot complete the transition until it receives the “read response” and a full set of “invalidate acknowledge” messages. The cache line will presumably transition to “modified” state via transition (b) as soon as the actual store completes.
本CPU对不在自身缓存中的缓存行数据项执行存储操作，因此发送“读-无效”消息。该CPU必须接收到“读响应”及完整的(所有其他cpu的)“无效确认”消息集合后才能完成状态转换。在实际存储操作完成后，该缓存行预计将通过(b)转换路径进入“已修改”状态。

- (k) I->S
This CPU loads a data item in a cache line that was not in its cache. The CPU transmits a “read” message, and completes the transition upon receiving the corresponding “read response”.
本CPU对不在自身缓存中的缓存行执行数据加载操作，因此发送"读"请求消息，之后接收到其他的cpu local cache或者memory的"读响应"消息，于是完成此状态转换，即将该cacheline从Invalid状态迁移到shared状态。

- (l) S->I
Some other CPU does a store to a data item in this cache line, but holds this cache line in read-only state due to its being held in other CPUs’ caches (such as the current CPU’s cache). This transition is initiated by the reception of an “invalidate” message, and this CPU responds with an “invalidate acknowledge” message.
当cacheline处于shared状态时，说明在多个cpu的local cache中存在只读副本，一旦其中一个cpu想要执行数据写入操作，必须先通过invalidate获取该数据的独占权，而其他的CPU会以invalidate acknowledge回应，并将其cacheline从shared状态修改成invalid状态。

Q: How does the hardware handle the delayed transitions described above?
A: Usually by adding additional states, though these additional states need not be actually stored with the cache line, due to the fact that only a few lines at a time will be transitioning. The need to delay transitions is but one issue that results in real-world cache coherence protocols being much more complex than the over-simplified MESI protocol described in this appendix. Hennessy and Patterson’s classic introduction to computer architecture [HP95](即《Computer Architecture: A Quantitative Approach》) covers many of these issues.

==== MESI Protocol Example
perfbook C2.4

Q: What sequence of operations would put the CPUs’ caches all back into the “invalid” state?
A: There is no such sequence, at least in absence of special “flush my cache” instructions in the CPU’s instruction set. Most CPUs do have such instructions.
问：需要怎样的操作序列才能让所有CPU缓存回到“无效”状态？
答：至少在没有CPU指令集提供的特殊“清空缓存”指令时，不存在这样的操作序列。目前大多数CPU都配备了这类专用指令。

== 参考
《Computer Organization and Design: The Hardware/Software Interface》4th 中: 计算机组成与设计硬件/软件接口
《Computer Architecture: A Quantitative Approach》6th 中: 计算机体系结构：量化研究方法
《Modern Processor Design: Fundamentals of Superscalar Processors》中: 超标量处理器基础
《Performance Analysis and Tuning on Modern CPUS》2nd: https://github.com/dendibakh/perf-book