= 了解Linux性能优化
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:homepage: http://orientye.com

<<<

== 概述
=== 性能指标
https://github.com/orientye/understand/blob/main/high/%E4%BA%86%E8%A7%A3concurrency/high-concurrency.asc#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87

=== 理论延迟
In 2020:
    L1 cache reference: 1 ns
    L2 cache reference: 4 ns
    Branch mispredict: 3 ns
    Mutex lock/unlock: 17 ns
    Main memory reference: 100 ns
    Compress 1K bytes with Zippy: 2000 ns
    Send 2K bytes over commodity network: 44 ns
    Read 1 MB sequentially from memory: 3000 ns
    Round trip within same datacenter: 500,000 ns
    Disk seek: 2,000,000 ns
    Read 1 MB sequentially from disk: 825,000 ns
    Read 1 MB sequentially from SSD: 49000 ns

https://colin-scott.github.io/personal_website/research/interactive_latency.html

== cpu

[format="csv", options="header", separator=#]
|===
指标#工具#说明
平均负载  #  top;uptime;cat /proc/loadavg  #  主要使用top
|===

=== 平均负载

平均负载

    System load averages is the average number of processes that are either in a runnable or uninterruptable state.  
    A process in a runnable state is either using the CPU or waiting to use the CPU(也就是R状态).
    A process in uninterruptable state is waiting for some I/O access, eg waiting for disk(也就是D状态).
    (man uptime)

多少合适

    平均负载可以简单地理解为平均活跃进程数，理想情况下等于CPU(核心)个数(lscpu或者grep 'model name' /proc/cpuinfo | wc -l)
    当平均负载高于CPU数量70%的时候，可以认为负载过高 或者监控看历史数据变化

vs. CPU使用率

    平均负载还包括D uninterruptible sleep (usually IO)以及R running or runnable (on run queue)中的runnable状态
    因此，没有对应关系
    例如IO密集型进程，可能导致平均负载较高，但CPU利用率较低
    例如大量进程，有些进程等待CPU调度(pidstat %wait列查看?)

原因分析

    借助mpstat, pidstat等命令分析

== 内存

- 系统内存指标

    已用内存
    剩余内存
    可用内存
    缺页异常: 主缺页异常, 次缺页异常
    缓存/缓冲区: 使用量, 命中率
    Slabs

- 进程内存指标

    虚拟内存(VSS)
    常驻内存(RSS)
    按比例分配共享内存后的物理内存(PSS)
    独占内存(USS)
    共享内存
    SWAP内存
    缺页异常: 主缺页异常, 次缺页异常

- SWAP

    已用空间
    剩余空间
    换入速度
    换出速度

[format="csv", options="header", separator=#]
|===
指标#工具#说明
系统已用,可用,剩余内存  #  free;vmstat;sar;cat /proc/meminfo  #  vmstat,sar比较全面
|===

== 文件
cat /proc/loadavg

=== 磁盘性能基本指标
使用率: 磁盘处理IO的时间百分比
饱和度: 磁盘处理IO的繁忙程度
IOPS: 每秒的IO请求数
吞吐量: 每秒的IO请求大小
响应时间

=== 工具
top
iostat
pidstat
iotop
bcc(filetop, opensnoop)
strace
lsof

==== IO基准测试
fio

=== 硬件层优化
SSD
RAID

=== 系统层优化
▪ 选择适合的文件系统。
比如Ubuntu默认使用ext4文件系统，而CentOS 7默认使用xfs文件系统。相比于ext4，xfs支持更大的磁盘分区和更大的文件数量，如xfs支持大于16TB的磁盘。xfs的缺点在于无法收缩，ext4则可以。

▪ 优化文件系统的配置选项
包括文件系统的特性(ext_attr、dir_index)、日志模式(如 journal、ordered、writeback)、挂载选项(如 noatime)等。

▪ 优化文件系统的缓存
比如pdflush脏页的刷新频率(如设置dirty_expire_centisecs和dirty_writeback_centisecs)以及脏页的限额(如调整dirty_background_ratio和dirty_ratio等)。再如，可以优化内核回收目录项缓存和索引节点缓存的倾向，即调整vfs_cache_pressure，数值越大，就表示越容易回收。

▪ 在不需要持久化时，用内存文件系统tmpfs可以获得更好的I/O性能。
tmpfs把数据直接保存在内存中，而不是磁盘中。比如/dev/shm/就是大多数Linux默认配置的一个内存文件系统，其大小默认为总内存的一半。

=== 应用层优化
▪ 顺序写代替随机写
▪ 缓存
▪ C标准库提供的fopen/fread等库函数，都会利用标准库的缓存，减少磁盘的操作。而直接使用open/read等系统调用时，就只能利用操作系统提供的页缓存和缓冲区等，而没有库函数的缓存可用
▪ 需要频繁读写同一块磁盘空间时，可以用mmap代替read/write，减少内存的拷贝次数
▪ 在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用fsync()取代O_SYNC
▪ 在多个应用程序共享磁盘时，为保证I/O不被某个应用完全占用，推荐使用cgroups的I/O子系统来限制进程/进程组的IOPS以及吞吐量
▪ 在使用CFQ调度器时，可以用ionice来调整进程的I/O调度优先级，特别是提高核心应用的I/O优先级
▪ io_uring

== 网络
https://github.com/orientye/understand/blob/main/network/%E4%BA%86%E8%A7%A3Network/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.asc

== 参考
倪鹏飞《Linux性能优化实战》