= 了解Linux性能优化
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:homepage: http://orientye.com

<<<

== 概述
=== 性能指标
https://github.com/orientye/understand/blob/main/high/high-concurrency.asc#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87

=== 理论延迟
In 2020:
    L1 cache reference: 1 ns
    L2 cache reference: 4 ns
    Branch mispredict: 3 ns
    Mutex lock/unlock: 17 ns
    Main memory reference: 100 ns
    Compress 1K bytes with Zippy: 2000 ns
    Send 2K bytes over commodity network: 44 ns
    Read 1 MB sequentially from memory: 3000 ns
    Round trip within same datacenter: 500,000 ns
    Disk seek: 2,000,000 ns
    Read 1 MB sequentially from disk: 825,000 ns
    Read 1 MB sequentially from SSD: 49000 ns

https://colin-scott.github.io/personal_website/research/interactive_latency.html

== CPU

[format="csv", options="header", separator=#]
|===
指标#工具#说明
平均负载  #  top,uptime,cat /proc/loadavg  #  主要使用top
系统CPU使用率  #  vmstat,mpstat,top,sar,/proc/stat  #  sar可以记录历史数据
进程CPU使用率  #  top,ps,pidstat,htop,atop  #  htop和atop更直观
系统上下文切换  #  vmstat  #  
进程上下文切换  #  pidstat  #  注意加上-w选项
软中断  #  top,mpstat,/proc/softirqs  #  top提供了软中断CPU使用率,/proc/softirqs和mpstat提供了各种软中断在每个CPU上的运行次数
硬中断  #  vmstat,/proc/interrupts  #  vmstat提供了总的中断次数,而/proc/interrupts提供各种中断在每个CPU上运行的累计次数
性能统计信息  #  perf  #  perf stat子命令
CPU数  #  lscpu,/proc/cpuinfo  #  
事件剖析  #  perf,火焰图,execsnoop  #  perf和火焰图用来分析热点函数以及调用栈,execsnoop用来监测短时进程
动态追踪  #  fstrace,bcc,systeminfo  #  ftrace用于跟踪内核函数调用栈,bcc和systeminfo用于跟踪内核或应用程序执行过程
|===

TIP: perf可以使用perf + gprof2dot + graphviz输出调用关系图, 从而迅速理清核心链路

=== 指标

==== 平均负载
- 平均负载

    System load averages:
    is the average number of processes that are either in a runnable or uninterruptable state.
    A process in a runnable state is either using the CPU or waiting to use the CPU(也就是R状态).
    A process in uninterruptable state is waiting for some I/O access, eg waiting for disk(也就是D状态).
    (man uptime)

- 命令

    top命令里的第一行: load average, 表示system load avg over the last 1, 5 and 15 minutes
    htop命令更为直观，与top不同的是，htop默认显示线程

- 多少合适

    平均负载可以简单地理解为平均活跃进程数，理想情况下等于CPU(核心)个数(lscpu或者grep 'model name' /proc/cpuinfo | wc -l)
    
    Q: 当平均负载为4时，意味着什么呢？
    在拥有4个CPU的系统上，意味着CPU刚好被完全占用；
    在拥有8个CPU的系统上，意味着CPU有50%的空闲；
    在只有2个CPU的系统上，意味着有一半的进程竞争不到CPU。
    
    当平均负载高于CPU数量70%的时候，可以认为负载过高

- vs. CPU使用率

    平均负载还包括D uninterruptible sleep (usually IO)以及R running or runnable (on run queue)中的runnable状态
    因此，平均负载与CPU使用率没有对应关系:
    例如IO密集型进程，可能导致平均负载较高，但CPU利用率较低
    例如大量进程，有些进程等待CPU调度(pidstat %wait列查看?)
    而CPU密集型进程，使用大量CPU会导致平均负载升高，此时两者会一致

- 原因分析

    借助mpstat, pidstat等命令分析

==== CPU使用率
- CPU使用率

    CPU Usage

- 命令

    top与ps命令里的%CPU
    工具给出的都是间隔一段时间的平均CPU使用率，因此要注意间隔时间的设置
    例如，top, htop, ps这几个命令报告的CPU使用率，默认的结果很可能不一样:
    因为默认分别使用3秒间隔(top)/2秒间隔(htop)/进程的整个生命周期(ps)

- 原因分析

    用户CPU和NiceCPU高，说明用户态进程占用了较多的 CPU，应该着重排查进程的性能问题
    系统CPU高，说明内核态占用了较多的CPU，应该着重排查内核线程或者系统调用的性能问题
    I/O等待CPU高，说明等待I/O的时间比较长，应该着重排查系统存储是不是出现了I/O问题
    软中断和硬中断高，说明软中断或硬中断的处理程序占用了较多的CPU，应该着重排查内核中的中断服务程序

    查看函数/事件:
    perf top -g -p $PID    #-g开启调用关系分析，-p指定进程ID

    还有概率较小的短时应用导致的问题:
    例如应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过top等工具也不容易发现
    再例如应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的 CPU
    对于这类进程，可以用pstree或者execsnoop找到父进程，再从父进程所在的应用入手，排查问题的根源

==== 上下文切换
- 上下文切换

    进程上下文切换
    线程上下文切换
    中断上下文切换

- 多少合适
    
    取决于系统本身的CPU性能
    如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。
    但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。
    这时，需要根据上下文切换的类型，再做具体分析。
    例如自愿上下文切换变多了，说明进程都在等待资源，有可能发生了I/O等其他问题；
    非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢CPU，说明CPU的确成了瓶颈；
    中断次数变多了，说明CPU被中断处理程序占用，可以查看/proc/interrupts文件来分析具体的中断类型。

=== 工具

==== top
注意:
top命令里默认显示的是进程，如果该进程有多个子线程，显示的CPU占用率是所有子线程之和

可以使用命令top -d 2 -Hp $PID 以2秒的频率显示PID所有子线程

==== pidstat
注意:
pidstat命令里默认显示进程，加上-t参数后，才会输出线程的指标，且一些指标并是不子线程之和

== 内存

[format="csv", options="header", separator=#]
|===
指标#工具#说明
系统已用,可用,剩余内存  #  free,vmstat,sar,cat /proc/meminfo  #  vmstat,sar比较全面
进程虚拟内存,常驻内存,共享内存  #  ps,top,pidstat,/proc/pid/stat(us)  #  pidstat -r
进程内存分布  #  pmap,/proc/pid/maps  #  
进程swap换出内存  #  top,/proc/pid/status  #  
进程缺页异常  #  ps,top,pidstat  #  pidstat -r
系统换页情况  #  sar  #  sar -B
缓存/缓冲区用量  #  free,vmstat,sar,cachestat  #  vmstat最常用,cachestat需要安装bcc
缓存/缓冲区命中率  #  cachetop  #  cachetop需要安装bcc
swap已用空间和剩余空间  #  free,sar  #  
swap换入换出  #  vmstat,sar  #  
内存泄漏检测  #  memleak,valgrind  #  memleak需要安装bcc
指定文件的缓存大小  #  pcstat  #  
|===

=== 指标
- 系统内存指标

    已用内存
    剩余内存
    可用内存
    缺页异常: 主缺页异常, 次缺页异常
    缓存/缓冲区: 使用量, 命中率
    Slabs

- 进程内存指标

    虚拟内存(VSS)
    常驻内存(RSS)
    按比例分配共享内存后的物理内存(PSS)
    独占内存(USS)
    共享内存
    SWAP内存
    缺页异常: 主缺页异常, 次缺页异常

- SWAP

    已用空间
    剩余空间
    换入速度
    换出速度

=== 工具

==== free
total = used + free + buffers + cache
available表示新进程可以使用多少内存，它不仅包含未使用内存，还包括了可回收的缓存，所以一般会比未使用内存更大。不过，并不是所有缓存都可以回收，因为有些缓存可能正在使用中。

==== vmstat
buff是对磁盘数据的缓存: 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”
cache是对文件数据的缓存: 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”
它们既会用在读请求中，也会用在写请求中
buff和cache单位: KB

bi和bo则分别表示块设备读取和写入的大小，单位为块/秒。因为Linux中块的大小是1KB，所以等价于KB/s

   Procs
       r: The number of runnable processes (running or waiting for run time).
       b: The number of processes blocked waiting for I/O to complete.

   Memory
       These are affected by the --unit option.
       swpd: the amount of swap memory used.

   Memory
       These are affected by the --unit option.
       swpd: the amount of swap memory used.
       free: the amount of idle memory.
       buff: the amount of memory used as buffers.
       cache: the amount of memory used as cache.
       inact: the amount of inactive memory.  (-a option)
       active: the amount of active memory.  (-a option)

   Swap
       These are affected by the --unit option.
       si: Amount of memory swapped in from disk (/s).
       so: Amount of memory swapped to disk (/s).

   IO
       bi: Blocks received from a block device (blocks/s).
       bo: Blocks sent to a block device (blocks/s).

   System
       in: The number of interrupts per second, including the clock.
       cs: The number of context switches per second.

   CPU
       These are percentages of total CPU time.
       us: Time spent running non-kernel code.  (user time, including nice time)
       sy: Time spent running kernel code.  (system time)
       id: Time spent idle.  Prior to Linux 2.5.41, this includes IO-wait time.
       wa: Time spent waiting for IO.  Prior to Linux 2.5.41, included in idle.
       st: Time stolen from a virtual machine.  Prior to Linux 2.6.11, unknown.

==== memleak(bcc)

==== sar

=== 优化

==== OOM(Out of Memory)
OOM是内核的一种保护机制。它监控进程的内存使用情况，并且使用om_score为每个进程的内存使用情况进行评分：
一个进程消耗的内存越大，oom_score就越大
一个进程占用的CPU越多，oom_score就越小

进程的oom_score越大，代表消耗的内存越多，也就越容易被杀死，从而可以更好保护系统。
为了实际工作的需要，可以通过/proc文件系统，设置进程的oom_adj，从而调整进程的oom_score。

== 文件

[format="csv", options="header", separator=#]
|===
指标#工具#说明
文件系统空间容量,使用量,剩余空间  #  df  # 
索引节点容量,使用量,剩余量  #  df  #  使用-i选项
页缓存,可回收slab缓存  #  /proc/meminfo,sar,vmstat  #  使用sar -r选项
缓冲区  #  /proc/meminfo,sar,vmstat  #  使用sar -r选项
目录项,索引节点及文件系统的缓存  #  /proc/slabinfo,slabtop  #  slabtop更直观
磁盘IO使用率,IOPS,吞吐量响应时间,IO平均大小及等待队列长度  #  iostat,sar,dstat  #  iostat -d -x或sar -d
进程IO大小以及IO延迟  #  pidstat,iotop  #  使用pidstat -d选项
块设备IO事件跟踪  #  blktrace  #  
进程IO系统调用跟踪  #  strace  #  
进程块设备IO大小跟踪  #  biosnoop,biotop  #  bcc
|===

=== 指标
- 磁盘性能基本指标

    使用率: 磁盘处理IO的时间百分比
    饱和度: 磁盘处理IO的繁忙程度
    IOPS: 每秒的IO请求数
    吞吐量: 每秒的IO请求大小
    响应时间

=== 工具
==== iostat
==== top
==== iotop
==== pidstat
==== bcc(filetop, opensnoop)
==== strace
==== lsof

=== 优化
==== IO基准测试
fio

==== 硬件层优化
SSD
RAID

==== 系统层优化
▪ 选择适合的文件系统。
比如Ubuntu默认使用ext4文件系统，而CentOS 7默认使用xfs文件系统。相比于ext4，xfs支持更大的磁盘分区和更大的文件数量，如xfs支持大于16TB的磁盘。xfs的缺点在于无法收缩，ext4则可以。

▪ 优化文件系统的配置选项
包括文件系统的特性(ext_attr、dir_index)、日志模式(如 journal、ordered、writeback)、挂载选项(如 noatime)等。

▪ 优化文件系统的缓存
比如pdflush脏页的刷新频率(如设置dirty_expire_centisecs和dirty_writeback_centisecs)以及脏页的限额(如调整dirty_background_ratio和dirty_ratio等)。再如，可以优化内核回收目录项缓存和索引节点缓存的倾向，即调整vfs_cache_pressure，数值越大，就表示越容易回收。

▪ 在不需要持久化时，用内存文件系统tmpfs可以获得更好的I/O性能。
tmpfs把数据直接保存在内存中，而不是磁盘中。比如/dev/shm/就是大多数Linux默认配置的一个内存文件系统，其大小默认为总内存的一半。

==== 应用层优化
▪ 顺序写代替随机写
▪ 缓存
▪ C标准库提供的fopen/fread等库函数，都会利用标准库的缓存，减少磁盘的操作。而直接使用open/read等系统调用时，就只能利用操作系统提供的页缓存和缓冲区等，而没有库函数的缓存可用
▪ 需要频繁读写同一块磁盘空间时，可以用mmap代替read/write，减少内存的拷贝次数
▪ 在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用fsync()取代O_SYNC
▪ 在多个应用程序共享磁盘时，为保证I/O不被某个应用完全占用，推荐使用cgroups的I/O子系统来限制进程/进程组的IOPS以及吞吐量
▪ 在使用CFQ调度器时，可以用ionice来调整进程的I/O调度优先级，特别是提高核心应用的I/O优先级
▪ io_uring

== 网络
https://github.com/orientye/understand/blob/main/network/%E4%BA%86%E8%A7%A3Network/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.asc

== 参考
倪鹏飞《Linux性能优化实战》