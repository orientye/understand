= 最优化(Optimization)
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath
:homepage: http://orientye.com

<<<

== 概念
=== 概括
https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E5%8C%96

最优化问题(也称优化问题)泛指定量决策问题，主要关心如何对有限资源进行有效分配和控制，并达到某种意义上的最优。它通常需要对需求进行定性和定量分析，建立恰当的数学模型来描述该问题，设计合适的计算方法来寻找问题的最优解，探索研究模型和算法的理论性质，考察算法的计算性能等。

- 最优化问题的一般形式
- 最优化问题的类型与应用背景

== 无约束优化问题
=== 约束和无约束优化问题的基础知识
=== 一维搜索方法
=== 梯度方法
=== 牛顿法(Newton's Method)
==== 概要
- 概念
又称牛顿-拉弗森方法(Newton-Raphson Method):
是一种用于求解非线性方程(组)的迭代优化算法。它通过利用函数的泰勒展开近似，快速收敛到方程的根或极值点。
牛顿法不仅可以用来求解方程的根，还可以用来求解函数的极值问题，二者在本质上是一个问题，因为求解函数极值的思路是寻找导数为0的点，本质上依然是求解方程。

- 基本思想
通过迭代逼近，利用函数的一阶导数(梯度)和二阶导数(Hessian矩阵)信息，构造局部线性或二次近似，从而找到方程的根或函数的极值。

- 应用场景
** 实数域和复数域上近似求解方程
** 优化问题: 寻找函数的极小值或极大值(此时需用二阶导数)

- 优缺点
    ** 优点:
        *** 收敛速度快: 若初始值合适，二阶收敛(误差平方级递减)。
        *** 高效: 尤其对于光滑凸函数。
    ** 缺点:
        *** 依赖初始值: 可能收敛到局部极值或发散。
        *** 需计算导数: 需显式给出梯度或Hessian矩阵，计算成本高。
        *** Hessian可能不正定: 优化问题中需修正(如拟牛顿法)。

==== 解非线性方程
===== 推导过程
Step1. 在Xn处对f(x)作一阶泰勒展开：
ifndef::env-github[]
[latexmath]
++++
f(x) \approx f(x_n) + f'(x_n)(x - x_n)
++++
endif::[]
ifdef::env-github[]
```math
f(x) \approx f(x_n) + f'(x_n)(x - x_n)
```
endif::[]

Step2. 令近似方程为零求根:
ifndef::env-github[]
[latexmath]
++++
0 \approx f(x_n) + f'(x_n)(x - x_n) \implies x = x_n - \frac{f(x_n)}{f'(x_n)}
++++
endif::[]
ifdef::env-github[]
```math
0 \approx f(x_n) + f'(x_n)(x - x_n) \implies x = x_n - \frac{f(x_n)}{f'(x_n)}
```
endif::[]

Step3. 得到迭代公式:
ifndef::env-github[]
[latexmath]
++++
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
++++
endif::[]
ifdef::env-github[]
```math
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
```
endif::[]

牛顿法与泰勒展开的关系:
牛顿法本质上是利用泰勒展开的一阶线性近似来构造迭代公式。
如果使用二阶泰勒展开(包含f′′(x)项)，可以得到更复杂的迭代方法(如Halley方法)，但计算成本更高。
泰勒展开的高阶项在牛顿法中未被使用，因此牛顿法仅具有局部快速收敛性，而无法保证全局收敛。

===== 几何解释
相当于在每一步用切线逼近函数，并找到切线与x轴的交点作为新的近似解。
如果初始值x0接近真实解，且f'(x)不为0，牛顿法通常能快速收敛。

===== 示例
求6的三次方的近似值

ifndef::env-github[]
示例:
[stem]
++++
\begin{aligned}
f(x) &= x^2 - 2, \quad f'(x) = 2x \\
x_{n+1} &= x_n - \frac{x_n^2 - 2}{2x_n} = \frac{x_n}{2} + \frac{1}{x_n}
\end{aligned}
++++

迭代过程:
|===
| 步骤 | \( x_n \)       | 计算
| 0    | 1.0             | 初始值
| 1    | 1.5             | \( \frac{1}{2} + \frac{2}{1} = 1.5 \)
| 2    | 1.4167          | \( \frac{1.5}{2} + \frac{1}{1.5} \approx 1.4167 \)
| 3    | 1.4142          | 进一步逼近真值
|===
endif::[]

===== 初始值

===== 参考
https://www.bilibili.com/video/BV1Nt411T7HT/

==== 求极值
===== 推导过程
ifndef::env-github[]
Step1. 在Xk处对f(x)作二阶泰勒展开: latexmath:[f(\mathbf{x}) \approx f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^T \nabla^2 f(\mathbf{x}_k) \Delta \mathbf{x}]
endif::[]
ifdef::env-github[]
Step1. 在Xk处对f(x)作二阶泰勒展开: $f(\mathbf{x}) \approx f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^T \nabla^2 f(\mathbf{x}_k) \Delta \mathbf{x}$
endif::[]

ifndef::env-github[]
Step2. 对近似函数求梯度并令其为零: latexmath:[\nabla f(\mathbf{x}_k) + \nabla^2 f(\mathbf{x}_k) \Delta \mathbf{x} = 0]
endif::[]
ifdef::env-github[]
Step2. 对近似函数求梯度并令其为零: $\nabla f(\mathbf{x}_k) + \nabla^2 f(\mathbf{x}_k) \Delta \mathbf{x} = 0$
endif::[]

ifndef::env-github[]
Step3. 解得迭代公式: latexmath:[\mathbf{x}_{k+1} = \mathbf{x}_k - [\nabla^2 f(\mathbf{x}_k)\]^{-1} \nabla f(\mathbf{x}_k)]
endif::[]
ifdef::env-github[]
Step3. 解得迭代公式: $\mathbf{x}_{k+1} = \mathbf{x}_k - [\nabla^2 f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k)$
endif::[]

===== 几何解释
牛顿法用二次曲面(二阶近似)拟合当前点附近的函数形状，并直接跳到该二次曲面的极小值点。若原函数是二次的，牛顿法一步收敛。

===== 示例
===== 注意事项

==== 收敛性

==== 参考
http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/lect/11-lect-newton.pdf
https://zhuanlan.zhihu.com/p/37588590

=== 共轭方向法
=== 拟牛顿法
=== 求解线性方程组
=== 无约束优化问题和神经网络
=== 全局搜索算法

== 线性规划
=== 线性规划概述
=== 单纯形法
=== 对偶
=== 非单纯形法
=== 整数规划

== 有约束的非线性优化问题
=== 仅含等式约束的优化问题
=== 含不等式约束的优化问题

=== 凸优化(Convex Optimization)
==== 概念
https://zh.wikipedia.org/wiki/%E5%87%B8%E5%84%AA%E5%8C%96

=== 有约束优化问题的求解算法

=== 多目标优化

== 参考
《An Introduction to Optimization, 4th》 中:《最优化导论》第四版
《最优化：建模、算法与理论》
《最优化理论与方法》
《Optimization Model》
《Numerical Optimization》
《Linear and Nonlinear Programming, 4th》
《Introduction to Linear Optimization》
《Nonlinear Programming》
《Convex Optimization》 中:《凸优化》
《Lectures on Modern Convex Optimization》
《Convex Optimization in signal and communication》