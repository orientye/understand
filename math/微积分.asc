= 微积分
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath
:homepage: http://orientye.com

<<<

== 连续性与可导性
=== 连续性
=== 可导性

== 导数(derivative)
=== 概念
导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。
Q: 导数 vs. 微分?
https://zhuanlan.zhihu.com/p/145620564
https://www.zhihu.com/question/39940688

Q: 可导 vs. 连续?

=== 性质

== 微分(differential calculus)
=== 概念
- 微分在深度学习中的应用
微分最重要的应用是优化问题，即考虑如何把事情做到最好。
在深度学习中，训练模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。通常情况下，变得更好意味着最小化一个损失函数(loss function)，即一个衡量模型有多糟糕这个问题的分数。最终，真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。但训练模型只能将模型与实际能看到的数据相拟合。因此，可以将拟合模型的任务分解为两个关键问题：
    优化(optimization): 用模型拟合观测数据的过程；
    泛化(generalization): 生成出有效性超出用于训练的数据集本身的模型。

- 数值微分(numerical differentiation)
** 原理:
    用差分逼近微分，核心公式源自泰勒展开: 忽略高阶项后，可推导出导数近似表达式。
** 数值微分的缺点
    *** 精度问题:
        步长 h 难以选择(太小→舍入误差；太大→截断误差)。
    *** 计算成本高:
        每计算一个梯度需 O(n) 次函数调用(n 是参数维度)。
    *** 不适用于高维:
        深度学习模型参数动辄百万级，数值微分无法胜任。
** 自动微分的优势
    *** 高精度:
        与数学解析解一致，无截断误差。
    *** 高效计算:
        反向模式一次计算所有梯度(适合高维优化)。
    *** 可扩展性:
        支持GPU加速(如PyTorch/TensorFlow)。

- 自动微分(automatic differentiation)
** 核心思想:
    利用链式法则分解计算图，精确计算导数。
** 前向模式(Forward Mode):
    沿计算图正向传播导数。
** 反向模式(Reverse Mode):
    反向传播梯度。
** 特点:
    不依赖差分，无截断误差，精度与解析解一致。
    适用于任意可微函数(包括分支、循环、递归等)。

=== 极值、零点、不等式
函数的微分与函数的单调性和凹凸性，乃至极值、零点和不等式有着紧密的联系。

==== 极值
===== 拉格朗日乘数法
- 概念
拉格朗日乘数法(Lagrange Multipliers)是数学中一种用于求解多元函数在约束条件下的极值的方法。它将约束优化问题转化为无约束优化问题，通过引入拉格朗日乘子(Lagrange multiplier)来构造新的方程组，从而找到可能的极值点。

- 核心思想
在约束 g(x,y)=0 下，求函数 f(x,y) 的极值时，极值点处 ∇f 和 ∇g 共线(即梯度方向相同或相反)。因此存在标量 λ(拉格朗日乘子)，使得：∇f=λ∇g
结合约束条件 g(x,y)=0，即可解出极值点。

- 几何解释
在极值点处，目标函数 f 的等高线与约束曲线 g=0 相切，此时两者的法向量(即梯度 ∇f 和 ∇g)平行。

- 注意事项
** 拉格朗日乘数法仅给出必要条件，需进一步验证是否为极值。
** 若约束条件为不等式(如 g(x,y)≤0)，需结合KKT条件(Karush-Kuhn-Tucker)推广。

== 偏导数(partial derivative)
https://zh.wikipedia.org/wiki/%E5%81%8F%E5%AF%BC%E6%95%B0

== 梯度(gradient)
=== 概念
梯度是微积分中的一个重要概念，它用于描述函数在某一点处沿各方向的变化率。

梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。

=== 定义
对于多变量函数f(x, y, z)，其梯度是一个向量，由函数关于各变量的偏导数组成。具体地，梯度grad f = (∂f/∂x, ∂f/∂y, ∂f/∂z)，其中∂f/∂x、∂f/∂y、∂f/∂z分别表示函数f关于x、y、z的偏导数。

在二维空间中，梯度是一个二维向量，通常用∇f(x,y) = ∂f/∂x,∂f/∂y表示。例如，对于二元函数f(x, y)，其梯度为(∂f/∂x, ∂f/∂y)。

== 链式法则(chain rule)
=== 概念
链式法则可以用来微分复合函数。

链式法则是微积分中的核心规则，用于计算复合函数的导数。它通过将复杂函数的求导问题分解为多个简单函数的导数乘积，大幅简化了梯度计算过程。

在神经网络中，链式法则是反向传播算法(backpropagation)的数学基础，使得模型能够高效地更新权重。

=== 链式求导法则
链式求导法则(版本1) 如果 h(x) = f(g(x)); 那么 h'(x) = f'(g(x))g'(x)

链式求导法则(版本2) 如果 y 是 u 的函数, 并且 u 是 x 的函数, 那么dy/dx = (dy/du)(du/dx)

=== 证明
《普林斯顿微积分读本》附录A.6.5

== 积分(integral calculus)

=== 积分变换(integral transform)
==== 傅里叶变换(fourier transform)
http://www.dspguide.com/pdfbook.htm
https://www.cnblogs.com/v-July-v/archive/2011/02/20/1983676.html
https://tracholar.github.io/math/2017/03/12/fourier-transform.html
https://github.com/Jezzamonn/fourier

== 级数
泰勒展开式

== 应用
https://www.cnblogs.com/edward-bian/p/5237962.html

== 工具
=== 等高线地图-可视化多元函数
https://zh.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/ways-to-represent-multivariable-functions/a/contour-maps
https://zhuanlan.zhihu.com/p/40520573

== 参考
《南开讲义》陈省身
《The Calculus Lifesaver》中文: 普林斯顿微积分读本(修订版)
《微积分进阶》楼红卫
