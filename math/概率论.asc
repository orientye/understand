= 概率论(Probability)
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath
:homepage: http://orientye.com

<<<

== 引言
=== 生日问题
生日问题 (表述 1)：房间里有多少人才能保证其中至少两个人的生日在同一天的概率不小于 50%？
生日问题 (表述 2)：假设对每个人来说, 出生在一年中任何一天的概率都是相等的. 那么, 房间里有多少人才能保证其中至少两个人的生日在同一天的概率不小于 50%？
生日问题 (表述 3)：假设客人的出生日期都是相互独立的, 并且每个人都等可能地出生在一年中的任何一天 (2 月 29 日除外), 那么房间里有多少人才能保证其中至少两个人的生日在同一天的概率不小于 50%？

=== 从投篮到几何级数
=== 赌博

=== 蒙提霍尔问题(Monty Hall problem)
==== 背景
也称三门问题，是一个源自博弈论的概率问题，它非常经典地展示了**条件概率**与直觉之间的巨大差异。
这个名字来源于美国电视游戏节目 *Let's Make a Deal* 的主持人蒙提·霍尔（Monty Hall）。

==== 问题场景设定

. 有三扇关闭的门：一扇门后面是汽车（奖品），另外两扇门后面是山羊（不值钱的）。
. 玩家选择：玩家首先选择一扇门（假设选1号门），但此时不打开。
. 主持人行动：知道汽车在哪里的主持人**必须**从剩下的两扇门中，打开一扇后面是**山羊**的门（主持人永远不会打开有车的门）。
. 机会：然后主持人问玩家：“你要不要改变你的选择，换成另一扇还没开的门？”

**问题：** 玩家**换**门获胜的概率高，还是**不换**获胜的概率高？

==== 违反直觉的答案
绝大多数人的第一直觉是：剩下两扇门，一车一羊，概率应该是 50%，换与不换没区别。
但正确答案是：

*   不换：获胜概率是 **1/3**。
*   换：获胜概率是 **2/3**。

==== 核心逻辑

主持人打开一扇羊门的动作，实际上向玩家**传递了信息**。这个信息改变了剩余那扇门的概率分布。

*   最初，玩家选对的概率是 1/3，错的概率是 2/3。
*   当主持人排除了一只羊之后，那初始 2/3 的错误概率，全部“浓缩”到了另一扇没开的门上。

==== 条件概率

*   **假设**：玩家最初选择了**门 1**。
*   **主持人规则**：
    1.  主持人知道汽车在哪里。
    2.  主持人必须从剩下的门（门 2 和门 3）中打开一扇。
    3.  主持人打开的门后面必须是羊（不能开有车的门）。
    4.  如果两扇门都是羊，主持人随机选择一扇打开（概率各 1/2）。

计算目标：**在主持人打开了某一扇门（比如门 3）的条件下，汽车在另一扇门（门 2）的概率是多少？**

==== 可视化：穷举法

假设汽车在 **A 门**（你一开始不知道）。

[options="header"]
|===
| 你最初选的门 | 汽车所在 | 主持人打开的门 | **换门的结果**
| A | A | B 或 C（随便一个羊） | **输**（换到羊）
| B | A | C（唯一能开的羊） | **赢**（换到A）
| C | A | B（唯一能开的羊） | **赢**（换到A）
|===

*   如果你选 A（1/3 概率），主持人开 B 或 C，你换就输。
*   如果你选 B（1/3 概率），主持人只能开 C，你换到 A 赢。
*   如果你选 C（1/3 概率），主持人只能开 B，你换到 A 赢。

**3 种情况中，换门赢了 2 次，输了 1 次。概率 2/3。**

==== 为什么直觉会错？

*   **忽略了主持人的知识**：直觉上，人们认为主持人开门后，剩下两扇门是“平等的”。但主持人并不是随机开的，他**刻意避免了开汽车**。这个非随机的动作破坏了平等性。
*   **错误地重置概率**：人们容易把问题简化为“两个门选一个”，忘记了最初的选择已经锁定了 1/3 的概率。

==== 极端情况
为了帮助理解，我们把问题放大：

*   假设有 **1000 扇门**，1 辆车，999 只羊。
*   你选 1 扇门（中奖概率 1/1000）。
*   主持人知道车在哪，他打开了 **998 扇有羊的门**，只留下你的门和另一扇门没开。
*   此时，你是坚持你那扇 1/1000 概率的门，还是换到主持人精心留下的那扇门？

显然，车几乎肯定在主持人没开的那扇门里（概率 999/1000）。

==== 总结
蒙提霍尔问题的精髓在于：**利用新信息（主持人的行动）更新概率，往往能做出更优的决策。**

== 基本概率论
=== 基本概率定律
==== 结果空间与事件
Ω 称为样本空间或结果空间，并把Ω中的元素称作事件。
Prob(A) 来表示事件 A 发生的概率, 简写成Pr(A)。

==== 概率公理
(柯尔莫戈洛夫的) 概率公理：

    Ω 是一个结果空间，∑ 是一个 σ 代数。如果概率函数满足下列条件，那么 (Ω,∑,Prob) 就是一个概率空间：
        ▪ 如果 A∈∑，那么 Pr(A) 是有定义的，并且 0≤Pr(A)≤1。 （概率第一公理）
        ▪ Pr(∅)=0 且 Pr(Ω)=1。（概率第二公理）
        ▪ 设 {Aᵢ} 是由有限个或可数个两两互不相交的集合构成的集族，并且每一个集合都是 ∑ 中的元素，那么 Pr(⋃ᵢAᵢ)=∑ᵢPr(Aᵢ)。（概率第三公理）

柯尔莫戈洛夫概率公理的标准表述，定义了概率空间的三个基本要素：

    ▪ 样本空间 Ω（所有可能结果的集合）
    ▪ σ-代数 ∑（事件集合，满足特定封闭性）
    ▪ 概率测度 Pr（满足非负性、规范性、可数可加性）

==== 基本概率规则
概率空间的有用规则：
设 (Ω, Σ, Prob) 是一个概率空间，那么可以得到如下结论：

(1) 如果 A ∈ Σ, 那么 Pr(A) + Pr(Aᶜ) = 1. 也就是说，Pr(A) = 1 − Pr(Aᶜ)
A 和 Aᶜ 是互不相交的集合, 并且两者的并是 Ω
两个互不相交事件的并的概率就等于两个事件的概率之和。  
实际上是全概率公式的一个特例：A 发生的概率应该是 1 减去 A 不发生的概率。这种重新表述非常有用，因为计算某件事不发生的概率往往比计算它发生的概率容易很多.

(2) Pr(A ∪ B) = Pr(A) + Pr(B) − Pr(A ∩ B)
这个式子可以进一步推广。例如，如果有三个事件，那么：
Pr(A₁ ∪ A₂ ∪ A₃) = Pr(A₁) + Pr(A₂) + Pr(A₃) − Pr(A₁ ∩ A₂) − Pr(A₁ ∩ A₃) − Pr(A₂ ∩ A₃) + Pr(A₁ ∩ A₂ ∩ A₃).
这也被称为容斥原理。

(3) 如果 A ⊂ B, 那么 Pr(A) ≤ Pr(B). 然而，如果 A 是 B 的真子集，那么不一定有 Pr(A) < Pr(B), 但可以确定有 Pr(B) = Pr(A) + Pr(B ∩ Aᶜ), 其中 B ∩ Aᶜ 指的是 B 中不属于 A 的所有元素。

(4) 如果对于任意的 i, 均有 Aᵢ ⊂ B, 那么 Pr(∪ᵢAᵢ) ≤ Pr(B).

==== 全概率公式(Law of Total Probability)
全概率公式是概率论中的基本定理，用于将一个复杂事件的概率分解为多个互斥情形下的条件概率之和。
全概率公式可以由概率第三公理直接推出。

.有限划分:
ifndef::env-github[]
[latexmath]
++++
P(A) = \sum_{i=1}^{n} P(B_i) \cdot P(A \mid B_i)
++++
endif::[]
ifdef::env-github[]
```math
P(A) = \sum_{i=1}^{n} P(B_i) \cdot P(A \mid B_i)
```
endif::[]

.可数无穷划分:
ifndef::env-github[]
[latexmath]
++++
P(A) = \sum_{i=1}^{\infty} P(B_i) \cdot P(A \mid B_i)
++++
endif::[]
ifdef::env-github[]
```math
P(A) = \sum_{i=1}^{\infty} P(B_i) \cdot P(A \mid B_i)
```
endif::[]

全概率公式常与 贝叶斯公式 搭配使用，用于计算其分母。

==== 并的概率与包含的概率
求交集要往往比求并集容易

对于给定的并集，通常希望能把它写成几个互不相交的集合的并。

=== 条件概率、独立性和贝叶斯定理
==== 条件概率
条件概率描述的是：在某个事件已经发生的条件下，另一个事件发生的概率。

设 A 和 B 是两个事件，且 P(A) > 0（即事件 A 发生的概率大于0）。那么，在 **事件 A 发生的条件下**，**事件 B 发生**的概率记为 P(B|A)。

其计算公式为：
ifndef::env-github[]
\[
P(B|A) = \frac{P(A \cap B)}{P(A)}
\]
endif::env-github[]
ifdef::env-github[]
[source, math]
----
P(B|A) = \frac{P(A \cap B)}{P(A)}
----
endif::env-github[]

*   P(B|A)：条件概率，读作“在A发生的条件下B发生的概率”。
*   P(A ∩ B)：A和B同时发生的概率（也称为联合概率）。
*   P(A)：A发生的概率（也称为边缘概率）。

Q: 条件概率公式怎么推导出来的？
条件概率的公式 并不是通过数学推导从更基础的公理“算出来”的，而是一个定义。
它是在人们处理随机现象时，为了满足逻辑一致性而人为设定的规则。
可以从两个角度来理解：
1. 从缩小样本空间的角度理解
2. 从概率的乘法法则倒推（公理化角度）： P(A∩B)=P(A)⋅P(B∣A) 即两个事都发生，等于“先发生 A，然后在 A 发生的基础上再发生 B”

==== 独立性

==== 贝叶斯定理

== 随机变量(random variable)
=== 概念
- random variable: a variable whose values depend on outcomes of a random event.

- uppercase letter X for random variable
- lowercase letter x for an observed value

- 离散型随机变量与连续型随机变量的特征
定义    取值是有限个或可数无限个         取值是一个区间或数个区间内的任意实数（不可数）
取值特点    可以一一列举，有“空隙”      充满整个区间，无法一一列举
概率描述    概率质量函数(其实就是概率密度函数)        概率密度函数
计算概率    对PMF求和: P(X=k)       对PDF积分: P(a≤X≤b)
分布函数    累积分布函数是右连续的阶梯函数      累积分布函数是连续的（且通常可导）
常见例子    抛硬币正面次数、掷骰子点数、一天顾客数      身高、体重、温度
离散随机变量：它的取值是可数的（可以像数数一样一个一个列出来）。比如：抛一个骰子，结果只能是1、2、3、4、5、6。可以把所有的可能性都写在纸上。
连续随机变量：它的取值是充满一个区间的（是不可数的，无法一个一个列完）。比如：身高、体重、完成一项任务所需的时间。

=== 离散(discrete)随机变量
==== 概念
离散型随机变量 X 就是定义在一个离散的结果空间 Ω (这意味着 Ω 是有限的或至多可数的) 上的实值函数。

==== 概率密度函数(probability density function, PDF)
离散型随机变量的概率密度函数:
设 X 是一个随机变量，它定义在离散的结果空间 Ω 上 (Ω 是有限的或至多可数的)，那么 X 的概率密度函数 (常记作 fX) 就是 X 取某个特定值的概率: fX(x) = Prob(ω∈Ω: X(ω)=x)

概率密度函数的值总是大于或等于 0，并且和始终为 1。

注意，有些教材用概率质量函数(Probability Mass Function, PMF)的说法，而非概率密度函数。

==== 累积分布函数(cumulative distribution function, CDF)
离散型随机变量的累积分布函数: 设 X 是一个随机变量，它定义在一个有限的或至多可数的离散结果空间 Ω 上。X 的概率密度函数（常记作 fX ）就是 X 取某个特定值的概率。累积分布函数（常记作 FX ）则表示 X 不超过某个特定值的概率。它们分别记作:
fX(x) = Prob(ω∈Ω: X(ω)=x)
FX(x) = Prob(ω∈Ω: X(ω)≤x)

Q: 为什么用大写字母来表示累积分布函数呢？
这种写法来源于微积分: 在微积分中，常用 f 表示函数，用 F 表示 f 的原函数，并且 F 与曲线下方的面积有关。这里的情况与之类似 (当讨论连续型随机变量时，会看到这种关联会更加明显)。累积分布函数是把某个给定点之前的所有概率都加起来，这与积分非常相似，从而解释了为什么会采用这种符号。

累积分布函数的极限:
设 FX 是离散型随机变量 X 的累积分布函数，那么
limₓ→₋∞FX(x)=0, limₓ→+∞FX(x)=1, 如果 y>x，那么 FX(y) ≥ FX(x)。

=== 连续(continuous)随机变量
==== 概率密度函数
设 X 是一个随机变量。如果存在一个实值函数 fX 满足
(1) fX是一个分段连续函数
(2) fX(x)≥0
(3) ∫−∞到∞fX(t)dt=1
那么 X 是一个连续型随机变量，fX 是 X 的概率密度函数。

==== 累积分布函数
X 的累积分布函数 FX(x) 就是 X 不大于 x 的概率：
FX(x) = Prob(X≤x) = ∫−∞到xfX(t)dt

设 X 是一个连续型随机变量，它的累积分布函数是 FX 那么
limₓ→₋∞FX(x)=0, limₓ→+∞FX(x)=1, 如果 y>x，那么 FX(y) ≥ FX(x)。

==== 示例
Q: 对于连续型随机变量，取任何单个具体值的概率是 0，为什么？
直观上，连续型随机变量的取值是实数轴上的不可数无限多个点，如果每个点的概率都大于 0，那么加起来会超过 1（因为不可数无穷多个正数相加不可能等于有限值 1）。

例：等公交车的时间
假设在一段时间内，公交车是随机到达的，平均每10分钟一班。你随机来到车站，开始等车。
随机变量： 设 X 为你的等待时间。
为什么是连续的？ 你的等待时间可以是0分钟（刚到车就来了），可以是3.25分钟，可以是7.879分钟，可以是9.999分钟。它可以是0到10之间的任何一个实数。
问：等待时间刚好等于 5分钟整 的概率是多少？
答：P(X=5)=0。因为时间是连续流动的，恰好停在一个没有任何误差的精确点上，在数学上是测度为0的事件。
问：等待时间在 4分钟到6分钟之间 的概率是多少？
答：这个概率是有意义的，它等于概率密度函数曲线下，从4到6之间的面积。

=== 处理多个随机变量
联合概率
条件概率
贝叶斯定理
边际化
独立性

=== 期望(expectation)
==== 期望值和矩(moments)
数学期望是概率论中描述随机变量“平均取值”的一个量，可以理解为加权平均，权重是概率。

期望值是随机变量取值的加权平均，是描述分布中心位置的最基本特征。

矩是期望值的推广。通过计算随机变量不同幂次（比如平方、立方等）的期望值，可以捕捉到分布更丰富的特征，如离散程度、偏斜度、峰度等。

矩不再只考虑 X 本身的平均，而是考虑 X 的某个函数的平均。

期望值是矩的特例：期望值就是一阶原点矩。
矩是更一般的数字特征：通过各阶矩（特别是前四阶）可以大致刻画一个分布的位置、离散度、对称性和峰态。
矩的存在性：高阶矩不一定存在（如柯西分布没有一阶及以上的矩），低阶矩存在是高阶矩存在的必要条件。
矩与分布：在某些条件下（如矩问题），一个分布的所有矩的序列能唯一确定该分布（但并非总是，需满足一定条件）。

ifndef::env-github[]
设 stem:[X] 是定义在 stem:[\mathbb{R}] 上的随机变量，它的概率密度函数是 stem:[f_X]。函数  stem:[g(X)] 的期望值是：
[stem]
++++
\mathbb{E}[g(X)] = 
\begin{cases} 
\int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx & \text{若 } X \text{ 是连续的} \\ 
\sum_n g(x_n) \cdot f_X(x_n) & \text{若 } X \text{ 是离散的} 
\end{cases}
++++
最重要的情形是 stem:[g(x) = x^r]。我们把 stem:[\mathbb{E}[X^r]] 称为 stem:[X] 的 stem:[r] 阶矩，把  \(
\mathbb{E}[(X - \mathbb{E}[X])^r] \) 称为 stem:[X] 的 stem:[r] 阶中心矩。
endif::[]
ifdef::env-github[]
设 $X$ 是定义在 $\mathbb{R}$ 上的随机变量，它的概率密度函数是 $f_X$。函数 $g(X)$ 的期望值是：
```math
\mathbb{E}[g(X)] = 
\begin{cases} 
\int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx & \text{若 } X \text{ 是连续的} \\ 
\sum_n g(x_n) \cdot f_X(x_n) & \text{若 } X \text{ 是离散的} 
\end{cases}
```
最重要的情形是 $g(x) = x^r$。我们把 $\mathbb{E}[X^r]$ 称为 $X$ 的 $r$ 阶矩，把 $\mathbb{E}[(X - \mathbb{E}[X])^r]$ 称为 $X$ 的 $r$ 阶中心矩。
endif::[]

==== 均值和方差
ifndef::env-github[]
设 \(X\) 是一个连续型或离散型的随机变量，它的概率密度函数是 \(f_X\)。
endif::[]
ifdef::env-github[]
设 $X$ 是一个连续型或离散型的随机变量，它的概率密度函数是 $f_X$。
endif::[]

均值就是期望值或平均值。如果从分布中不断地取出很多值，然后对得到的结果求平均值，那么这个平均值应该非常接近于 µX。
标准差可以预测出结果与均值之间差距的波动程度，标准差越小，结果就越容易分布在均值附近。

===== 均值
ifndef::env-github[]
\(X\) 的均值（即平均值或期望值）是一阶矩。我们把它表示为 \(E[X]\) 或 \(\mu_X\)（当随机变量很明确时，通常不给出下标 \(X\)，而只写 \(\mu\)）。具体地说，

[horizontal]
连续型:: \(\mu = \int_{-\infty}^{\infty} x \cdot f_X(x) dx\)
离散型:: \(\mu = \sum_n x_n \cdot f_X(x_n)\)
endif::[]
ifdef::env-github[]
$X$ 的均值（即平均值或期望值）是一阶矩。我们把它表示为 $E[X]$ 或 $\mu_X$（当随机变量很明确时，通常不给出下标 $X$，而只写 $\mu$）。具体地说，

[horizontal]
连续型:: $\mu = \int_{-\infty}^{\infty} x \cdot f_X(x) dx$
离散型:: $\mu = \sum_n x_n \cdot f_X(x_n)$
endif::[]

===== 方差
ifndef::env-github[]
\(X\) 的方差（记作 \(\sigma_X^2\) 或 \(\operatorname{Var}(X)\)）是二阶中心距，也可以说是 \(g(X) = (X - \mu_X)^2\) 的期望值。同样，当随机变量很明确时，通常不给出下标 \(X\)，而只写 \(\sigma^2\)。把它完整地写出来，就是

[horizontal]
连续型:: \(\sigma_X^2 = \int_{-\infty}^{\infty} (x - \mu_X)^2 f_X(x) dx\)
离散型:: \(\sigma_X^2 = \sum_n (x_n - \mu_X)^2 f_X(x_n)\)

因为 \(\mu_X = E[X]\)，所以在一系列代数运算后，有
\[\sigma^2 = E[(X - E[X])^2] = E[X^2] - E[X]^2.\]

这个式子把方差和 \(X\) 的前二阶矩联系起来，在很多计算中都非常有用。
endif::[]
ifdef::env-github[]
$X$ 的方差（记作 $\sigma_X^2$ 或 $Var(X)$）是二阶中心距，也可以说是 $g(X) = (X - \mu_X)^2$ 的期望值。同样，当随机变量很明确时，通常不给出下标 $X$，而只写 $\sigma^2$。把它完整地写出来，就是

[horizontal]
连续型:: $\sigma_X^2 = \int_{-\infty}^{\infty} (x - \mu_X)^2 f_X(x) dx$
离散型:: $\sigma_X^2 = \sum_n (x_n - \mu_X)^2 f_X(x_n)$

因为 $\mu_X = E[X]$，所以在一系列代数运算后，有
```math
\sigma^2 = E[(X - E[X])^2] = E[X^2] - E[X]^2.
```

这个式子把方差和 $X$ 的前二阶矩联系起来，在很多计算中都非常有用。
endif::[]

===== 标准差
ifndef::env-github[]
标准差是方差的平方根，即 \(\sigma_X = \sqrt{\sigma^2}\)。
endif::[]
ifdef::env-github[]
标准差是方差的平方根，即 $\sigma_X = \sqrt{\sigma^2}$。
endif::[]

方差与标准差：
与方差相比，标准差的优势在于它和均值有相同的单位。因此，标准差是衡量结果在均值附近波动幅度的自然尺度。

===== 技术说明
为了保证均值存在，希望
ifndef::env-github[]
[horizontal]
连续型:: \(\int_{-\infty}^{\infty} |x| f_X(x) dx\) 是有限的
离散型:: \(\sum_n |x_n| f_X(x_n)\) 是有限的
endif::[]
ifdef::env-github[]
[horizontal]
连续型:: $\int_{-\infty}^{\infty} |x| f_X(x) dx$ 是有限的
离散型:: $\sum_n |x_n| f_X(x_n)$ 是有限的
endif::[]

==== 联合分布
===== 定义
联合分布描述的是两个或多个随机变量同时发生的概率情况。

===== 离散型随机变量的联合分布
如果 X 和 Y 都是离散型随机变量（即它们的取值是可数的），它们的联合分布可以用联合概率质量函数来表示：

* **定义**： 
ifndef::env-github[]
latexmath:[p(x, y) = P(X = x \text{ 且 } Y = y)]
endif::[]
ifdef::env-github[]
$p(x, y) = P(X = x \text{ 且 } Y = y)$
endif::[]
这表示 X 恰好等于某个特定值 x，同时 Y 恰好等于某个特定值 y 的概率。

* **性质**：
ifndef::env-github[]
  1.  latexmath:[p(x, y) \ge 0] 对于所有的 x, y 成立。
  2.  latexmath:[\sum_x \sum_y p(x, y) = 1] （所有可能组合的概率之和为1）。
endif::[]
ifdef::env-github[]
  1.  $p(x, y) \ge 0$ 对于所有的 x, y 成立。
  2.  $\sum_x \sum_y p(x, y) = 1$ （所有可能组合的概率之和为1）。
endif::[]

===== 连续型随机变量的联合分布
如果 X 和 Y 都是连续型随机变量（即它们的取值是连续的，不可数），联合分布则用联合概率密度函数来表示：

* **定义**：有一个函数 f(x, y)，它本身并不是概率。概率是通过计算该函数在某个区域下的**体积**（即二重积分）得到的。
ifndef::env-github[]
latexmath:[P((X, Y) \in A) = \iint_A f(x, y) \, dx \, dy]
endif::[]
ifdef::env-github[]
$P((X, Y) \in A) = \iint_A f(x, y) \, dx \, dy$
endif::[]
例如，想知道 X 在 a 和 b 之间，且 Y 在 c 和 d 之间的概率，就计算：
ifndef::env-github[]
latexmath:[P(a \le X \le b, c \le Y \le d) = \int_c^d \int_a^b f(x, y) \, dx \, dy]
endif::[]
ifdef::env-github[]
$P(a \le X \le b, c \le Y \le d) = \int_c^d \int_a^b f(x, y) \, dx \, dy$
endif::[]

* **性质**：
ifndef::env-github[]
  1.  latexmath:[f(x, y) \ge 0] 对于所有的 x, y 成立。
  2.  latexmath:[\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) \, dx \, dy = 1] （整个分布空间下的总体积为1）。
endif::[]
ifdef::env-github[]
  1.  $f(x, y) \ge 0$ 对于所有的 x, y 成立。
  2.  $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) \, dx \, dy = 1$ （整个分布空间下的总体积为1）。。
endif::[]

===== 边缘分布
从联合分布出发，可以“忽略”其中一个变量，只看另一个变量的分布，这被称为边缘分布。

* **离散情况的边缘概率质量函数(以X为例)：**
ifndef::env-github[]
latexmath:[p_X(x) = \sum_y p(x, y)] （将某一行的所有概率加起来）
endif::[]
ifdef::env-github[]
$p_X(x) = \sum_y p(x, y)$ （将某一行的所有概率加起来）
endif::[]

* **连续情况的边缘概率密度函数(以Y为例)：**
ifndef::env-github[]
latexmath:[f_Y(y) = \int_{-\infty}^{\infty} f(x, y) \, dx] （沿 latexmath:[x] 方向积分）
endif::[]
ifdef::env-github[]
$f_Y(y) = \int_{-\infty}^{\infty} f(x, y) \, dx$ （沿 $x$ 方向积分）
endif::[]

===== 条件分布
联合分布还可以推导出**条件分布**，即在已知一个变量取值的情况下，另一个变量的分布。

* **离散情况**：给定 Y=y 时，X 的条件概率质量函数为：
ifndef::env-github[]
latexmath:[p_{X|Y}(x|y) = \frac{p(x, y)}{p_Y(y)} \quad (\text{要求} p_Y(y) > 0)]
endif::[]
ifdef::env-github[]
$p_{X|Y}(x|y) = \frac{p(x, y)}{p_Y(y)} \quad (\text{要求} p_Y(y) > 0)$
endif::[]

* **连续情况**：给定 Y=y 时，X 的条件概率密度函数为：
ifndef::env-github[]
latexmath:[f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)} \quad (\text{要求} f_Y(y) > 0)]
endif::[]
ifdef::env-github[]
$f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)} \quad (\text{要求} f_Y(y) > 0)$
endif::[]

===== 独立性
联合分布是判断随机变量是否独立的关键。
如果 X 和 Y 是独立的，那么联合概率密度（或质量函数）可以分解为两个边缘分布的乘积：
ifndef::env-github[]
latexmath:[f(x, y) = f_X(x) \cdot f_Y(y) \quad (\text{对所有 } x, y \text{ 成立})]
endif::[]
ifdef::env-github[]
$f(x, y) = f_X(x) \cdot f_Y(y) \quad (\text{对所有 } x, y \text{ 成立})$
endif::[]

==== 期望的线性性质

==== 均值和方差的性质

==== 偏斜度与峰度

==== 协方差

== 随机向量
=== 概念
一个 随机向量 是将多个随机变量作为一个整体来考虑。形式上：
设 X₁, X₂, ...，Xₙ 是定义在同一个样本空间 Ω 和概率空间 (Ω, F, P) 上的随机变量，则称：X = (X₁, X₂, ...，Xₙ)ᵀ为一个 n维随机向量。
直观理解：不再只观察一个随机特征，而是同时观察多个相关联的随机特征，例如:
一个人的（身高，体重，年龄）
一个地区的（温度，湿度，气压）
一只股票的（今日收益率，昨日成交量，市场指数）

== 参数估计
=== 概念
参数估计目标是根据样本数据对总体分布的未知参数进行推断。主要分为点估计(Point Estimation)与区间估计(Interval Estimation)。

=== 点估计(Point Estimation)
=== 矩估计
=== 最大似然估计(Maximum Likelihood Estimation)
- 似然(likelihood) vs. 概率(probability)
* 概率 (Probability)
    ** 描述的是：在已知参数 θ 的情况下，观测到某数据 D 的可能性。
    ** 它是关于数据的函数，参数 θ 是固定已知的。
    ** 关心的问题是：如果模型确定了，那么不同数据出现的可能性有多大？
* 似然 (Likelihood)
    ** 描述的是：在已知观测数据 D 的情况下，某个参数 θ 的可能性。
    ** 它是关于参数的函数，数据 D 是固定已知的。
    ** 关心的问题是：已经观测到了这些数据，哪个参数值能最好地解释它们？

- 最大似然估计
MLE 是一种统计推断方法，用于根据样本数据来估计概率分布的参数。
其核心思想是：在给定样本数据的情况下，寻找使得样本出现概率最大的参数值。

- 基本原理
假设有一个统计模型，其参数为θ，并且有一组观察数据X。最大似然估计的目标是找到使模型产生这组观察数据的概率最大的参数θ:
1. 定义似然函数：似然函数L(θ|X)表示在给定参数θ下，模型产生观察数据X的概率。通常，会使用概率密度函数（对于连续数据）或概率质量函数（对于离散数据）来定义似然函数。
2. 最大化似然函数：通过求解使似然函数达到最大值的参数θ，得到最大似然估计值。这通常涉及到对似然函数求导，并找到其导数为0的点（即极值点），然后检查这些点以确定哪个是最大值。
3. 求解参数：对于某些模型，似然函数的最大化可能涉及复杂的数学运算，如数值优化方法。

- 注意事项
最大似然估计的结果依赖于观察数据的数量和质量。
对于某些模型，最大似然估计可能不是唯一的，或者可能不存在（如当似然函数没有最大值时）。
最大似然估计通常不提供关于参数估计不确定性的直接信息。为了评估这种不确定性，可能需要使用其他方法，如贝叶斯估计或置信区间。
总的来说，最大似然估计是一种强大且广泛使用的统计方法，用于从观察数据中估计模型参数。

- 参考
https://www.zhihu.com/question/54082000
https://zhuanlan.zhihu.com/p/148968222

=== 最小方差无偏估计
=== 贝叶斯估计
=== 区间估计(Interval Estimation)

== 特殊分布
=== 离散分布
=== 连续型随机变量: 均匀分布与指数分布
=== 连续型随机变量: 正态分布(normal distribution)
也称为高斯分布(gaussian distribution)

不仅在概率论中，正态分布对于整个数学和科学领域都非常重要。这主要是因为中心极限定理: 在很多情况下，相互独立的随机变量之和会收敛于正态分布。

中心极限定理(Central Limit Theorem): 大量独立随机变量的平均值(或和)的分布趋近于正态分布，无论原始分布是什么。

正态分布是统计学和概率论中最重要的连续概率分布之一。其概率密度函数呈对称的钟形曲线，由均值(μ)和标准差(σ)完全决定。
https://en.wikipedia.org/wiki/Normal_distribution

正态分布的前世今生:
https://cosx.org/2013/01/story-of-normal-distribution-1
https://cosx.org/2013/01/story-of-normal-distribution-2

=== 伽马函数与相关分布
=== 卡方分布

== 极限定理
=== 不等式与大数定律
=== 斯特林公式
=== 生成函数与卷积
=== 中心极限定理的证明
=== 傅里叶分析与中心极限定理

== 随机过程
=== 马尔可夫过程(Markov Process)
一般的马尔可夫过程:
马尔可夫过程本质上是这样一个系统: 为了预测n + 1时刻的行为，关键是看它在 n 时刻的状态。换句话说，知道如何到达 n 时刻的状态并不能为预测下一刻会发生什么提供任何额外的信息。
马尔可夫过程是一类具有**无记忆性**（马尔可夫性）的随机过程，其未来状态仅依赖于当前状态，而与历史无关。数学表示为:
ifndef::env-github[]
stem:[P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)]
endif::[]
ifdef::env-github[]
```math
P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)
```
endif::[]

=== 高斯过程

== 随机算法
=== 基本随机数生成算法
=== 遗传算法
=== 蒙特卡洛算法

== 采样算法
=== 概念
采样算法的核心目标是：从一个已知的、复杂的概率分布 p(x) 中，生成服从该分布的样本。

采样算法与参数估计：


=== 拒绝采样(Rejection Sampling)
=== 重要性采样(Importance Sampling)

== MISC
=== 假设检验
=== 差分方程、马尔可夫过程和概率论

=== 最小二乘法(Least Squares Method)
==== 概念
最小二乘法，又称最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳拟合曲线或函数，广泛应用于回归分析、曲线拟合以及参数估计等领域。

参考:
https://en.wikipedia.org/wiki/Least_squares

==== 原理

==== 几何意义

==== 解法

==== 局限性与适用场景

==== 参考
https://zhuanlan.zhihu.com/p/38128785

== 参考
《The Probability Lifesaver》中文: 普林斯顿概率论读本
《Introduction to Probability》中文: 概率导论(第2版·修订版)
《Probability Theory》中文: 概率论沉思录
《概率论及其应用(第三版)》英: An Introduction to Probability Theory and Its Applications
《伊藤清概率论》
《概率论与数理统计》陈希孺
《概率》施利亚耶夫
《Introduction to Probability Models》第12版 中文:《应用随机过程: 概率模型导论》