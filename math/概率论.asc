= 概率论(Probability)
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath
:homepage: http://orientye.com

<<<

== 引言
=== 生日问题
生日问题 (表述 1)：房间里有多少人才能保证其中至少两个人的生日在同一天的概率不小于 50%？
生日问题 (表述 2)：假设对每个人来说, 出生在一年中任何一天的概率都是相等的. 那么, 房间里有多少人才能保证其中至少两个人的生日在同一天的概率不小于 50%？
生日问题 (表述 3)：假设客人的出生日期都是相互独立的, 并且每个人都等可能地出生在一年中的任何一天 (2 月 29 日除外), 那么房间里有多少人才能保证其中至少两个人的生日在同一天的概率不小于 50%？

=== 从投篮到几何级数
=== 赌博

== 基本概率论
=== 基本概率定律
==== 结果空间与事件
Ω 称为样本空间或结果空间，并把Ω中的元素称作事件。
Prob(A) 来表示事件 A 发生的概率, 简写成Pr(A)。

==== 概率公理
(柯尔莫戈洛夫的) 概率公理：

    Ω 是一个结果空间，∑ 是一个 σ 代数。如果概率函数满足下列条件，那么 (Ω,∑,Prob) 就是一个概率空间：
        ▪ 如果 A∈∑，那么 Pr(A) 是有定义的，并且 0≤Pr(A)≤1。
        ▪ Pr(∅)=0 且 Pr(Ω)=1。
        ▪ 设 {Aᵢ} 是由有限个或可数个两两互不相交的集合构成的集族，并且每一个集合都是 ∑ 中的元素，那么 Pr(⋃ᵢAᵢ)=∑ᵢPr(Aᵢ)。

柯尔莫戈洛夫概率公理的标准表述，定义了概率空间的三个基本要素：

    ▪ 样本空间 Ω（所有可能结果的集合）
    ▪ σ-代数 ∑（事件集合，满足特定封闭性）
    ▪ 概率测度 Pr（满足非负性、规范性、可数可加性）

==== 全概率公式(Law of Total Probability)
全概率公式是概率论中的基本定理，用于将一个复杂事件的概率分解为多个互斥情形下的条件概率之和。

.有限划分:
ifndef::env-github[]
[latexmath]
++++
P(A) = \sum_{i=1}^{n} P(B_i) \cdot P(A \mid B_i)
++++
endif::[]
ifdef::env-github[]
```math
P(A) = \sum_{i=1}^{n} P(B_i) \cdot P(A \mid B_i)
```
endif::[]

.可数无穷划分:
ifndef::env-github[]
[latexmath]
++++
P(A) = \sum_{i=1}^{\infty} P(B_i) \cdot P(A \mid B_i)
++++
endif::[]
ifdef::env-github[]
```math
P(A) = \sum_{i=1}^{\infty} P(B_i) \cdot P(A \mid B_i)
```
endif::[]

全概率公式常与 贝叶斯公式 搭配使用，用于计算其分母。

=== 条件概率、独立性和贝叶斯定理

== 随机变量(random variable)
=== 概念
- random variable: a variable whose values depend on outcomes of a random event.

- uppercase letter X for random variable
- lowercase letter x for an observed value

- 离散型随机变量与连续型随机变量的特征
定义    取值是有限个或可数无限个         取值是一个区间或数个区间内的任意实数（不可数）
取值特点    可以一一列举，有“空隙”      充满整个区间，无法一一列举
概率描述    概率质量函数(其实就是概率密度函数)        概率密度函数
计算概率    对PMF求和: P(X=k)       对PDF积分: P(a≤X≤b)
分布函数    累积分布函数是右连续的阶梯函数      累积分布函数是连续的（且通常可导）
常见例子    抛硬币正面次数、掷骰子点数、一天顾客数      身高、体重、温度

=== 离散(discrete)随机变量
==== 概念
离散型随机变量 X 就是定义在一个离散的结果空间 Ω (这意味着 Ω 是有限的或至多可数的) 上的实值函数。

==== 概率密度函数(probability density function, PDF)
离散型随机变量的概率密度函数:
设 X 是一个随机变量，它定义在离散的结果空间 Ω 上 (Ω 是有限的或至多可数的)，那么 X 的概率密度函数 (常记作 fX) 就是 X 取某个特定值的概率: fX(x) = Prob(ω∈Ω: X(ω)=x)

概率密度函数的值总是大于或等于 0，并且和始终为 1。

注意，有些教材用概率质量函数(Probability Mass Function, PMF)的说法，而非概率密度函数。

==== 累积分布函数(cumulative distribution function, CDF)
离散型随机变量的累积分布函数: 设 X 是一个随机变量，它定义在一个有限的或至多可数的离散结果空间 Ω 上。X 的概率密度函数（常记作 fX ）就是 X 取某个特定值的概率。累积分布函数（常记作 FX ）则表示 X 不超过某个特定值的概率。它们分别记作:
fX(x) = Prob(ω∈Ω: X(ω)=x)
FX(x) = Prob(ω∈Ω: X(ω)≤x)

Q: 为什么用大写字母来表示累积分布函数呢？
这种写法来源于微积分: 在微积分中，常用 f 表示函数，用 F 表示 f 的原函数，并且 F 与曲线下方的面积有关。这里的情况与之类似 (当讨论连续型随机变量时，会看到这种关联会更加明显)。累积分布函数是把某个给定点之前的所有概率都加起来，这与积分非常相似，从而解释了为什么会采用这种符号。

累积分布函数的极限:
设 FX 是离散型随机变量 X 的累积分布函数，那么
limₓ→₋∞FX(x)=0, limₓ→+∞FX(x)=1, 如果 y>x，那么 FX(y) ≥ FX(x)。

=== 连续(continuous)随机变量
==== 概率密度函数
设 X 是一个随机变量。如果存在一个实值函数 fX 满足
(1) fX是一个分段连续函数
(2) fX(x)≥0
(3) ∫−∞到∞fX(t)dt=1
那么 X 是一个连续型随机变量，fX 是 X 的概率密度函数。

==== 累积分布函数
X 的累积分布函数 FX(x) 就是 X 不大于 x 的概率：
FX(x) = Prob(X≤x) = ∫−∞到xfX(t)dt

设 X 是一个连续型随机变量，它的累积分布函数是 FX 那么
limₓ→₋∞FX(x)=0, limₓ→+∞FX(x)=1, 如果 y>x，那么 FX(y) ≥ FX(x)。

Q: 对于连续型随机变量，取任何单个具体值的概率是 0，为什么？

=== 处理多个随机变量
联合概率
条件概率
贝叶斯定理
边际化
独立性

=== 期望(expectation)
==== 期望值和矩(moments)
数学期望是概率论中描述随机变量“平均取值”的一个量，可以理解为加权平均，权重是概率。

期望值是随机变量取值的加权平均，是描述分布中心位置的最基本特征。

矩是期望值的推广。通过计算随机变量不同幂次（比如平方、立方等）的期望值，可以捕捉到分布更丰富的特征，如离散程度、偏斜度、峰度等。

矩不再只考虑 X 本身的平均，而是考虑 X 的某个函数的平均。

期望值是矩的特例：期望值就是一阶原点矩。
矩是更一般的数字特征：通过各阶矩（特别是前四阶）可以大致刻画一个分布的位置、离散度、对称性和峰态。
矩的存在性：高阶矩不一定存在（如柯西分布没有一阶及以上的矩），低阶矩存在是高阶矩存在的必要条件。
矩与分布：在某些条件下（如矩问题），一个分布的所有矩的序列能唯一确定该分布（但并非总是，需满足一定条件）。

== 随机向量
=== 概念
一个 随机向量 是将多个随机变量作为一个整体来考虑。形式上：
设 X₁, X₂, ...，Xₙ 是定义在同一个样本空间 Ω 和概率空间 (Ω, F, P) 上的随机变量，则称：X = (X₁, X₂, ...，Xₙ)ᵀ为一个 n维随机向量。
直观理解：不再只观察一个随机特征，而是同时观察多个相关联的随机特征，例如:
一个人的（身高，体重，年龄）
一个地区的（温度，湿度，气压）
一只股票的（今日收益率，昨日成交量，市场指数）

== 参数估计
=== 概念
参数估计目标是根据样本数据对总体分布的未知参数进行推断。主要分为点估计(Point Estimation)与区间估计(Interval Estimation)。

=== 点估计(Point Estimation)
=== 矩估计
=== 最大似然估计(Maximum Likelihood Estimation)
- 似然(likelihood) vs. 概率(probability)
* 概率 (Probability)
    ** 描述的是：在已知参数 θ 的情况下，观测到某数据 D 的可能性。
    ** 它是关于数据的函数，参数 θ 是固定已知的。
    ** 关心的问题是：如果模型确定了，那么不同数据出现的可能性有多大？
* 似然 (Likelihood)
    ** 描述的是：在已知观测数据 D 的情况下，某个参数 θ 的可能性。
    ** 它是关于参数的函数，数据 D 是固定已知的。
    ** 关心的问题是：已经观测到了这些数据，哪个参数值能最好地解释它们？

- 最大似然估计
MLE 是一种统计推断方法，用于根据样本数据来估计概率分布的参数。
其核心思想是：在给定样本数据的情况下，寻找使得样本出现概率最大的参数值。

- 基本原理
假设有一个统计模型，其参数为θ，并且有一组观察数据X。最大似然估计的目标是找到使模型产生这组观察数据的概率最大的参数θ:
1. 定义似然函数：似然函数L(θ|X)表示在给定参数θ下，模型产生观察数据X的概率。通常，会使用概率密度函数（对于连续数据）或概率质量函数（对于离散数据）来定义似然函数。
2. 最大化似然函数：通过求解使似然函数达到最大值的参数θ，得到最大似然估计值。这通常涉及到对似然函数求导，并找到其导数为0的点（即极值点），然后检查这些点以确定哪个是最大值。
3. 求解参数：对于某些模型，似然函数的最大化可能涉及复杂的数学运算，如数值优化方法。

- 注意事项
最大似然估计的结果依赖于观察数据的数量和质量。
对于某些模型，最大似然估计可能不是唯一的，或者可能不存在（如当似然函数没有最大值时）。
最大似然估计通常不提供关于参数估计不确定性的直接信息。为了评估这种不确定性，可能需要使用其他方法，如贝叶斯估计或置信区间。
总的来说，最大似然估计是一种强大且广泛使用的统计方法，用于从观察数据中估计模型参数。

- 参考
https://www.zhihu.com/question/54082000
https://zhuanlan.zhihu.com/p/148968222

=== 最小方差无偏估计
=== 贝叶斯估计
=== 区间估计(Interval Estimation)

== 特殊分布
=== 离散分布
=== 连续型随机变量: 均匀分布与指数分布
=== 连续型随机变量: 正态分布(normal distribution)
也称为高斯分布(gaussian distribution)

不仅在概率论中，正态分布对于整个数学和科学领域都非常重要。这主要是因为中心极限定理: 在很多情况下，相互独立的随机变量之和会收敛于正态分布。

中心极限定理(Central Limit Theorem): 大量独立随机变量的平均值(或和)的分布趋近于正态分布，无论原始分布是什么。

正态分布是统计学和概率论中最重要的连续概率分布之一。其概率密度函数呈对称的钟形曲线，由均值(μ)和标准差(σ)完全决定。
https://en.wikipedia.org/wiki/Normal_distribution

正态分布的前世今生:
https://cosx.org/2013/01/story-of-normal-distribution-1
https://cosx.org/2013/01/story-of-normal-distribution-2

=== 伽马函数与相关分布
=== 卡方分布

== 极限定理
=== 不等式与大数定律
=== 斯特林公式
=== 生成函数与卷积
=== 中心极限定理的证明
=== 傅里叶分析与中心极限定理

== 随机过程
=== 马尔可夫过程(Markov Process)
一般的马尔可夫过程:
马尔可夫过程本质上是这样一个系统: 为了预测n + 1时刻的行为，关键是看它在 n 时刻的状态。换句话说，知道如何到达 n 时刻的状态并不能为预测下一刻会发生什么提供任何额外的信息。
马尔可夫过程是一类具有**无记忆性**（马尔可夫性）的随机过程，其未来状态仅依赖于当前状态，而与历史无关。数学表示为:
ifndef::env-github[]
stem:[P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)]
endif::[]
ifdef::env-github[]
```math
P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)
```
endif::[]

=== 高斯过程

== 随机算法
=== 基本随机数生成算法
=== 遗传算法
=== 蒙特卡洛算法

== 采样算法
=== 概念
采样算法的核心目标是：从一个已知的、复杂的概率分布 p(x) 中，生成服从该分布的样本。

采样算法与参数估计：


=== 拒绝采样(Rejection Sampling)
=== 重要性采样(Importance Sampling)

== MISC
=== 假设检验
=== 差分方程、马尔可夫过程和概率论

=== 最小二乘法(Least Squares Method)
==== 概念
最小二乘法，又称最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳拟合曲线或函数，广泛应用于回归分析、曲线拟合以及参数估计等领域。

参考:
https://en.wikipedia.org/wiki/Least_squares

==== 原理

==== 几何意义

==== 解法

==== 局限性与适用场景

==== 参考
https://zhuanlan.zhihu.com/p/38128785

== 参考
《The Probability Lifesaver》中文: 普林斯顿概率论读本
《Introduction to Probability》中文: 概率导论(第2版·修订版)
《Probability Theory》中文: 概率论沉思录
《概率论及其应用(第三版)》英: An Introduction to Probability Theory and Its Applications
《伊藤清概率论》
《概率论与数理统计》陈希孺
《概率》施利亚耶夫
《Introduction to Probability Models》第12版 中文:《应用随机过程: 概率模型导论》