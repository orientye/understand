= 概率论(Probability)
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath
:homepage: http://orientye.com

<<<

== 基本概率论
=== 基本概率定律
=== 条件概率、独立性和贝叶斯定理

== 随机变量(random variable)
=== 概念
- random variable: a variable whose values depend on outcomes of a random event.

- uppercase letter X for random variable
- lowercase letter x for an observed value

- 概率密度函数(probability density function, PDF)

特征    离散型随机变量      连续型随机变量
定义    取值是有限个或可数无限个（如自然数）	    取值是一个区间或数个区间内的任意实数（不可数）
取值特点	可以一一列举，有“空隙”	    充满整个区间，无法一一列举
概率描述	概率质量函数        概率密度函数
计算概率	对PMF求和：P(X=k)       对PDF积分：P(a≤X≤b)
分布函数	累积分布函数是右连续的阶梯函数      累积分布函数是连续的（且通常可导）
常见例子	抛硬币正面次数、掷骰子点数、一天顾客数      身高、体重、温度、完成作业所需时间

=== 离散(discrete)随机变量
离散型随机变量:
离散型随机变量 X 就是定义在一个离散的结果空间 Ω (这意味着 Ω 是有限的或至多可数的) 上的实值函数。

如骰子的每一面

=== 连续(continuous)随机变量
如人的体重和身高

=== 处理多个随机变量
联合概率
条件概率
贝叶斯定理
边际化
独立性

=== 期望和方差

== 随机向量

== 参数估计
=== 概念
参数估计是统计学和概率论的核心内容之一，其目标是根据样本数据对总体分布的未知参数进行推断。参数估计主要分为点估计(Point Estimation)与区间估计(Interval Estimation)。

=== 点估计
=== 矩估计
=== 最大似然估计(Maximum Likelihood Estimation)
- 似然(likelihood) vs. 概率(probability)
* 概率 (Probability)
    ** 描述的是：在已知参数 θ 的情况下，观测到某数据 D 的可能性。
    ** 它是关于数据的函数，参数 θ 是固定已知的。
    ** 关心的问题是：如果模型确定了，那么不同数据出现的可能性有多大？
* 似然 (Likelihood)
    ** 描述的是：在已知观测数据 D 的情况下，某个参数 θ 的可能性。
    ** 它是关于参数的函数，数据 D 是固定已知的。
    ** 关心的问题是：已经观测到了这些数据，哪个参数值能最好地解释它们？

- 最大似然估计
MLE 是一种统计推断方法，用于根据样本数据来估计概率分布的参数。
其核心思想是：在给定样本数据的情况下，寻找使得样本出现概率最大的参数值。

- 基本原理
假设有一个统计模型，其参数为θ，并且有一组观察数据X。最大似然估计的目标是找到使模型产生这组观察数据的概率最大的参数θ:
1. 定义似然函数：似然函数L(θ|X)表示在给定参数θ下，模型产生观察数据X的概率。通常，会使用概率密度函数（对于连续数据）或概率质量函数（对于离散数据）来定义似然函数。
2. 最大化似然函数：通过求解使似然函数达到最大值的参数θ，得到最大似然估计值。这通常涉及到对似然函数求导，并找到其导数为0的点（即极值点），然后检查这些点以确定哪个是最大值。
3. 求解参数：对于某些模型，似然函数的最大化可能涉及复杂的数学运算，如数值优化方法。

- 注意事项
最大似然估计的结果依赖于观察数据的数量和质量。
对于某些模型，最大似然估计可能不是唯一的，或者可能不存在（如当似然函数没有最大值时）。
最大似然估计通常不提供关于参数估计不确定性的直接信息。为了评估这种不确定性，可能需要使用其他方法，如贝叶斯估计或置信区间。
总的来说，最大似然估计是一种强大且广泛使用的统计方法，用于从观察数据中估计模型参数。

- 参考
https://www.zhihu.com/question/54082000
https://zhuanlan.zhihu.com/p/148968222

=== 最小方差无偏估计
=== 贝叶斯估计
=== 区间估计

== 特殊分布
=== 离散分布
=== 连续型随机变量: 均匀分布与指数分布
=== 连续型随机变量: 正态分布(normal distribution)
也称为高斯分布(gaussian distribution)

不仅在概率论中，正态分布对于整个数学和科学领域都非常重要。这主要是因为中心极限定理: 在很多情况下，相互独立的随机变量之和会收敛于正态分布。

中心极限定理: 大量独立随机变量的平均值(或和)的分布趋近于正态分布，无论原始分布是什么。

正态分布是统计学和概率论中最重要的连续概率分布之一。其概率密度函数呈对称的钟形曲线，由均值(μ)和标准差(σ)完全决定。
https://en.wikipedia.org/wiki/Normal_distribution

正态分布的前世今生:
https://cosx.org/2013/01/story-of-normal-distribution-1
https://cosx.org/2013/01/story-of-normal-distribution-2

=== 伽马函数与相关分布
=== 卡方分布

== 极限定理
=== 不等式与大数定律
=== 斯特林公式
=== 生成函数与卷积
=== 中心极限定理的证明
=== 傅里叶分析与中心极限定理

== 随机过程
=== 马尔可夫过程(Markov Process)
一般的马尔可夫过程:
马尔可夫过程本质上是这样一个系统: 为了预测n + 1时刻的行为，关键是看它在 n 时刻的状态。换句话说，知道如何到达 n 时刻的状态并不能为预测下一刻会发生什么提供任何额外的信息。
马尔可夫过程是一类具有**无记忆性**（马尔可夫性）的随机过程，其未来状态仅依赖于当前状态，而与历史无关。数学表示为:
ifndef::env-github[]
stem:[P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)]
endif::[]
ifdef::env-github[]
```math
P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)
```
endif::[]

=== 高斯过程

== MISC
=== 假设检验
=== 差分方程、马尔可夫过程和概率论

=== 最小二乘法(Least Squares Method)
==== 概念
最小二乘法，又称最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳拟合曲线或函数，广泛应用于回归分析、曲线拟合以及参数估计等领域。

参考:
https://en.wikipedia.org/wiki/Least_squares

==== 原理

==== 几何意义

==== 解法

==== 局限性与适用场景

==== 参考
https://zhuanlan.zhihu.com/p/38128785

== 参考
《The Probability Lifesaver》中文: 普林斯顿概率论读本
《Introduction to Probability》中文: 概率导论(第2版·修订版)
《Probability Theory》中文: 概率论沉思录
《概率论及其应用(第三版)》英: An Introduction to Probability Theory and Its Applications
《伊藤清概率论》
《概率论与数理统计》陈希孺
《概率》施利亚耶夫
《Introduction to Probability Models》第12版 中文:《应用随机过程: 概率模型导论》