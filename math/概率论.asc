= 概率论(Probability)
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath
:homepage: http://orientye.com

<<<

== 基本概率论
=== 基本概率定律
=== 条件概率、独立性和贝叶斯定理

== 随机变量(random variable)
=== 离散(discrete)随机变量
离散(discrete)随机变量(如骰子的每一面)

=== 连续(continuous)随机变量
连续(continuous)随机变量(如人的体重和身高)

=== 处理多个随机变量
联合概率
条件概率
贝叶斯定理
边际化
独立性

=== 期望和方差

== 随机向量

== 参数估计
参数估计是统计学和概率论的核心内容之一，其目标是根据样本数据对总体分布的未知参数进行推断。参数估计主要分为点估计(Point Estimation)与区间估计(Interval Estimation)。

=== 点估计
=== 矩估计
=== 最大似然估计
- 概念
Q: 什么是似然(likelihood)？似然 vs. 概率(probability)
极大似然估计（Maximum Likelihood Estimation，简称 MLE）是一种统计推断方法，用于根据样本数据来估计概率分布的参数。其核心思想是：在给定样本数据的情况下，寻找使得样本出现概率最大的参数值。

- 基本原理
假设有一个统计模型，其参数为θ，并且有一组观察数据X。最大似然估计的目标是找到使模型产生这组观察数据的概率最大的参数θ:
1. 定义似然函数：似然函数L(θ|X)表示在给定参数θ下，模型产生观察数据X的概率。通常，会使用概率密度函数（对于连续数据）或概率质量函数（对于离散数据）来定义似然函数。
2. 最大化似然函数：通过求解使似然函数达到最大值的参数θ，得到最大似然估计值。这通常涉及到对似然函数求导，并找到其导数为0的点（即极值点），然后检查这些点以确定哪个是最大值。
3. 求解参数：对于某些模型，似然函数的最大化可能涉及复杂的数学运算，如数值优化方法。

- 注意事项
最大似然估计的结果依赖于观察数据的数量和质量。
对于某些模型，最大似然估计可能不是唯一的，或者可能不存在（如当似然函数没有最大值时）。
最大似然估计通常不提供关于参数估计不确定性的直接信息。为了评估这种不确定性，可能需要使用其他方法，如贝叶斯估计或置信区间。
总的来说，最大似然估计是一种强大且广泛使用的统计方法，用于从观察数据中估计模型参数。

- 参考
https://www.zhihu.com/question/54082000

=== 最小方差无偏估计
=== 贝叶斯估计
=== 区间估计

== 特殊分布
=== 离散分布
=== 连续型随机变量: 均匀分布与指数分布
=== 连续型随机变量: 正态分布(normal distribution)
也称为高斯分布(gaussian distribution)

不仅在概率论中，正态分布对于整个数学和科学领域都非常重要。这主要是因为中心极限定理: 在很多情况下，相互独立的随机变量之和会收敛于正态分布。

中心极限定理: 大量独立随机变量的平均值(或和)的分布趋近于正态分布，无论原始分布是什么。

正态分布是统计学和概率论中最重要的连续概率分布之一。其概率密度函数呈对称的钟形曲线，由均值(μ)和标准差(σ)完全决定。
https://en.wikipedia.org/wiki/Normal_distribution

正态分布的前世今生:
https://cosx.org/2013/01/story-of-normal-distribution-1
https://cosx.org/2013/01/story-of-normal-distribution-2

=== 伽马函数与相关分布
=== 卡方分布

== 极限定理
=== 不等式与大数定律
=== 斯特林公式
=== 生成函数与卷积
=== 中心极限定理的证明
=== 傅里叶分析与中心极限定理

== 随机过程
=== 马尔可夫过程(Markov Process)
一般的马尔可夫过程:
马尔可夫过程本质上是这样一个系统：为了预测n + 1时刻的行为，关键是看它在 n 时刻的状态。换句话说，知道如何到达 n 时刻的状态并不能为预测下一刻会发生什么提供任何额外的信息。
马尔可夫过程是一类具有**无记忆性**（马尔可夫性）的随机过程，其未来状态仅依赖于当前状态，而与历史无关。数学表示为：

```math
P(X_{t+1} = x \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} = x \mid X_t)
```

=== 高斯过程

== MISC
=== 假设检验
=== 差分方程、马尔可夫过程和概率论

=== 最小二乘法(Least Squares Method)
==== 概念
最小二乘法，又称最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳函数匹配，广泛应用于回归分析、信号处理、机器学习等领域。

利用最小二乘法可以简便的求得未知的数据，并使得求得的数据与实际数据之间误差的平方和为最小。

最小二乘法是对线性方程组，即方程个数比未知数更多的方程组，以回归分析求得近似解的标准方法。在这整个解决方案中，最小二乘法演算为每一方程式的结果中，将残差平方和的总和最小化。

最重要的应用是在曲线拟合上。最小二乘所涵义的最佳拟合，即残差（残差为：观测值与模型提供的拟合值之间的差距）平方总和的最小化。当问题在自变量（x变量）有重大不确定性时，那么使用简易回归和最小二乘法会发生问题；在这种情况下，须另外考虑变量-误差-拟合模型所需的方法，而不是最小二乘法。

最小二乘问题分为两种：线性或普通的最小二乘法，和非线性的最小二乘法，取决于在所有未知数中的残差是否为线性。线性的最小二乘问题发生在统计回归分析中；它有一个封闭形式的解决方案。非线性的问题通常经由迭代细致化来解决；在每次迭代中，系统由线性近似，因此在这两种情况下核心演算是相同的。

最小二乘法所得出的多项式，即以拟合曲线的函数来描述自变量与预计因变量的方差关系。

当观测值来自指数族且满足轻度条件时，最小二乘估计和最大似然估计是相同的。最小二乘法也能从动差法得出。

最小二乘法大多是以线性函数形式来表示，但对于更广泛的函数族，最小二乘法也是有效和实用的。此外，迭代地将局部的二次近似应用于或然性（借由费希尔信息），最小二乘法可用于拟合广义线性模型。

参考:
https://en.wikipedia.org/wiki/Least_squares

==== 局限性与适用场景

== 参考
《The Probability Lifesaver》中文: 普林斯顿概率论读本
《Introduction to Probability》中文: 概率导论(第2版·修订版)
《Probability Theory》中文: 概率论沉思录
《伊藤清概率论》
《概率论与数理统计》 陈希孺
《概率》施利亚耶夫