= 程序员数学
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:homepage: http://orientye.com

<<<

== 数学简史
《数学历史的启示》龚昇
《数学简史》莫里斯·克莱因

== 位运算

=== 与运算&
判断是否为2次幂

    NO: if (n & (n - 1))

2次幂的性质: % -> &

    ringbuffer(如kfifo): in % size 可以转化为 in & (size – 1), 其中size为2次幂

=== 异或运算^
x ^ x = 0
x ^ 0 = x

== unsigned

=== kfifo
自动溢出结果依然正确:
数据长度: in - out
空闲长度: size - in + out
缓冲区空: in == out
缓冲区满: size == (in - out)

=== bsearch/bsort/pivot
http://orientye.com/go-sort/

== 浮点数与定点数
https://langdev.stackexchange.com/questions/665/what-are-the-pros-and-cons-of-fixed-point-arithmetics-vs-floating-point-arithmet

== 随机数

== 微积分
=== 导数(derivative)
导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。

Q: 导数 vs. 微分？

=== 微分(differential calculus)
- 微分在深度学习中的应用
微分最重要的应用是优化问题，即考虑如何把事情做到最好。
在深度学习中，训练模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。通常情况下，变得更好意味着最小化一个损失函数(loss function)，即一个衡量模型有多糟糕这个问题的分数。最终，真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。但训练模型只能将模型与实际能看到的数据相拟合。因此，可以将拟合模型的任务分解为两个关键问题：
    优化(optimization): 用模型拟合观测数据的过程；
    泛化(generalization): 生成出有效性超出用于训练的数据集本身的模型。

- 数值微分(numerical differentiation)
** 原理:
    用差分逼近微分，核心公式源自泰勒展开: 忽略高阶项后，可推导出导数近似表达式。
** 数值微分的缺点
    *** 精度问题:
        步长 h 难以选择(太小→舍入误差；太大→截断误差)。
    *** 计算成本高:
        每计算一个梯度需 O(n) 次函数调用(n 是参数维度)。
    *** 不适用于高维:
        深度学习模型参数动辄百万级，数值微分无法胜任。
** 自动微分的优势
    *** 高精度:
        与数学解析解一致，无截断误差。
    *** 高效计算:
        反向模式一次计算所有梯度(适合高维优化)。
    *** 可扩展性:
        支持GPU加速(如PyTorch/TensorFlow)。

- 自动微分(automatic differentiation)
** 核心思想:
    利用链式法则分解计算图，精确计算导数。
** 前向模式(Forward Mode):
    沿计算图正向传播导数。
** 反向模式(Reverse Mode):
    反向传播梯度。
** 特点:
    不依赖差分，无截断误差，精度与解析解一致。
    适用于任意可微函数(包括分支、循环、递归等)。

=== 偏导数(partial derivative)
https://zh.wikipedia.org/wiki/%E5%81%8F%E5%AF%BC%E6%95%B0

=== 梯度(gradient)
==== 概念
梯度是微积分中的一个重要概念，它用于描述函数在某一点处沿各方向的变化率。

梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。

==== 定义
对于多变量函数f(x, y, z)，其梯度是一个向量，由函数关于各变量的偏导数组成。具体地，梯度grad f = (∂f/∂x, ∂f/∂y, ∂f/∂z)，其中∂f/∂x、∂f/∂y、∂f/∂z分别表示函数f关于x、y、z的偏导数。

在二维空间中，梯度是一个二维向量，通常用∇f(x,y) = ∂f/∂x,∂f/∂y表示。例如，对于二元函数f(x, y)，其梯度为(∂f/∂x, ∂f/∂y)。

=== 链式法则(chain rule)
链式法则可以用来微分复合函数。

链式法则是微积分中的核心规则，用于计算复合函数的导数。它通过将复杂函数的求导问题分解为多个简单函数的导数乘积，大幅简化了梯度计算过程。

在神经网络中，链式法则是反向传播算法(backpropagation)的数学基础，使得模型能够高效地更新权重。

=== 积分(integral calculus)

==== 积分变换(integral transform)
===== 傅里叶变换(fourier transform)
http://www.dspguide.com/pdfbook.htm
https://www.cnblogs.com/v-July-v/archive/2011/02/20/1983676.html
https://tracholar.github.io/math/2017/03/12/fourier-transform.html
https://github.com/Jezzamonn/fourier

=== 级数
泰勒展开式

=== 应用
https://www.cnblogs.com/edward-bian/p/5237962.html

=== 参考
《南开讲义》陈省身
《The Calculus Lifesaver》中文: 普林斯顿微积分读本(修订版)

== 线性代数
=== 标量
标量(scalar)是一个单独的数值，它只有大小，没有方向。

=== 向量
向量是一组有序的数，它既有大小又有方向。向量可以被视为标量值组成的列表。

=== 矩阵
向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。

==== 秩
- 概念
矩阵的秩(Rank)是线性代数中的一个重要概念，表示矩阵中线性无关的行或列的最大数目。
矩阵的秩反映了矩阵所包含的信息量，是矩阵的一个重要特征。

- 定义
对于一个 m×n 的矩阵 A，其秩定义为：
行秩：矩阵中线性无关的行向量的最大数目。
列秩：矩阵中线性无关的列向量的最大数目。
行秩和列秩是相等的，因此统称为矩阵的秩，记作 rank(A)。

- 计算方法
初等行变换法:
    通过初等行变换将矩阵化为行阶梯形（Row Echelon Form），非零行的数目即为矩阵的秩。
子式法: 矩阵的秩等于其最高阶非零子式的阶数。

- 性质
对于 m×n 矩阵 A，有 rank(A)≤min(m,n)。
若 A 是方阵且满秩（即 rank(A)=n），则 A 可逆。
矩阵的秩在初等变换下不变。
对于矩阵 A 和 B，有 rank(A+B)≤rank(A)+rank(B)。
对于矩阵 A 和 B，有 rank(AB)≤min(rank(A),rank(B))。

- 应用
矩阵的秩在许多领域有广泛应用，如线性方程组的求解、矩阵分解、机器学习中的特征选择等。

- 参考
https://www.zhihu.com/question/21605094

==== 特征值(eigenvalues)与特征向量(eigenvectors)
- 概念
特征值是描述矩阵在线性变换中的缩放因子的标量。
特征向量是与特征值相关联的非零向量，它表示在线性变换中受到特征值缩放的方向。
https://tracholar.github.io/math/2018/01/28/matrix.html
https://zhuanlan.zhihu.com/p/165382601

- 向量特征-vs-特征向量
向量特征(vector features):
向量特征是指一个向量所具有的能够描述其某种性质或特点的属性。这些属性可以是向量的长度、方向、元素之间的关系、在某个坐标系下的坐标值等。
特征向量(eigenvector):
在矩阵和线性变换的背景下，设A是一个nxn的一个的方阵，若存在非零向量和实数，使得Ax=bx，则向量x称为矩阵A的属于特征值b的特征向量。

=== 张量
向量是标量的推广，矩阵是向量的推广，张量是描述具有任意数量轴的n维数组的通用方法。

任何按元素的一元运算都不会改变其操作数的形状。同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。

=== 范数
范数是定义在向量空间（或矩阵）上的一个函数，用于衡量向量或矩阵的大小或长度。

深度学习中常见的范数:
L1范数：向量x的L1范数定义为向量各元素绝对值之和，即∥x∥1=∑|xi|。L1范数对异常值（离群点）的敏感度较低，常用于正则化项中，以促进模型的稀疏性。
L2范数：向量x的L2范数定义为向量各元素平方和的平方根，即∥x∥2=√(∑xi2，以避免根号运算。
Lp范数：Lp范数是L1范数和L2范数的推广，定义为∥x∥p=(∑|xi|(1/p)。当p取不同值时，可以得到不同的范数。
L∞范数：向量x的L∞范数定义为向量各元素绝对值的最大值，即∥x∥∞=max(|xi|)。L∞范数常用于衡量向量的最大元素值。
Frobenius范数：对于矩阵X，其Frobenius范数定义为矩阵各元素平方和的平方根，即∥X∥F=√(∑xij^2)。Frobenius范数满足向量范数的所有性质，常用于衡量矩阵的“大小”。

=== 参考
《Linear Algebra and Its Applications》

== 概率论与数理统计
=== 基本概率论
概率论公理

=== 随机变量(random variable)
离散(discrete)随机变量(如骰子的每一面)、连续(continuous)随机变量(如人的体重和身高)

正态分布(normal distribution):
也称为高斯分布(gaussian distribution)
https://en.wikipedia.org/wiki/Normal_distribution
正态分布的前世今生:
https://cosx.org/2013/01/story-of-normal-distribution-1
https://cosx.org/2013/01/story-of-normal-distribution-2

=== 处理多个随机变量
联合概率
条件概率
贝叶斯定理
边际化
独立性

=== 期望和方差

=== 参数估计
==== 点估计
==== 矩估计
==== 最大似然估计
Q: 什么是似然(likelihood)？似然 vs. 概率(probability)

极大似然估计（Maximum Likelihood Estimation，简称 MLE）是一种统计推断方法，用于根据样本数据来估计概率分布的参数。其核心思想是：在给定样本数据的情况下，寻找使得样本出现概率最大的参数值。

基本原理:
假设有一个统计模型，其参数为θ，并且有一组观察数据X。最大似然估计的目标是找到使模型产生这组观察数据的概率最大的参数θ:
1. 定义似然函数：似然函数L(θ|X)表示在给定参数θ下，模型产生观察数据X的概率。通常，会使用概率密度函数（对于连续数据）或概率质量函数（对于离散数据）来定义似然函数。
2. 最大化似然函数：通过求解使似然函数达到最大值的参数θ，得到最大似然估计值。这通常涉及到对似然函数求导，并找到其导数为0的点（即极值点），然后检查这些点以确定哪个是最大值。
3. 求解参数：对于某些模型，似然函数的最大化可能涉及复杂的数学运算，如数值优化方法。

最大似然估计的结果依赖于观察数据的数量和质量。
对于某些模型，最大似然估计可能不是唯一的，或者可能不存在（如当似然函数没有最大值时）。
最大似然估计通常不提供关于参数估计不确定性的直接信息。为了评估这种不确定性，可能需要使用其他方法，如贝叶斯估计或置信区间。
总的来说，最大似然估计是一种强大且广泛使用的统计方法，用于从观察数据中估计模型参数。

参考:
https://www.zhihu.com/question/54082000

==== 最小方差无偏估计
==== 贝叶斯估计
==== 区间估计

==== 差分方程、马尔可夫过程和概率论
===== 马尔可夫过程
一般的马尔可夫过程:
马尔可夫过程本质上是这样一个系统：为了预测n + 1时刻的行为，关键是看它在 n 时刻的状态。换句话说，知道如何到达 n 时刻的状态并不能为预测下一刻会发生什么提供任何额外的信息。

==== 最小二乘法
最小二乘法是确定数据最佳拟合线的一种方法，其证明会用到微积分和线性代数。

=== 参考
《The Probability Lifesaver》中文: 普林斯顿概率论读本
《Introduction to Probability》中文: 概率导论(第2版·修订版)
《Probability Theory》中文: 概率论沉思录
《伊藤清概率论》

== 参考
《Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning》
https://oi-wiki.org/math/
https://www.zhihu.com/question/266030969/answer/49307951365