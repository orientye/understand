:toc:
:toclevels: 5
:hardbreaks-option:

== TCP

=== 格式

https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_segment_structure

▪ TCP Flags
SYN - The SYN, or Synchronisation flag, is used as a first step in establishing a 3-way handshake between two hosts. Only the first packet from both the sender and receiver should have this flag set. The following diagram illustrates a 3-way handshake process.

ACK - The ACK flag, which stands for "Acknowledgment", is used to acknowledge the successful receipt of a packet. As we can see from the diagram above, the receiver sends an ACK as well as a SYN in the second step of the 3-way handshake process to tell the sender that it received its initial packet.

FIN - The FIN flag, which stands for "Finished", means there is no more data from the sender. Therefore, it is used in the last packet sent from the sender.

URG - The URG flag is used to notify the receiver to process the urgent packets before processing all other packets. The receiver will be notified when all known urgent data has been received. See RFC 6093 for more details.

PSH - The PSH flag, which stands for "Push", is somewhat similar to the URG flag and tells the receiver to process these packets as they are received instead of buffering them.

RST - The RST flag, which stands for "Reset", gets sent from the receiver to the sender when a packet is sent to a particular host that was not expecting it.  

ECE - This flag is responsible for indicating if the TCP peer is ECN capable. See RFC 3168 for more details.

CWR - The CWR flag, which stands for Congestion Window Reduced, is used by the sending host to indicate it received a packet with the ECE flag set. See RFC 3168 for more details.

NS (experimental) - The NS flag, which stands for Nonce Sum, is still an experimental flag used to help protect against accidental malicious concealment of packets from the sender. See RFC 3540 for more details.

▪ Synchronize Sequence Numbers
Has a dual role:
If the SYN flag is set (1), then this is the initial sequence number. The sequence number of the actual first data byte and the acknowledged number in the corresponding ACK are then this sequence number plus 1.
If the SYN flag is clear (0), then this is the accumulated sequence number of the first data byte of this segment for the current session.
RFC793中说, ISN会和一个假的时钟绑在一起, 这个时钟会在每4微秒对ISN做加一操作, 直到超过2^32, 又从0开始。这样, 一个ISN的周期大约是4.55个小时。因为, 假设TCP Segment在网络上的存活时间不会超过Maximum Segment Lifetime(MSL), 所以, 只要MSL的值小于4.55小时, 那么, 我们就不会重用到ISN。
一般来说, 各个OS实现不一样, It is a random number between 0 and 4,294,967,295。
Wireshark为了显示更友好, 使用了Relative SeqNum相对序号, 只要在右键菜单中的protocol preference中取消掉就可以看到"Absolute SeqNum".
Sequence Number的增加可能也不一样。

▪ Window size (16 bits)
The size of the receive window, which specifies the number of window size units that the sender of this segment is currently willing to receive.

=== 状态

      CLOSE
            The socket is not being used.

      LISTEN 
            The socket is listening for incoming connections. 
            Such sockets are not included in the output unless you specify the --listening (-l) or --all (-a) option.

      SYN_SENT
            The socket is actively attempting to establish a connection.

      SYN_RECV
            A connection request has been received from the network.

      ESTABLISHED
            The socket has an established connection.

      FIN_WAIT1
            The socket is closed, and the connection is shutting down.

      FIN_WAIT2
            Connection is closed, and the socket is waiting for a shutdown from the remote end.

      TIME_WAIT
            The socket is waiting after close to handle packets still in the network.

      CLOSE_WAIT
            The remote end has shut down, waiting for the socket to close.

      LAST_ACK
            The remote end has shut down, and the socket is closed. Waiting for acknowledgement.

      CLOSING
            Both sockets are shut down but we still don't have all our data sent.

==== sync
▪ 建立连接时SYN超时
如果server端接到了clien发的SYN后回了SYN-ACK后, client掉线了, server端没有收到client回来的ACK, 那么, 这个连接处于一个中间状态, 即没成功, 也没失败。于是, server端如果在一定时间内没有收到的TCP会重发SYN-ACK。Linux下,默认重试次数为5次, 5次的重试时间间隔为1s, 2s, 4s, 8s, 16s, 总共31s, 第5次发出后还要等32s都知道第5次也超时了, 因此，总共需要1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s, TCP才会把断开这个连接。

▪ SYN Flood攻击
一些恶意的人就为此制造了SYN Flood攻击: 给服务器发了一个SYN后就下线了, 于是服务器需要默认等63s才会断开连接, 这样, 攻击者就可以把服务器的syn连接的队列耗尽, 让正常的连接请求不能处理。于是, Linux下给了一个叫tcp_syncookies的参数来应对这个事: 当SYN队列满了后, TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去(又叫cookie), 如果是攻击者则不会有响应, 如果是正常连接, 则会把这个SYN Cookie发回来, 然后服务端可以通过cookie建连接(即使你不在SYN队列中)。请注意, 请先千万别用tcp_syncookies来处理正常的大负载的连接的情况。因为, synccookies是妥协版的TCP协议, 并不严谨。对于正常的请求, 应该调整三个TCP参数: tcp_synack_retries用来减少重试次数; tcp_max_syn_backlog用来增大SYN连接数; tcp_abort_on_overflow处理不过来干脆就直接拒绝连接。 

参考: https://coolshell.cn/articles/11564.html

==== time_wait
▪ Why TIME_WAIT state
To implement TCP's full-duplex connection termination reliably
To allow old duplicate segments to expire in the network

The first reason can be explained by looking at Figure 2.5 and assuming that the final ACK is lost. The server will resend its final FIN, so the client must maintain state information, allowing it to resend the final ACK. If it did not maintain this information, it would respond with an RST (a different type of TCP segment), which would be interpreted by the server as an error. If TCP is performing all the work necessary to terminate both directions of data flow cleanly for a connection (its full-duplex close), then it must correctly handle the loss of any of these four segments. This example also shows why the end that performs the active close is the end that remains in the TIME_WAIT state: because that end is the one that might have to retransmit the final ACK.

To understand the second reason for the TIME_WAIT state, assume we have a TCP connection between 12.106.32.254 port 1500 and 206.168.112.219 port 21. This connection is closed and then sometime later, we establish another connection between the same IP addresses and ports: 12.106.32.254 port 1500 and 206.168.112.219 port 21. This latter connection is called an incarnation of the previous connection since the IP addresses and ports are the same. TCP must prevent old duplicates from a connection from reappearing at some later time and being misinterpreted as belonging to a new incarnation of the same connection. To do this, TCP will not initiate a new incarnation of a connection that is currently in the TIME_WAIT state. Since the duration of the TIME_WAIT state is twice the MSL, this allows MSL seconds for a packet in one direction to be lost, and another MSL seconds for the reply to be lost. By enforcing this rule, we are guaranteed that when we successfully establish a TCP connection, all old duplicates from previous incarnations of the connection have expired in the network.

▪ Why 2MSL
参考:《Unix Networking Programming Volume 1, 3rd》2.7

▪ 问题: 如果存在大量的time_wait连接, 会影响新的连接的创建: 该socket需2MSL即2*(30s-2min)才会完全关闭释放
▪ 解决:
首选方法: 尽量让客户端主动关闭
方法二: There's another way to terminate a TCP connection and that's by aborting the connection and sending an RST rather than a FIN. This is usually achieved by setting the SO_LINGER socket option to 0. 
https://stackoverflow.com/questions/3757289/when-is-tcp-option-so-linger-0-required

不太好的方式:
tcp_to_recycle    for reduce MSL?
tcp_to_reuse or SO_REUSEADDR(Q: vs. SO_REUSEPORT)
tcp_max_tw_buckets(控制并发的TIME_WAIT的数量, 默认值是180000)
net.ipv4.tcp_fin_timeout

==== close_wait
▪ 问题: 如果存在大量的close_wait连接
▪ 解决: 通常是代码问题, 即被动关闭方未关闭socket造成

==== RST
▪ 什么情况下发生
1、目标端口未打开/目的主机或者网络路径中防火墙拦截: 目标会向对方发送RST
2、socket接收缓冲取Recv-Q中的数据未完全被应用程序读取时关闭该socket: 会向对方发送RST
3、向对端已关闭的socket发送数据: 对端会向发送方发送RST
4、使用SO_LINGER规定close()行为是发送RST, 而不是发送FIN
5、向对端已经消逝的连接中发送数据
消逝连接指的是, 当前这个连接状态操作系统已经不再维护, 其数据结构内核已经注销。
比如对端FIN_WAIT2超时后, 其实该连接已经不存在;
比如半打开(Half Open)连接的对端, 由于某种原因已经不存在;
比如服务器重启, 端口号不变, 此时客户端没有检测到服务器重启仍向服务器发送数据, 则收到服务器发来的RST;
比如客户端断网, 重新连接网络, 但是没有连接服务器, 此时服务器没有检测到客户端断网重连仍向客户端发送数据, 则收到客户端发来的RST;

另: https://stackoverflow.com/questions/251243/what-causes-a-tcp-ip-reset-rst-flag-to-be-sent

▪ 处理:
在TCP协议中, rst段标识复位, 用来异常的关闭连接。
1. 发送RST包关闭连接时, 不必等缓冲区的包都发出去, 直接就丢弃缓冲区中的包, 发送RST。
2. 收到RST包后, 也不必发送ACK包来确认。接收端收到RST知道发送端是异常关闭。

RFC793(page36, section3.4)

Reset Processing
In all states except SYN-SENT, all reset (RST) segments are validated by checking their SEQ-fields.  A reset is valid if its sequence number is in the window.  In the SYN-SENT state (a RST received in response to an initial SYN), the RST is acceptable if the ACK field acknowledges the SYN.

The receiver of a RST first validates it, then changes state.  If the receiver was in the LISTEN state, it ignores it.  If the receiver was in SYN-RECEIVED state and had previously been in the LISTEN state,then the receiver returns to the LISTEN state, otherwise the receiver aborts the connection and goes to the CLOSED state.  If the receiver was in any other state, it aborts the connection and advises the user and goes to the CLOSED state.

▪ 实现:
tcp_v4_send_reset(): https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_ipv4.c

▪ RST攻击与防御

==== 异常行为
▪ 对端程序异常结束
      崩溃/断电/重启
            一段时间内重连         本端有/无操作分别会发生什么
            一段时间内未重连       本端有/无操作分别会发生什么
            始终没有重连           本端有/无操作分别会发生什么

▪ 对端程序程序未结束
      拔掉网线/中间路径超时
            一段时间内             本端有/无操作分别会发生什么
            超过一段时间           本端有/无操作分别会发生什么

=== 超时与重传
==== ARQ(Automatic Repeat Request)
==== 基于计时器的重传
==== 快速重传
==== 带选择确认的重传(SACK, Selective Acknowledgment)
https://www.youtube.com/watch?v=VERgI8QaYPY
https://datatracker.ietf.org/doc/html/rfc2018

==== 伪超时重传
DSACK-重复SACK
Eifel检测算法
Forward-RTO Recovery
Eifel响应算法

=== 流控与拥塞
==== 概念
- 流量控制(点对点通信控制)

让发送方发送速率不要太快, 使接收方来得及接收。
基于滑动窗口的流量控制机制: 接收方根据接收缓存的大小动态控制发送方的发送窗口大小(调整TCP首部"窗口"字段值), 限制发送方网络注入报文的速率, 同时根据网络拥塞程度估计窗口值。

- 拥塞控制(全局, 涉及全网络主机、路由器等)

防止过多的数据注入网络, 可以使网络中的路由器或链路不致于过载。
算法: 慢开始、拥塞避免、快重传、快恢复

发送方维持一个拥塞窗口(cwnd)的状态变量, 该大小取决于网络拥塞程度, 并动态变化
变化原则: 网络无拥塞, 窗口增大;网络拥塞则减小

新建立的连接不能够一开始就大量发送数据包, 而是根据网络情况逐步增加每次发送的数量。
为防止cwnd增长过大引起网络阻塞, 设置慢开始门限ssthresh状态变量选择慢开始算法与拥塞避免算法

拥塞窗口缓慢增长, 每经过一个往返时间RTT就把发送发的拥塞窗口+1, 而不是加倍。
(以上两个阶段若出现拥塞, 门限设置为出现拥塞时发送窗口的一半, 窗口值设为1, 执行慢开始算法)

▪ 慢开始与拥塞避免

▪ 慢开始(cwnd指数型增长)

▪ 拥塞避免(cwnd线性增长)

▪ 快重传与快恢复

▪ 快重传
接收方在收到一个失序的报文段后立即发出重复确认, 发送方只要收到三个重复确认就应当立即重传尚未收到的报文段, 不必等待设置的重传计时器时间到期。

▪ 快恢复
发送方收到三个重复确认时, ssthresh门限减半, 然后执行拥塞避免算法。

==== 流控
▪ Flow Control解决的问题

▪ 延时确认(Delayed Acknowledgments)

The Nagle algorithm says that when a TCP connection has outstanding data that has not yet been acknowledged, small segments (those smaller than the SMSS) cannot be sent until all outstanding data is acknowledged. Instead, small amounts of data are collected by TCP and sent in a single segment when an acknowledg- ment arrives. 

▪ Nagle算法

禁用Nagle算法: TCP_NODELAY

vs TCP_CORK aggressively accumulates data. If TCP_CORK is enabled in a socket, it will not send data until the buffer fills to a fixed limit. Similar to Nagle's algorithm, it also accumulates data from user but until the buffer fills to a fixed limit not until receiving ACK. This will be useful while sending multiple blocks of data. But you have to be more careful while using TCP_CORK.Until 2.6 kernel, both of these options are mutually exclusive. But in later kernel, both of them can exist together. In such case, TCP_CORK will be given more preference.

TCP_CORK (or TCP_NOPUSH in FreeBSD)
If set, don't send out partial frames. All queued partial frames are sent when the option is cleared again. This is useful for prepending headers before calling sendfile(2), or for throughput optimization. As currently implemented, there is a 200-millisecond ceiling on the time for which output is corked by TCP_CORK. If this ceiling is reached, then queued data is automatically transmitted. This option can be combined with TCP_NODELAY only since Linux 2.5.71. This option should not be used in code intended to be portable.

tcp_nodelay on;  (nginx, go等默认)
dotnet: 默认不开启tcp_nodelay https://learn.microsoft.com/en-us/dotnet/api/system.net.sockets.tcpclient.nodelay
tcp_nopush off;  (nginx默认)

▪ 窗口大小
TCP头部的窗口大小表明接收端可以缓存空间的大小, 该字段为16位, 也就是64K, RFC 1323扩展之后, 就可以使用32位的值来表示窗口的大小了

▪ 滑动窗口

▪ 零窗口

▪ 糊涂窗口综合症(Silly Window Syndrome)

▪ Large Buffers and Auto-Tuning:

      net.core.rmem_max = 131071
      net.core.wmem_max = 131071
      net.core.rmem_default = 110592
      net.core.wmem_default = 110592
      In addition, the auto-tuning parameters are given by the following variables:
      net.ipv4.tcp_rmem = 4096 87380 174760
      net.ipv4.tcp_wmem = 4096 16384 131072

==== 拥塞
===== 概念
▪ Congestion
when a router is forced to discard data because it cannot handle the arriving traffic rate, is called congestion. 

▪ 命令

    查看支持的拥塞算法: sysctl net.ipv4.tcp_available_congestion_control
    查看使用的拥塞算法: sysctl net.ipv4.tcp_congestion_control

▪ 注意: 拥塞控制算法只与局部有关, 两端可以使用不同的拥塞算法

▪ 拥塞检测
丢包可能由拥塞(主要是有线网络)引起, 也可能由传输和接收错误(主要是无线网络)引起

▪ 显式拥塞通知
ECN允许拥塞控制的端对端通知而避免丢包。ECN为一项可选功能，如果底层网络设施支持，则可能被启用ECN的两个端点使用。
通常来说，TCP/IP网络通过丢弃数据包来表明信道阻塞。在ECN成功协商的情况下，ECN感知路由器可以在IP头中设置一个标记来代替丢弃数据包，以标明阻塞即将发生。数据包的接收端回应发送端的表示，降低其传输速率，就如同在往常中检测到包丢失那样。
https://en.wikipedia.org/wiki/Explicit_Congestion_Notification

▪ 减缓TCP发送
The sender’s actual (usable) window W is then written as the minimum of the receiver’s advertised window awnd and the congestion window: W = min(cwnd, awnd) 

▪ 1988年TCP Tahoe 提出了1)慢启动, 2)拥塞避免, 3)拥塞发生时的快速重传 
▪ 1990年TCP Reno 在Tahoe的基础上增加了4)快速恢复

▪ 对标准算法的改进
New Reno
SACK        FACK  限制传输   CWV(Congestion Window Validation)拥塞窗口校验

▪ 其它
HSTCP     BIC和CUBIC(linux2.6.18起默认)  Vegas  FAST CTCP(compound tcp 复合tcp)

▪ 常见的拥塞算法可以分为三类: 
▪ 基于路径时延(如Vegas、Westwood)
▪ 基于丢包(如Cubic、NewReno)
将路径时延上升作为发生拥塞的信号, 在单一的网络环境下(所有连接都使用基于路径时延的拥塞算法)是可行的, 但是在复杂的网络环境下, 带宽容易被其他算法抢占, 带宽利用率最低。
将丢包作为发生拥塞的信号, 其背后的逻辑是路由器、交换机的缓存都是有限的, 拥塞会导致缓存用尽, 进而队列中的一些报文会被丢弃。
拥塞会导致丢包, 但是丢包却不一定拥塞导致的。事实上, 丢包可以分为两类, 一类是拥塞丢包, 另一类是噪声丢包, 特别是在无线网络环境中, 数据以无线电的方式进行传递, 无线路由器信号干扰、蜂窝信号不稳定等都会导致信号失真, 最终数据链路层 CRC 校验失败将报文丢弃。
基于丢包的拥塞算法容易被噪声丢包干扰, 在高丢包率高延迟的环境中带宽利用率较低。
▪ 基于带宽时延探测(如BBR)
既然无法区分拥塞丢包和噪声丢包, 那么就不以丢包作为拥塞信号, 而是通过探测最大带宽和最小路径时延来确定路径的容量。抗丢包能力强, 带宽利用率高。

三种类型的拥塞算法没有谁好谁坏, 都是顺应当时的网络环境的产物, 随着路由器、交换机缓存越来越大, 无线网络的比例越来越高, 基于路径时延和基于丢包的的拥塞算法就显得不合时宜了。对于流媒体、文件上传等对带宽需求比较大的场景, BBR成为更优的选择。

参考: https://www.infoq.cn/article/SY0KFJ2pyJomB6sAkqls

===== 慢启动(slow start)
1) 连接建好的开始先初始化cwnd = 1, 表明可以传一个MSS大小的数据(准确来说是smss, 发送方最大段大小, 大部分情况下, smss为接收方mss和路径mtu两者中较小值)
2) 每当收到一个ACK, cwnd++; 呈线性上升
3) 每当过了一个RTT, cwnd = cwnd*2; 呈指数上升
4) 上限ssthresh(slow start threshold), 当cwnd >= ssthresh时, 就会进入拥塞避免算法
因此, 如果网速很快的话, ACK也会返回得快, RTT也会短, 那么, 这个慢启动就一点也不慢。

Google的论文《An Argument for Increasing TCP’s Initial Congestion Window》Linux 3.0后采用了这篇论文的建议: 把cwnd初始化成了10个MSS。
而Linux 3.0以前, 比如2.6, Linux采用了RFC3390, cwnd是跟MSS的值来变的, 如果MSS < 1095则cwnd = 4;如果MSS > 2190则cwnd = 2; 其它情况下则为3。

===== 拥塞避免(Congestion Avoidance)
一般来说ssthresh的值是65535, 单位是字节, 当cwnd达到这个值时后, 算法如下: 
1) 收到一个ACK时, cwnd = cwnd + 1 / cwnd
2) 当每过一个RTT时, cwnd = cwnd + 1
这样就可以避免增长过快导致网络拥塞, 慢慢的增加调整到网络的最佳值。很明显, 是一个线性上升的算法。
注: ssthresh是变化的 ssthresh = max(flight size/2, 2*SMSS)
In Microsoft’s most recent ("Next Generation") TCP/IP stack, this equation is reportedly changed to the somewhat more conservative relationship: ssthresh = max(min(cwnd, awnd)/2, 2*SMSS) 

===== 拥塞发生时的快速重传(Congestion Avoidance)
当丢包的时候, 会有两种情况:
1) 等到RTO超时, 重传数据包。TCP认为这种情况太糟糕, 反应也很强烈。
sshthresh = cwnd / 2    cwnd重置为1, 进入慢启动过程
2) Fast Retransmit算法, 也就是在收到3个duplicate ACK时就开启重传, 而不用等到RTO超时。
TCP Tahoe的实现和RTO超时一样。
TCP Reno的实现是: 
cwnd = cwnd / 2    sshthresh = cwnd    进入快速恢复算法-Fast Recovery

===== 快速恢复(Fast Recovery)
TCP Reno, RFC5681
快速重传和快速恢复算法一般同时使用。
快速恢复算法是认为, 还有3个Duplicated Acks说明网络也不那么糟糕, 因此没有必要像RTO超时那么强烈。
注意, 正如前面所说, 进入Fast Recovery之前, cwnd和sshthresh已被更新: 
cwnd = cwnd / 2
sshthresh = cwnd
然后, 真正的Fast Recovery算法如下:
cwnd = sshthresh + 3 * MSS (3的意思是确认有3个数据包被收到了)
重传Duplicated ACKs指定的数据包
如果再收到duplicated Acks, 那么cwnd = cwnd + 1
如果收到了新的Ack, 那么cwnd = sshthresh , 然后就进入了拥塞避免的算法了。
这个算法也有问题, 那就是它依赖于3个重复的Acks。注意, 3个重复的Acks并不代表只丢了一个数据包, 很有可能是丢了好多包。但这个算法只会重传一个, 而剩下的那些包只能等到RTO超时, 于是, 进入了恶梦模式: 超时一个窗口就减半一下, 多个超时会超成TCP的传输速度呈级数下降, 而且也不会触发Fast Recovery算法了。

通常来说, 正如我们前面所说的, SACK或D-SACK的方法可以让Fast Recovery或Sender在做决定时更聪明一些, 但是并不是所有的TCP的实现都支持SACK(SACK需要两端都支持), 因此, 需要一个没有SACK的解决方案。而通过SACK进行拥塞控制的算法是FACK(后面会讲)。

TCP New Reno
于是, 1995年, TCP New Reno(RFC 6582)算法提出来, 主要就是在没有SACK的支持下改进Fast Recovery算法: 当sender这边收到了3个Duplicated Acks, 进入Fast Retransimit模式, 开发重传重复Acks指示的那个包。如果只有这一个包丢了, 那么, 重传这个包后回来的Ack会把整个已经被sender传输出去的数据ack回来。如果没有的话, 说明有多个包丢了。我们叫这个ACK为Partial ACK。
一旦Sender这边发现了Partial ACK出现, 那么, sender就可以推理出来有多个包被丢了, 于是乎继续重传sliding window里未被ack的第一个包。直到再也收不到了Partial Ack, 才真正结束Fast Recovery。这个过程我们可以看到, 这个"Fast Recovery的变更"是一个非常激进的玩法, 其同时延长了Fast Retransmit和Fast Recovery的过程。

===== BBR
▪ Google BBR(Bottleneck Bandwidth and Round-trip propagation time)
谷歌在2016年开发的一种新型的TCP 拥塞控制算法
Linux kernel 4.9

主要思想:
经典的拥塞控制算法比如reno/newReno/Cubic无一例外都是将丢包作为拥塞的信号, 然后降低发送速率。而在BBR中, 不考虑丢包, 而是基于这样一个定义: 当网络上的包数大于BDP(带宽时延乘积)时, 就认为出现了拥塞, 因此重点就在于如何准确地测量出瓶颈链路的带宽和整个链路的传播时延。

TCP BBR解决带宽和延迟无法同时测准的方法是:
交替测量带宽和延迟;
用一段时间内的带宽极大值(max bandwidth)和延迟极小值(min RTT)作为估计值。

BBR为什么快:
https://cloud.google.com/blog/products/networking/tcp-bbr-congestion-control-comes-to-gcp-your-internet-just-got-faster
在此以前，互联网主要使用基于丢包的拥塞控制策略，只依靠丢失数据包的迹象作为减缓发送速率的信号。这样做的的效果还是不错的，但随着全球化互联网的迅速普及，我们所使用的网络已经发生了巨大的变化。我们拥有了越来越大的带宽，而现在的互联网质量也越来越好。于是我们观察到了一些新的问题，比如影响延迟的缓冲区膨胀的问题。BBR尝试通过使用全新的拥塞控制来解决这个问题，它使用基于延迟而不是丢包作为决定发送速率的主要因素。
使用BBR，可以获得显著的网络吞吐量的提升和延迟的降低。吞吐量的改善在远距离路径上尤为明显，比如跨太平洋的文件或者大数据的传输，尤其是在有轻微丢包的网络条件下。延迟的改善主要体现在最后一公里的路径上，而这一路径经常受到缓冲膨胀(Bufferbloat)的影响。所谓"缓冲膨胀"指的网络设备或者系统不必要地设计了过大的缓冲区。当网络链路拥塞时，就会发生缓冲膨胀，从而导致数据包在这些超大缓冲区中长时间排队。在先进先出队列系统中，过大的缓冲区会导致更长的队列和更高的延迟，并且不会提高网络吞吐量。由于BBR并不会试图填满缓冲区，所以在避免缓冲区膨胀方面往往会有更好的表现。

优点:
网络在没有丢包的情况下，Cubic和BBR对于这些较长时延的链路都有很好的表现。
在中度丢包的情况下，BBR的表现就非常突出了。为什么这一点很重要呢？或者换一个说法，为什么要针对丢包情况而进行优化？
对于这个问题，让我们考虑一下这样的场景：我们在不同的地方放置有服务器，需要在系统或者服务器之间有源源不断的数据传输。例如日志文件、数据库同步、业务数据的异地备份等等。而在复杂的网络环境下，会因为各种原因而出现丢包的情况。在这种场景下，BBR将会大放异彩，帮助您维护更好的网络数据传输。
显而易见，BBR对所谓的"长肥网络"(带宽延迟积大、丢包率高的网络)非常有效，在CDN和视频应用等场景下也被证明有很好的表现。事实上，Youtube、Spotify和Dropbox大规模应用BBR已经有了很多的积累。这主要是因为BBR会积极地提升到最佳发送速率，使视频流加载或者文件下载速度更快。

缺点:
首先, 倾向于抢占cubic算法的带宽, 在网络公平性上略显不足
其次, BBR的机制会导致高重传率
再者, 在WIFI环境下用户的网速明显变慢

BBRv2:
解决第一版中存在的主要缺点:
使用聚合/运行中的参数增强了网络建模
增加了对ECN(显示拥塞通知)的支持等。

参考: https://aws.amazon.com/cn/blogs/china/talking-about-network-optimization-from-the-flow-control-algorithm/

=== 参考
https://zh.wikipedia.org/zh-hans/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6

=== keepalive
▪ 设置
setsockopt(sock, SOL_SOCKET, SO_KEEPALIVE, (char*)&value, sizeof(long));
setsockopt(sock, SOL_TCP, TCP_KEEPCNT, (char*)&value, sizeof(long));
setsockopt(sock, SOL_TCP, TCP_KEEPIDLE, (char*)&value, sizeof(long));
setsockopt(sock, SOL_TCP, TCP_KEEPINTVL, (char*)&value, sizeof(long));

▪ TCP与Keep-Alive
TCP协议的实现中，提供了KeepAlive报文，用来探测连接的对端是否存活。在应用交互的过程中，可能存在以下几种情况：
客户端或服务器意外断电，死机，崩溃，重启;
中间网络已经中断，而客户端与服务器并不知道;
利用保活探测功能，可以探知这种对端的意外情况，从而保证在意外发生时，可以释放半打开的TCP连接。

▪ 应用层心跳保活
虽然TCP提供了KeepAlive机制，但是并不能替代应用层心跳保活。原因主要如下：
(1) Keep Alive机制开启后，TCP层将在定时时间到后发送相应的KeepAlive探针以确定连接可用性。默认时间为7200s(两小时)，失败后重试10次，每次超时时间75s。显然默认值无法满足移动网络下的需求;
tips: sudo sysctl -a | grep keepalive
// net.ipv4.tcp_keepalive_time = 7200 每隔7200s检测一次
// net.ipv4.tcp_keepalive_probes = 9 一次最多重传9个包
// net.ipv4.tcp_keepalive_intvl = 75 每个包的间隔重传间隔75s
(2) 即便修改了(1)中的默认值，也不能很好的满足业务需求。TCP的KeepAlive用于检测连接的死活而不能检测通讯双方的存活状态。比如某台服务器因为某些原因导致负载超高，无法响应任何业务请求，但是使用TCP探针则仍旧能够确定连接状态，这就是典型的连接活着但业务提供方已死的状态，对客户端而言，这时的最好选择就是断线后重新连接其他服务器，而不是一直认为当前服务器是可用状态，一直向当前服务器发送些必然会失败的请求。
(3) socks代理会让Keep Alive失效。socks协议只管转发TCP层具体的数据包，而不会转发TCP协议内的实现细节的包。因此，一个应用如果使用了socks代理，那么TCP的KeepAlive机制就失效了。
(4) 部分复杂情况下Keep Alive会失效，如路由器挂掉，网线直接被拔除等
因此，KeepAlive并不适用于检测双方存活的场景，这种场景还得依赖于应用层的心跳。
应用层心跳也具备着更大的灵活性，可以控制检测时机，间隔和处理流程，甚至可以在心跳包上附带额外信息。

▪ HTTP与Keep-Alive
实现HTTP/1.0 keep-alive连接的客户端可以通过包含Connection:Keep-Alive首部请求将一条连接保持在打开状态，如果服务器愿意为下一条请求将连接保持在打开状态，就在响应中包含相同的首部。如果响应中没有Connection: Keep-Alive首部，客户端就认为服务器不支持keep-alive，会在发回响应报文之后关闭连接。HTTP/1.1以后Keep-Alive是默认打开的。

Q: 应用层心跳是客户端主动发送, 还是服务器主动发送呢? 
A: 通常由客户端主动发起，https://www.zhihu.com/question/35896874
通常做法:
1 客户端每隔一个时间间隔发生一个探测包给服务器，同时启动一个超时定时器
2 服务器端接收到检测包，返回一个应答包，同时也会启动一个超时定时器
3 如果客户机:
3.1 正常收到服务器的应答包，说明正常，删除超时定时器，继续步骤1
3.2 接收服务器的应答包超时，说明出问题了，通常需要销毁连接
4 如果服务器:
4.1 正常收到客户端的探测包，则说明正常，重新启动超时定时器 
4.2 接受客户端的探测包超时，也说明出问题了，通常需要销毁连接

=== 参考
https://en.wikipedia.org/wiki/Transmission_Control_Protocol