:toc:
:toclevels: 5
:hardbreaks-option:

== 性能优化

=== 概念
C10K问题, C1000K(百万), C10M(千万)

=== 指标与工具
指标与工具:
[format="csv", options="header", separator=#]
|===
指标#工具
b(bit)ps吞吐量 #                      sar, nethogs, iftop
p(package)ps #                        sar, /proc/net/dev
连接数 #                              netstat, ss
延迟 #                                ping hping3
连接跟踪 #                            conntrack
路由 #                                mtr, route, traceroute
DNS #                                dig, nslookup
防火墙 #                              iptables, firewall-cmd
网卡功能#                             ethtool
丢包 #                                netstat, ifconfig, ethtool
抓包 #                                tcpdump, wireshark
内核协议栈追踪 #                       bcc, systemtap
内核协议栈剖析 #                       perf
|===

=== 硬件
▪ RDMA

=== 设置
▪ 调小TCP协议的time_wait超时时间

    操作系统默认240秒后才会关闭处于time_wait状态的连接

▪ 调大服务器所支持的最大文件句柄数

    操作系统默认最大fd数量为1024

▪ TCP发送缓冲区与接收缓冲区大小

    linux根据系统状态自动调整缓冲区大小
    相关参数由net.ipv4.tcp_wmem和net.ipv4.tcp_rmem控制, 参数是一个三元组(min, default, max)
    但如果在socket上设置SO_SNDBUF或SO_RCVBUF, 会关闭缓冲区的系统动态调整功能
    除非明确自己的需求,并进行充分的评估和验证, 否则不要轻易设置TCP缓冲区大小
    
    大小如何设置:
    取决于协议, 太大或太小都不合适
    参考: https://stackoverflow.com/questions/2811006/what-is-a-good-buffer-size-for-socket-programming

▪ 开启tcp_nodelay选项

    Nagle算法是时代的产物, 当时网络带宽有限, 现在带宽则宽裕得多
    因此目前的TCP/IP协议栈默认将Nagle算法关闭，即SO_NODELAY=1。

=== API/库/框架
▪ unix domain sockets

    unix domain sockets vs. internet sockets(localhost):
    https://lists.freebsd.org/pipermail/freebsd-performance/2005-February/001143.html

▪ io_uring

▪ QUIC

▪ DPDK(Data Plane Development Kit)
    
    思想: kernel pass(内核旁路)
    优点: 极致的性能
    缺点: 
        很难与现有系统集成
        上层应用必须将内核中已经成熟的模块在用态重新实现一遍, 如路由表, 高层协议栈
        内核提供的常见工具和部署方式在一些情况下不可用
        系统越来越复杂，且破坏了内核把控的安全边界
    参考: https://en.wikipedia.org/wiki/Data_Plane_Development_Kit

▪ XDP(eXpress Data Path)

    思想: 给内核网络栈添加可编程能力
    缺点: 性能上不如kernel pass方案
    优点:
        与内核协议栈协同工作, 保持了内核安全边界, 无需配置和工具的修改
        无需任何硬件特性, 现有驱动只需一些更改就能支持XDP hooks
        可以选择性地复用内核网络栈的现有功能
        对应用透明
        服务不中断的前提下动态重新编程
        无需预留专门的CPU做包处理，CPU功耗与流量高低直接相关更节能
    参考: https://en.wikipedia.org/wiki/Express_Data_Path

=== 性能测试