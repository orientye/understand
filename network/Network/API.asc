:toc:
:toclevels: 5
:hardbreaks-option:

== API

=== socket setsockopt/getsockopt
man socket
https://linux.die.net/man/7/tcp
https://linux.die.net/man/7/udp

=== bind
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

bind之前一般都会setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, ...)

Q: SO_REUSEADDR vs. SO_REUSEPORT
https://stackoverflow.com/questions/14388706/how-do-so-reuseaddr-and-so-reuseport-differ

Q: TCP和UDP可以同时绑定相同的端口吗？
A: 可以。TCP/UDP各自的端口号相互独立，互不影响: 当主机收到数据包后，根据IP包头的协议号字段确定该数据包是TCP还是UDP来处理，传递给TCP/UDP模块的报文将根据端口号确定送给哪个应用程序处理。

Q: 多个TCP服务进程可以绑定同一个端口吗？
A: 如果两个TCP服务进程绑定的IP地址不同，而端口相同的话，也是可以绑定成功的；
如果两个TCP服务进程绑定的IP地址相同，端口也相同的话，也是可以绑定成功的: 需要设置SO_REUSEPORT。

=== listen
int listen(int sockfd, int backlog);

server建立连接会维护两个队列:
一个存放SYN的队列(即半连接队列) - 对应SYN_REVD状态(pending socket queue)
一个存放完成连接的队列(即全连接队列) - 对应ESTABELLISHED状态(established socket queue)

全连接队列长度 = min(backlog, 内核参数net.core.somaxconn(默认为128));
半连接队列长度 = min(backlog, 内核参数net.core.somaxconn, 内核参数tcp_max_syn_backlog)
当使用SYNCookie时(即内核参数net.ipv4.tcp_syncookies=1), tcp_max_syn_backlog无效

example:

    Redis: #define CONFIG_DEFAULT_TCP_BACKLOG       511    /* TCP listen backlog. */
    LibUV: 大部分是128
    Nginx: #define NGX_LISTEN_BACKLOG  -1

Q: 队列满了系统是如何处理的？

backlog:
参考: https://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html

Q: 服务端没有listen的情况下，客户端发起连接建立，会发生什么？
A: 服务端会回应RST报文。

=== accept
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
int accept4(int sockfd, struct sockaddr *addr, socklen_t *addrlen, int flags);

非阻塞accept:
    If the socket is marked nonblocking and no pending connections are present on the queue, accept() fails with the error EAGAIN or EWOULDBLOCK

https://stackoverflow.com/questions/7003234/which-systems-define-eagain-and-ewouldblock-as-different-values

accept惊群

example:

redis:
[source, c]
.https://github.com/redis/redis/blob/unstable/src/socket.c
----
static void connSocketAcceptHandler(aeEventLoop *el, int fd, void *privdata, int mask) {
    int cport, cfd, max = MAX_ACCEPTS_PER_CALL;
    char cip[NET_IP_STR_LEN];
    UNUSED(el);
    UNUSED(mask);
    UNUSED(privdata);

    while(max--) {
        cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &cport);
        if (cfd == ANET_ERR) {
            if (errno != EWOULDBLOCK)
                serverLog(LL_WARNING,
                    "Accepting client connection: %s", server.neterr);
            return;
        }
        serverLog(LL_VERBOSE,"Accepted %s:%d", cip, cport);
        acceptCommonHandler(connCreateAcceptedSocket(cfd, NULL),0,cip);
    }
}
----

nginx:
[source, c]
.https://github.com/nginx/nginx/blob/master/src/event/ngx_event_accept.c
----
void
ngx_event_accept(ngx_event_t *ev)
{
    //...
#if (NGX_HAVE_ACCEPT4)
        if (use_accept4) {
            s = accept4(lc->fd, &sa.sockaddr, &socklen, SOCK_NONBLOCK);
        } else {
            s = accept(lc->fd, &sa.sockaddr, &socklen);
        }
#else
        s = accept(lc->fd, &sa.sockaddr, &socklen);
#endif

        if (s == (ngx_socket_t) -1) {
            err = ngx_socket_errno;

            if (err == NGX_EAGAIN) {
                ngx_log_debug0(NGX_LOG_DEBUG_EVENT, ev->log, err,
                               "accept() not ready");
                return;
            }
            //...
        }
    //...
}
----

Q: 没有accept，能建立TCP连接吗？
能。三次握手照常进行并建立连接。
在服务端执行accept()前，如果客户端发送消息给服务端，服务端是能够正常回复ack确认包的。
一段时间后，服务端正常执行accept()，客户端之前发送的消息，也是能正常收到的。
accept()主要是从全连接队列里取出一条连接。

=== getaddrinfo
getaddrinfo: https://man7.org/linux/man-pages/man3/getaddrinfo.3.html

getaddrinfo_a: https://man7.org/linux/man-pages/man3/getaddrinfo_a.3.html

windows: https://learn.microsoft.com/zh-cn/windows/win32/api/ws2tcpip/nf-ws2tcpip-getaddrinfoexa

example:

    libuv

=== connect
int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

非阻塞Connect:

linux: EINPROGRESS

The socket is non-blocking and the connection cannot be completed immediately.  It is possible to select(2) or poll(2) for completion by  selecting the  socket  for  writing.   After  select(2) indicates writability, use getsockopt(2) to read the SO_ERROR option at level 
SOL_SOCKET to determine whether connect() completed successfully (SO_ERROR is zero) or unsuccessfully (SO_ERROR is one of the usual error codes listed here, explaining the reason for the failure).

其它类型的失败需要close socket

example:
redis:
[source, c]
----
    if (connect(s,(struct sockaddr*)&sa,sizeof(sa)) == -1) {                                                                                                                     
            if (errno == EINPROGRESS &&
    390             flags & ANET_CONNECT_NONBLOCK)
    391             return s;
    392 
    393         anetSetError(err, "connect: %s", strerror(errno));
    394         close(s);
    395         return ANET_ERR;
    396     }
----
nginx: core/ngx_resolver.c/ngx_tcp_connect

UDP:
https://stackoverflow.com/questions/6189831/whats-the-purpose-of-using-sendto-recvfrom-instead-of-connect-send-recv

=== send
write/writev/send/sendto/sendmsg

    ssize_t write(int fd, const void *buf, size_t count);
    ssize_t writev(int fd, const struct iovec *iov, int iovcnt);
    ssize_t send(int sockfd, const void *buf, size_t len, int flags);
    ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);
    ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);

阻塞/非阻塞

    send实质上仅仅是把数据放入发送缓冲区而已。
    阻塞和非阻塞区别在于是否等待发送缓冲区腾出足够的空间。

Q: 阻塞下的write/read表现
参考:
https://www.cnblogs.com/junneyang/p/6126635.html

Q: why sendmsg/recvmsg

    A few things recvmsg and sendmsg can do:
    You can do scatter/gather buffers. For example, let's day you want to receive exactly 1MB of data, but you only have 10 buffers that are each 100KB, then you can fill up each in a single recvmsg call.
    Access to Control flags, ancillary data, and IP packet header fields. For example, for UDP, you can get the destination IP/port address that the packet was addressed by enumerating the control data (with certain ioctls enabled) returned from recvmsg.

example:

    redis: networking.c/writeToClient
    nginx: src/os/unix/ngx_unix_send

SIGPIPE一般为什么要忽略

    A SIGPIPE is sent to a process if it tried to write to a socket that had been shutdown for writing or isn't connected (anymore).
    UNNP 5.13：When a process writes to a socket that has received an RST, the SIGPIPE signal is sent to the process

=== recv
read/readv/recv/recvfrom/recvmsg 

    ssize_t read(int fd, void *buf, size_t count);
    ssize_t readv(int fd, const struct iovec *iov, int iovcnt);
    ssize_t recv(int sockfd, void *buf, size_t len, int flags);
    ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);
    ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags);

阻塞/非阻塞

example:

redis:networking.c/readQueryFromClient
[source, c]
.https://github.com/redis/redis/blob/unstable/src/networking.c
----
    if (nread == -1) {
        if (errno == EAGAIN) {
            return;
        } else {
            serverLog(LL_VERBOSE, "Reading from client: %s",strerror(errno));
            freeClient(c);
            return;
        }
    } else if (nread == 0) {
        serverLog(LL_VERBOSE, "Client closed connection");
        freeClient(c);
        return;
    } else
----
nginx:os/unix/ngx_send.c/ngx_recv.c

=== blocking
https://stackoverflow.com/questions/5407182/blocking-sockets-when-exactly-does-send-return

=== close
int close(int fd);

Q: close之前本端发送的数据, 能成功发送出去吗？
Q: close之后本端能发送数据吗？对端呢？
Q: close之后本端能接收数据吗？对端呢？

异常释放

    发送一个复位报文段而不是FIN来释放一个连接, 称为异常释放(ahortive release)。
    异常终止一个连接对应用程序来说有两个优点:
    (1)丢弃任何待发数据并立即发送复位报文段;
    (2)RST的接收方会区分另一端执行的是异常关闭还是正常关闭。
    应用程序使用的API必须提供产生异常关闭而不是正常关闭的手段。
    Socket API通过lingeronclose选项(SO_LINGER)提供了这种异常关闭的能力: 停留时间设为0。

SO_LINGER

    struct linger{
        int l_onoff;
        int l_linger;
    };
    注意: 需要考虑平台(以及阻塞/非阻塞?)
    l_onoff = 0
    l_onoff != 0, l_linger = 0
    l_onoff !=0, l_linger > 0

[source, c]
.https://github.com/torvalds/linux/blob/master/net/ipv4/tcp.c
----
    void tcp_close(struct sock *sk, long timeout)
        else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)
            /* Check zero linger _after_ checking for unread data. */
            sk->sk_prot->disconnect(sk, 0);
            NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
----

[source, c]
.https://github.com/torvalds/linux/blob/master/net/ipv4/af_inet.c
----
int inet_release(struct socket *sock)
    /* If linger is set, we don't return until the close
        * is complete.  Otherwise we return immediately. The
        * actually closing is done the same either way.
        *
        * If the close is due to the process exiting, we never
        * linger..
        */
    timeout = 0;
    if (sock_flag(sk, SOCK_LINGER) &&
        !(current->flags & PF_EXITING))
        timeout = sk->sk_lingertime;
    sk->sk_prot->close(sk, timeout);
    sock->sk = NULL;
----

▪ vs. shutdown:
close: 关闭本进程的socket id, 但链接还是开着的, 用这个socket id的其它进程还能用这个链接, 能读或写这个socket id。
shutdown: 破坏了socket链接, 读的时候可能侦探到EOF结束符, 写的时候可能会收到一个SIGPIPE信号, 这个信号可能直到socket buffer被填充了才收到, shutdown有一个关闭方式的参数, 0不能再读, 1不能再写, 2读写都不能。

▪ socket多进程中的shutdown、close的使用
当所有的数据操作结束以后, 可以调用close()函数来释放该socket, 从而停止在该socket上的任何数据操作:
close(sockfd);使用close中止一个连接, 但它只是减少描述符的参考数, 并不直接关闭连接, 只有当描述符的参考数为0时才关闭连接。因此在多进程/线程程序中, close只是确保了对于某个特定的进程或线程来说, 该连接是关闭的。使用client_fd = accept()后fork()以在子进程中处理请求, 此时在父进程中使用close()关闭该连接, 子进程仍可以继续使用该连接。

也可以调用shutdown()函数来关闭该socket。该函数允许你只停止在某个方向上的数据传输, 而一个方向上的数据传输继续进行。如你可以关闭某socket的写操作而允许继续在该socket上接受数据, 直至读入所有数据。int shutdown(int sockfd,int how);shutdown可直接关闭描述符, 不考虑描述符的参考数, 可选择中止一个方向的连接。

▪ 注意
1.如果有多个进程共享一个套接字, close每被调用一次, 计数减1, 直到计数为0时, 也就是所用进程都调用了close, 套接字将被释放。
2.在多进程中如果一个进程中shutdown(sfd, SHUT_RDWR)后其它的进程将无法进行通信。如果一个进程close(sfd)将不会影响到其它进程, 得自己理解引用计数的用法了。

▪ 更多关于close和shutdown的说明
1.只要TCP栈的读缓冲里还有未读取(read)数据, 则调用close时会直接向对端发送RST。
2.shutdown与socket描述符没有关系, 即使调用shutdown(fd, SHUT_RDWR)也不会关闭fd, 最终还需close(fd)。
3.可以认为shutdown(fd, SHUT_RD)是空操作, 因为shutdown后还可以继续从该socket读取数据, 这点也许还需要进一步证实。在已发送FIN包后write该socket描述符会引发EPIPE/SIGPIPE。
4.当有多个socket描述符指向同一socket对象时, 调用close时首先会递减该对象的引用计数, 计数为0时才会发送FIN包结束TCP连接。shutdown不同, 只要以SHUT_WR/SHUT_RDWR方式调用即发送FIN包。
5.SO_LINGER与close, 当SO_LINGER选项开启但超时值为0时, 调用close直接发送RST(这样可以避免进入TIME_WAIT状态, 但破坏了TCP协议的正常工作方式), SO_LINGER对shutdown无影响。
6.TCP连接上出现RST与随后可能的TIME_WAIT状态没有直接关系, 主动发FIN包方必然会进入TIME_WAIT状态, 除非不发送FIN而直接以发送RST结束连接。

参考:
https://stackoverflow.com/questions/4160347/close-vs-shutdown-socket
https://blog.codingnow.com/2021/02/skynet_tcp_halfclose.html

=== shutdown

==== 使用
int shutdown(int sockfd, int how);

     how-The constants SHUT_RD, SHUT_WR, SHUT_RDWR have the value 0, 1, 2

参考: https://linux.die.net/man/3/shutdown

==== 场景
1. 当想要确保所有写好的数据已经发送成功时。如果在发送数据的过程中, 网络意外断开或者出现异常, 系统不一定会返回异常, 这时可能以为对端已经接收到数据了。此时需要用shutdown()来确定数据是否发送成功, 因为调用shutdown()时只有在缓存中的数据全部发送成功后才会返回。
2. 当程序使用了fork()或者使用多线程时, 想防止其他线程或进程访问到该资源, 又或者想立刻关闭这个socket, 那么可以用shutdown()来实现。

Given that we can close a socket, why is shutdown needed?
There are several reasons.
First, close will deallocate the network endpoint only when the last active reference is closed. If we duplicate the socket (with dup, for example), the socket won’t be deallocated until we close the last file descriptor referring to it. The shutdown function allows us to deactivate a socket independently of the number of active file descriptors referencing it.
Second, it is sometimes convenient to shut a socket down in one direction only. For example, we can shut a socket down for writing if we want the process we are communicating with to be able to tell when we are done transmitting data, while still allowing us to use the socket to receive data sent to us by the process.
参考: 《Advanced.Programming.in.the.UNIX.Environment.3rd.Edition》16.2

==== 示例
redis:

    ./src/socket.c:    shutdown(conn->fd, SHUT_RDWR);

nginx:

    ./src/os/unix/ngx_socket.h:  #define NGX_RDWR_SHUTDOWN  SHUT_RDWR
    ./src/os/win32/ngx_socket.h: #define NGX_RDWR_SHUTDOWN  SD_BOTH

=== select/poll
- select
- poll
- 场景: 跨平台, 对效率要求不高的场景

pselect vs select:

    超时精度
        select使用的是struct timeval结构体，精确到微秒
        pselect使用的是struct timespec结构体，精确到纳秒
    超时参数
        select可能会更新其超时参数timeout
        pselect6系统调用可能会更新其超时参数，glibc的封装pselect不会更新其超时参数
    信号掩码
        pselect可以设置信号掩码，若其为NULL，则行为与select相同

ppoll vs poll:

    超时精度
        poll使用的超时精度为毫秒
        ppoll使用的是struct timespec结构体，精确到纳秒
    信号掩码
        ppoll可以设置信号掩码，若其为NULL，则行为与poll相同

=== epoll/kqueue/evport
- epoll_create/epollcreate1
https://man7.org/linux/man-pages/man2/epoll_create.2.html

- epoll_ctrl
https://man7.org/linux/man-pages/man2/epoll_ctl.2.html

- epoll_wait
https://man7.org/linux/man-pages/man2/epoll_wait.2.html

- LT/ET

- example

[source, c]
.https://github.com/redis/redis/blob/unstable/src/ae_epoll.c
----
    if (e->events & EPOLLIN) mask |= AE_READABLE;
    if (e->events & EPOLLOUT) mask |= AE_WRITABLE;
    if (e->events & EPOLLERR) mask |= AE_WRITABLE|AE_READABLE;
    if (e->events & EPOLLHUP) mask |= AE_WRITABLE|AE_READABLE;
----

Q: why EPOLLOUT

- epoll惊群
EPOLLEXCLUSIVE(since Linux 4.5): https://man7.org/linux/man-pages/man2/epoll_ctl.2.html
参考: https://zhuanlan.zhihu.com/p/359774959

=== iocp
https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports

=== io_uring

=== Q&A
Q: linux中每个TCP连接最少占用多少内存？
A: 3k多一点 https://zhuanlan.zhihu.com/p/25241630

Q: 一台服务器最多可以支撑多少条TCP连接？假设所有的TCP连接都是空连接，那么一台服务器上最多可以支撑多少条TCP连接？一台机器有可能撑起100W的并发长连接吗？

Q: 一台客户端最多可以支撑多少条TCP连接？

Q: 在端口不足的情况下，connect系统调用的CPU消耗会大幅度增加？原因是？