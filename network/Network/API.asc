:toc:
:toclevels: 5
:hardbreaks-option:

== API

=== socket setsockopt/getsockopt
man socket
https://man7.org/linux/man-pages/man7/tcp.7.html
https://man7.org/linux/man-pages/man7/udp.7.html

=== 错误码
==== EAGAIN or EWOULDBLOCK
https://stackoverflow.com/questions/7003234/which-systems-define-eagain-and-ewouldblock-as-different-values

在绝大多数现代系统中，EAGAIN 和 EWOULDBLOCK 是同一个错误，具有相同的数值。它们都表示：在一个非阻塞（non-blocking）的文件描述符（比如套接字）上进行 I/O 操作时，该操作无法立即完成，如果这是一个阻塞的描述符，那么调用将会被阻塞（"挂起"），直到数据就绪。

具体场景:
读操作（recv, read）
原因: 套接字的接收缓冲区暂时没有数据可读。
应对: 稍后（例如在 epoll 或 select 报告该套接字可读时）再次尝试读取。
写操作（send, write）
原因: 套接字的发送缓冲区已满，无法容纳要发送的数据。
应对: 稍后（例如在 epoll 或 select 报告该套接字可写时）再次尝试写入。
接受连接（accept）
原因: 监听套接字的已完成连接队列为空，没有新的连接可以立即接受。
应对: 稍后（例如在 epoll 或 select 报告监听套接字可读时）再次尝试接受。

这主要是历史原因造成的，EAGAIN 源于 System V，EWOULDBLOCK 则源于 BSD。
现代情况: 在 Linux、macOS、FreeBSD 等几乎所有可能会遇到的现代操作系统中，EAGAIN 和 EWOULDBLOCK 被定义为相同的错误码，以linux为例:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/uapi/asm-generic/errno.h
----
#define	EWOULDBLOCK	EAGAIN	/* Operation would block */
----

代码示例:
[source, c]
----
ssize_t n;
char buffer[BUFFER_SIZE];
// 在一个非阻塞套接字 fd 上读取数据
n = recv(fd, buffer, sizeof(buffer), 0);
if (n < 0) {
    if (errno == EAGAIN || errno == EWOULDBLOCK) {
        // 没有数据可读，这是正常情况，不是错误。
        // 返回，等待下次事件循环再试。
        return; // 或者 continue; 在循环中
    } else {
        // 这是一个真正的错误（如连接重置 ECONNRESET）
        perror("recv");
        close(fd);
        return;
    }
} else if (n == 0) {
    // 对端关闭了连接
    close(fd);
    return;
}
process_data(buffer, n);
----

==== EINPROGRESS or EWOULDBLOCK
EINPROGRESS 不是真正的错误，而是一种状态指示，意思是"Operation now in progress"（操作正在进行中）。
EINPROGRESS 和 EWOULDBLOCK 在某些系统上可能是同一个值，但最好都检查一下。

为什么会出现 EINPROGRESS？
当一个非阻塞的 Socket 调用 connect() 时，如果无法立即建立连接，但连接尝试已经成功启动，那么 connect() 会返回 -1 并将 errno 设置为 EINPROGRESS。
TCP 连接需要经过三次握手过程：客户端发送 SYN、服务器回复 SYN-ACK、客户端发送 ACK。
在阻塞模式下，connect() 会等待所有这三个步骤完成才返回。但在非阻塞模式下，connect() 在发出第一个 SYN 包后就立即返回，含义是：已经开始工作了，但还没完成，请稍后检查结果。

处理:
EINPROGRESS 是非阻塞 Connect 的核心机制：
它表示"操作已开始，请稍后查询结果"
需要配合 I/O 多路复用（select/poll/epoll）来等待连接完成
使用 getsockopt(fd, SOL_SOCKET, SO_ERROR, ...) 来获取最终连接状态

==== SIGPIPE与EPIPE
- 当以下所有条件满足时，会触发 SIGPIPE 信号:
** 向已关闭的管道或套接字写入数据
** 对方是通过关闭读端来断开连接的（对于套接字，就是收到了 FIN 包）
** 这是第一次尝试写入（某些情况下是第二次）
    *** 第一次还是第二次写入触发？ 这取决于时序：
    *** 如果 write 时内核已经知道连接关闭（收到了 RST），第一次写入就触发
    *** 如果 write 时只收到了 FIN，数据可能进入发送缓冲区，第二次写入才触发
*** 没有忽略 SIGPIPE 信号


- 当以下任一情况发生时，write 返回 -1 且 errno == EPIPE:

    情况一: 忽略了 SIGPIPE 信号
        // 告诉系统忽略 SIGPIPE 信号
        signal(SIGPIPE, SIG_IGN);
        // 现在向已关闭的套接字写入会返回 EPIPE 而不是终止进程
        ssize_t n = write(sockfd, buffer, size);
        if (n == -1 && errno == EPIPE) {
            printf("对端已经关闭连接\n");
            close(sockfd);
        }
    情况二: 使用 send() 带有 MSG_NOSIGNAL 标志
        // MSG_NOSIGNAL 标志阻止 SIGPIPE 生成
        ssize_t n = send(sockfd, buffer, size, MSG_NOSIGNAL);
        if (n == -1 && errno == EPIPE) {
            // 安全地处理错误，进程不会终止
        }

=== bind
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
https://man7.org/linux/man-pages/man2/bind.2.html

bind之前一般都会setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, ...)

Q: SO_REUSEADDR vs. SO_REUSEPORT
在大多数服务器程序中，都应该设置 SO_REUSEADDR:
不设置 SO_REUSEADDR 的话，服务器程序（比如一个 Web 服务器在端口 8080）崩溃、被杀死或正常关闭后，如果试图立即重启它，会遇到类似 "Address already in use" 的错误。这是因为之前服务器使用的（地址，端口）对还处于 TIME_WAIT 状态，必须等待 2MSL 的时间后，服务器才能成功重启。这对于开发和运维来说是不可接受的。
SO_REUSEADDR 主要是一种 “宽容” 策略，解决了 TIME_WAIT 导致的地址无法重用问题。
SO_REUSEPORT 主要是一种 “并行” 策略，实现了真正的端口完全复用，用于性能扩展和负载均衡。
https://stackoverflow.com/questions/14388706/how-do-so-reuseaddr-and-so-reuseport-differ

Q: TCP和UDP可以同时绑定相同的端口吗？
A: 可以。TCP/UDP各自的端口号相互独立，互不影响: 当主机收到数据包后，根据IP包头的协议号字段确定该数据包是TCP还是UDP来处理，传递给TCP/UDP模块的报文将根据端口号确定送给哪个应用程序处理。
操作系统内核为TCP和UDP维护着不同的套接字表。
TCP套接字由一个四元组唯一标识: {源IP 源端口 目标IP 目标端口}。对于一个监听状态的服务器，它关心的是{本地IP 本地端口}。
UDP套接字虽然是无连接的，但同样由一个本地IP和本地端口来标识。
很多网络服务正是利用了这一点来在一个端口上同时提供两种协议的服务，最经典的例子是DNS（域名系统）：
DNS主要使用 UDP 53 端口进行快速的查询和响应。
但当响应数据太大（超过512字节）时，或者在进行区域传输等操作时，它会使用 TCP 53 端口来保证数据的可靠传输。

Q: 多个TCP服务进程可以绑定同一个端口吗？
A: 如果两个TCP服务进程绑定的IP地址不同，而端口相同的话，也是可以绑定成功的；
如果两个TCP服务进程绑定的IP地址相同，端口也相同的话，也是可以绑定成功的: 需要设置SO_REUSEPORT。

=== listen
int listen(int sockfd, int backlog);
https://man7.org/linux/man-pages/man2/listen.2.html

server建立连接会维护两个队列:
半连接队列(SYN Queue) - 存放收到客户端SYN包但还未完成三次握手的连接。这些连接处于SYN_RCVD状态。
全连接队列(Accept Queue) - 存放已完成三次握手，但尚未被应用程序通过accept()系统调用取走的连接。这些连接处于ESTABLISHED状态。

全连接队列长度 = min(backlog, 内核参数net.core.somaxconn(默认为128));
半连接队列长度 = min(backlog, 内核参数net.core.somaxconn, 内核参数tcp_max_syn_backlog)
当使用SYNCookie时(即内核参数net.ipv4.tcp_syncookies=1), tcp_max_syn_backlog无效

backlog:
参考: https://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html

example:

    Redis: 511
        ./src/config.c:    createIntConfig(...server.tcp_backlog, 511...)
        ./redis.conf:      tcp-backlog 511
    LibUV: 大部分是128
    Nginx: 大部分是511(linux/windows)，darwin/freebsd是-1
        ./src/os/unix/ngx_linux_config.h    #define NGX_LISTEN_BACKLOG        511

Q: 全连接队列满了系统是如何处理的？
原因: 应用程序调用accept()的速度跟不上连接建立的速度，导致已完成三次握手的连接堆积在全连接队列中，直到队列被填满。
处理: 当全连接队列满时，服务器的行为由/proc/sys/net/ipv4/tcp_abort_on_overflow参数决定:
tcp_abort_on_overflow = 0（默认值）：服务器会忽略客户端发来的ACK包，与此同时，客户端的连接状态却是 ESTABLISHED，只要服务器没有为请求回复 ACK，请求就会被多次重发。如果服务器上的进程只是短暂的繁忙造成 accept 队列满，那么当 TCP 全连接队列有空位时，再次接收到的请求报文由于含有 ACK，仍然会触发服务器端成功建立连接。
tcp_abort_on_overflow = 1: 服务器会直接回复RST包重置连接，客户端会看到“Connection reset by peer”之类的错误，这有助于快速失败。

    netstat -s | grep "overflowed"
        100 times the listen queue of a socket overflowed
    # 或者
    ss -lnt | grep -i wait

Q: 半连接队列满了系统是如何处理的？
原因:
    通常发生在两种场景下:
    高并发快速连接: 正常的突发流量。
    SYN Flood 攻击: 恶意客户端只发送 SYN 而不回复 ACK，故意占满队列，导致拒绝服务。
处理:
    情况一: tcp_syncookies = 1（推荐和现代系统的默认值）
    这是系统的主要防御模式。当半连接队列满时，它不会简单地丢弃新连接，而是启动启用 syncookie 机制。
    情况二: tcp_syncookies = 0
    直接丢弃新收到的 SYN 包，不做任何响应。在这种模式下，无论是攻击流量还是正常用户流量，都会被无差别地丢弃，导致服务不可用，这正中了 SYN Flood 攻击的下怀。

    netstat -natp | grep "SYN_RECV" | wc -l #处于半连接状态的 TCP 的个数

Q: 服务端没有listen的情况下，客户端发起连接建立，会发生什么？
A: 服务端会回应RST报文。

=== accept
==== accept()
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
int accept4(int sockfd, struct sockaddr *addr, socklen_t *addrlen, int flags);
https://man7.org/linux/man-pages/man2/accept.2.html
accept4 是 accept 的增强版，它允许在接受连接的同时，直接设置新套接字的某些属性（SOCK_NONBLOCK 和 SOCK_CLOEXEC），从而避免了需要再次调用 fcntl 等函数的额外步骤，使其更高效、更安全。

非阻塞accept:
If the socket is marked nonblocking and no pending connections are present on the queue, accept() fails with the error EAGAIN or EWOULDBLOCK

accept惊群:
早期内核采用了最简单的唤醒所有策略。当一个连接完成三次握手，进入全连接队列（accept queue）后，内核会检查有哪些进程正在这个套接字上休眠，然后把它们全部唤醒。
现代Linux内核（大约2.6版本以后）已经修复了这个问题。

==== redis
[source, c]
.https://github.com/redis/redis/blob/unstable/src/socket.c
----
static void connSocketAcceptHandler(aeEventLoop *el, int fd, void *privdata, int mask) {
    int cport, cfd, max = MAX_ACCEPTS_PER_CALL;
    char cip[NET_IP_STR_LEN];
    UNUSED(el);
    UNUSED(mask);
    UNUSED(privdata);

    while(max--) {
        cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &cport);
        if (cfd == ANET_ERR) {
            if (errno != EWOULDBLOCK)
                serverLog(LL_WARNING,
                    "Accepting client connection: %s", server.neterr);
            return;
        }
        serverLog(LL_VERBOSE,"Accepted %s:%d", cip, cport);
        acceptCommonHandler(connCreateAcceptedSocket(cfd, NULL),0,cip);
    }
}
----

==== nginx
[source, c]
.https://github.com/nginx/nginx/blob/master/src/event/ngx_event_accept.c
----
void
ngx_event_accept(ngx_event_t *ev)
{
    //...
#if (NGX_HAVE_ACCEPT4)
        if (use_accept4) {
            s = accept4(lc->fd, &sa.sockaddr, &socklen, SOCK_NONBLOCK);
        } else {
            s = accept(lc->fd, &sa.sockaddr, &socklen);
        }
#else
        s = accept(lc->fd, &sa.sockaddr, &socklen);
#endif

        if (s == (ngx_socket_t) -1) {
            err = ngx_socket_errno;

            if (err == NGX_EAGAIN) {
                ngx_log_debug0(NGX_LOG_DEBUG_EVENT, ev->log, err,
                               "accept() not ready");
                return;
            }
            //...
        }
    //...
}
----

==== Q&A
- Q: 没有accept，能建立TCP连接吗？
能。
** 三次握手独立于 accept():
    *** 客户端发送 SYN → 服务端内核自动回复 SYN-ACK → 客户端回复 ACK。此时，连接已在服务端内核的全连接队列（ESTABLISHED 状态）中，无需 accept() 参与。这是操作系统内核的行为，与用户态代码（如 accept()）无关。
** accept() 的作用:
    *** 仅仅是从全连接队列中取出一个已建立的连接，交给应用程序使用。不调用 accept() 只会导致队列堆积，不影响握手的完成。
** 没有 accept() 的影响:
    *** 连接会积压在全连接队列
        队列大小由 listen() 的 backlog 参数限制（例如 Linux 默认 128）。如果队列满，服务端内核会丢弃后续的 SYN，导致客户端超时重试。
    *** 客户端认为连接已建立
        客户端在收到 SYN-ACK 并发送 ACK 后，会进入 ESTABLISHED 状态，可能开始发送数据。但服务端应用层无法处理（因未 accept()），数据会积压在内核接收缓冲区。

=== getaddrinfo
getaddrinfo: https://man7.org/linux/man-pages/man3/getaddrinfo.3.html
getaddrinfo_a: https://man7.org/linux/man-pages/man3/getaddrinfo_a.3.html
一般用于客户端连接服务器前的DNS解析，getaddrinfo_a 是 getaddrinfo 的异步版本。

windows: https://learn.microsoft.com/zh-cn/windows/win32/api/ws2tcpip/nf-ws2tcpip-getaddrinfoexa

example:

    libuv

=== connect
==== connect()
int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
https://man7.org/linux/man-pages/man2/connect.2.html

非阻塞Connect:
linux: EINPROGRESS
The socket is nonblocking and the connection cannot be completed immediately.(UNIX domain sockets failed with EAGAIN instead.)
It is possible to select(2) or poll(2) for completion by selecting the socket for writing.
After select(2) indicates writability, use getsockopt(2) to read the SO_ERROR option at level SOL_SOCKET to determine whether connect() completed successfully (SO_ERROR is zero) or unsuccessfully (SO_ERROR is one of the usual error codes listed here, explaining the reason for the failure).
其它类型的失败需要close socket

The completion of a nonblocking connect is considered to make a socket writable. (UNP 6.10)

非阻塞 connect 的核心思想是：
将 socket 设置为非阻塞模式后调用 connect。connect 会立即返回，而不是等待连接完成。程序随后通过 select, poll, 或 epoll 等 I/O 多路复用技术来监听这个 socket 的“可写”事件，以判断连接是否成功建立。

立即返回与 EINPROGRESS：connect 在非阻塞模式下立即返回 -1 且 errno 为 EINPROGRESS 是正常现象。
可写事件代表完成：使用 I/O 多路复用监听 socket 的可写事件来获知连接建立完成。
必须检查最终状态：Socket 变为可写不代表连接成功，必须使用 getsockopt(fd, SOL_SOCKET, SO_ERROR, ...) 来获取真实的错误码。这是最容易被忽略也最关键的一步。
超时处理：必须为等待过程设置超时，避免无限期等待。
异常事件监听：为了健壮性，最好同时监听异常事件集合。

==== redis
以anetUnixGenericConnect()为例，anetTcpGenericConnect()其实类似:
[source, c]
.https://github.com/redis/redis/blob/unstable/src/anet.c
----
int anetUnixGenericConnect(char *err, const char *path, int flags)
{
    int s;
    struct sockaddr_un sa;

    if ((s = anetCreateSocket(err,AF_LOCAL)) == ANET_ERR)
        return ANET_ERR;

    sa.sun_family = AF_LOCAL;
    redis_strlcpy(sa.sun_path,path,sizeof(sa.sun_path));
    if (flags & ANET_CONNECT_NONBLOCK) {
        if (anetNonBlock(err,s) != ANET_OK) {
            close(s);
            return ANET_ERR;
        }
    }
    if (connect(s,(struct sockaddr*)&sa,sizeof(sa)) == -1) {
        if (errno == EINPROGRESS &&
            flags & ANET_CONNECT_NONBLOCK)
            return s;

        anetSetError(err, "connect: %s", strerror(errno));
        close(s);
        return ANET_ERR;
    }
    return s;
}
----
核心步骤:
connect前通过anetNonBlock函数设置非阻塞模式
创建非阻塞socket (anetTcpNonBlockConnect)
注册可写事件 (aeCreateFileEvent with AE_WRITABLE)
在回调中检查连接状态 (getsockopt with SO_ERROR)
连接成功后删除可写事件，注册读事件

==== nginx
[source, c]
.https://github.com/nginx/nginx/blob/master/src/core/ngx_resolver.c
----
static ngx_int_t
ngx_tcp_connect(ngx_resolver_connection_t *rec)
{
    //...
    if (ngx_nonblocking(s) == -1) {
        ngx_log_error(NGX_LOG_ALERT, &rec->log, ngx_socket_errno,
                      ngx_nonblocking_n " failed");

        goto failed;
    }
    //...
    rc = connect(s, rec->sockaddr, rec->socklen);

    if (rc == -1) {
        err = ngx_socket_errno;


        if (err != NGX_EINPROGRESS
#if (NGX_WIN32)
            /* Winsock returns WSAEWOULDBLOCK (NGX_EAGAIN) */
            && err != NGX_EAGAIN
#endif
            )
        {
            if (err == NGX_ECONNREFUSED
#if (NGX_LINUX)
                /*
                 * Linux returns EAGAIN instead of ECONNREFUSED
                 * for unix sockets if listen queue is full
                 */
                || err == NGX_EAGAIN
#endif
                || err == NGX_ECONNRESET
                || err == NGX_ENETDOWN
                || err == NGX_ENETUNREACH
                || err == NGX_EHOSTDOWN
                || err == NGX_EHOSTUNREACH)
            {
                level = NGX_LOG_ERR;

            } else {
                level = NGX_LOG_CRIT;
            }

            ngx_log_error(level, &rec->log, err, "connect() to %V failed",
                          &rec->server);

            ngx_close_connection(c);
            rec->tcp = NULL;

            return NGX_ERROR;
        }
    }
    //...
    if (rc == -1) {

        /* NGX_EINPROGRESS */

        if (ngx_add_event(wev, NGX_WRITE_EVENT, event) != NGX_OK) {
            goto failed;
        }

        return NGX_AGAIN;
    }
    //...
failed:

    ngx_close_connection(c);
    rec->tcp = NULL;

    return NGX_ERROR;
}
----

==== UDP
如果 UDP 应用需要与固定的对端进行频繁通信，使用 connect() 可以获得更好的性能和错误处理能力。如果是广播、多播或需要与多个不同对端通信的场景，则使用未连接的 UDP socket。
特性	未连接 UDP	已连接 UDP
发送函数	sendto()	send() / write()
接收函数	recvfrom()	recv() / read()
接收过滤	接收所有来源	只接收对端数据
性能	每次发送都要路由查找	路由信息缓存，更快
错误反馈	无异步错误反馈	有异步错误反馈
灵活性	可与任何主机通信	主要与固定对端通信
https://stackoverflow.com/questions/6189831/whats-the-purpose-of-using-sendto-recvfrom-instead-of-connect-send-recv
https://stackoverflow.com/questions/9741392/can-you-bind-and-connect-both-ends-of-a-udp-connection
示例:
https://www.geeksforgeeks.org/udp-server-client-implementation-c/
https://www.geeksforgeeks.org/computer-networks/udp-client-server-using-connect-c-implementation/

=== send
==== 函数
- write/writev/send/sendto/sendmsg

    ssize_t write(int fd, const void *buf, size_t count);
    ssize_t writev(int fd, const struct iovec *iov, int iovcnt);
    ssize_t send(int sockfd, const void *buf, size_t len, int flags);
    ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);
    ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);
    https://manpages.ubuntu.com/manpages/resolute/en/man2/write.2.html
    https://manpages.ubuntu.com/manpages/resolute/en/man2/writev.2.html
    https://manpages.ubuntu.com/manpages/resolute/en/man2/send.2.html

- 阻塞/非阻塞

    send实质上仅仅是把数据放入发送缓冲区而已。
    阻塞和非阻塞区别在于是否等待发送缓冲区腾出足够的空间。
    https://stackoverflow.com/questions/5407182/blocking-sockets-when-exactly-does-send-return

- Q: 阻塞下的write表现
    ** 充足空间: 立即写入所有数据，返回请求的字节数
    ** 部分空间: 写入尽可能多的数据，返回实际写入的字节数
    ** 无空间: 阻塞，直到有空间可用
        *** 空间可用 → 返回实际写入的字节数
        *** 对端关闭
            **** 对端正常关闭（发送 FIN）
            **** 对端异常关闭（发送 RST）
                立即返回 -1、errno 被设置为 ECONNRESET
    ** 出错: 返回 -1，通过 errno 判断错误

- Q: why sendmsg/recvmsg
A few things recvmsg and sendmsg can do:
You can do scatter/gather buffers. For example, let's day you want to receive exactly 1MB of data, but you only have 10 buffers that are each 100KB, then you can fill up each in a single recvmsg call.
Access to Control flags, ancillary data, and IP packet header fields. For example, for UDP, you can get the destination IP/port address that the packet was addressed by enumerating the control data (with certain ioctls enabled) returned from recvmsg.

- Q: SIGPIPE一般为什么要忽略?

    write(2) EPIPE:
        fd is connected to a pipe or socket whose reading end is closed. 
        When this happens the writing process will also receive a SIGPIPE signal.
        (Thus, the write return value is seen only if the program catches, blocks or ignores this signal.)
        https://man7.org/linux/man-pages/man2/write.2.html
    即防止进程因向已关闭的连接写入数据而被 SIGPIPE 信号意外终止

- Q: 如何正确地忽略SIGPIPE？

    方法一: 在发送时使用 MSG_NOSIGNAL 标志（推荐）
        ssize_t ret = send(sockfd, buf, len, MSG_NOSIGNAL);
    方法二: 在进程启动时全局忽略 SIGPIPE 信号
        signal(SIGPIPE, SIG_IGN);

==== redis
https://github.com/redis/redis/blob/unstable/src/networking.c

    sendReplyToClient
        writeToClient
            最终调用connWrite

[source, c]
.https://github.com/redis/redis/blob/unstable/src/connection.h
----
static inline int connWrite(connection *conn, const void *data, size_t data_len) {
    return conn->type->write(conn, data, data_len);
}
----

[source, c]
.https://github.com/redis/redis/blob/unstable/src/socket.c
----
static ConnectionType CT_Socket = {
    //...
    .write = connSocketWrite,
    .writev = connSocketWritev,
    //...
};
//...
static int connSocketWrite(connection *conn, const void *data, size_t data_len) {
    int ret = write(conn->fd, data, data_len);
    if (ret < 0 && errno != EAGAIN) {
        conn->last_errno = errno;

        /* Don't overwrite the state of a connection that is not already
         * connected, not to mess with handler callbacks.
         */
        if (errno != EINTR && conn->state == CONN_STATE_CONNECTED)
            conn->state = CONN_STATE_ERROR;
    }

    return ret;
}

static int connSocketWritev(connection *conn, const struct iovec *iov, int iovcnt) {
    int ret = writev(conn->fd, iov, iovcnt);
    if (ret < 0 && errno != EAGAIN) {
        conn->last_errno = errno;

        /* Don't overwrite the state of a connection that is not already
         * connected, not to mess with handler callbacks.
         */
        if (errno != EINTR && conn->state == CONN_STATE_CONNECTED)
            conn->state = CONN_STATE_ERROR;
    }

    return ret;
}
----

==== nginx
[source, c]
.https://github.com/nginx/nginx/blob/master/src/os/unix/ngx_send.c
----
ssize_t
ngx_unix_send(ngx_connection_t *c, u_char *buf, size_t size)
{
    ssize_t       n;
    ngx_err_t     err;
    ngx_event_t  *wev;

    wev = c->write;

#if (NGX_HAVE_KQUEUE)

    if ((ngx_event_flags & NGX_USE_KQUEUE_EVENT) && wev->pending_eof) {
        (void) ngx_connection_error(c, wev->kq_errno,
                               "kevent() reported about an closed connection");
        wev->error = 1;
        return NGX_ERROR;
    }

#endif

    for ( ;; ) {
        n = send(c->fd, buf, size, 0);

        ngx_log_debug3(NGX_LOG_DEBUG_EVENT, c->log, 0,
                       "send: fd:%d %z of %uz", c->fd, n, size);

        if (n > 0) {
            if (n < (ssize_t) size) {
                wev->ready = 0;
            }

            c->sent += n;

            return n;
        }

        err = ngx_socket_errno;

        if (n == 0) {
            ngx_log_error(NGX_LOG_ALERT, c->log, err, "send() returned zero");
            wev->ready = 0;
            return n;
        }

        if (err == NGX_EAGAIN || err == NGX_EINTR) {
            wev->ready = 0;

            ngx_log_debug0(NGX_LOG_DEBUG_EVENT, c->log, err,
                           "send() not ready");

            if (err == NGX_EAGAIN) {
                return NGX_AGAIN;
            }

        } else {
            wev->error = 1;
            (void) ngx_connection_error(c, err, "send() failed");
            return NGX_ERROR;
        }
    }
}
----

=== recv
==== 函数
- read/readv/recv/recvfrom/recvmsg 

    ssize_t read(int fd, void *buf, size_t count);
    ssize_t readv(int fd, const struct iovec *iov, int iovcnt);
    ssize_t recv(int sockfd, void *buf, size_t len, int flags);
    ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);
    ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags);
    https://manpages.ubuntu.com/manpages/resolute/en/man2/read.2.html
    https://manpages.ubuntu.com/manpages/resolute/en/man2/readv.2.html
    https://manpages.ubuntu.com/manpages/resolute/en/man2/recv.2.html

- 阻塞/非阻塞

- Q: 阻塞下的read表现
    ** 有数据可读: 立即返回，读取 min(请求字节数, 可用字节数)
    ** 缓冲区空: 阻塞，直到:
        **** 数据到达 → 返回读取的字节数
        **** 对端关闭 → 返回 0
    ** 出错: 返回 -1，通过 errno 判断错误

==== redis
[source, c]
.https://github.com/redis/redis/blob/unstable/src/networking.c
----
    if (nread == -1) {
        if (errno == EAGAIN) {
            return;
        } else {
            serverLog(LL_VERBOSE, "Reading from client: %s",strerror(errno));
            freeClient(c);
            return;
        }
    } else if (nread == 0) {
        serverLog(LL_VERBOSE, "Client closed connection");
        freeClient(c);
        return;
    } else
----

==== nginx
[source, c]
.https://github.com/nginx/nginx/blob/master/src/os/unix/ngx_recv.c
----
ssize_t
ngx_unix_recv(ngx_connection_t *c, u_char *buf, size_t size)
{
    ssize_t       n;
    ngx_err_t     err;
    ngx_event_t  *rev;

    rev = c->read;

#if (NGX_HAVE_KQUEUE)

    if (ngx_event_flags & NGX_USE_KQUEUE_EVENT) {
        ngx_log_debug3(NGX_LOG_DEBUG_EVENT, c->log, 0,
                       "recv: eof:%d, avail:%d, err:%d",
                       rev->pending_eof, rev->available, rev->kq_errno);

        if (rev->available == 0) {
            if (rev->pending_eof) {
                rev->ready = 0;
                rev->eof = 1;

                if (rev->kq_errno) {
                    rev->error = 1;
                    ngx_set_socket_errno(rev->kq_errno);

                    return ngx_connection_error(c, rev->kq_errno,
                               "kevent() reported about an closed connection");
                }

                return 0;

            } else {
                rev->ready = 0;
                return NGX_AGAIN;
            }
        }
    }

#endif

#if (NGX_HAVE_EPOLLRDHUP)

    if ((ngx_event_flags & NGX_USE_EPOLL_EVENT)
        && ngx_use_epoll_rdhup)
    {
        ngx_log_debug2(NGX_LOG_DEBUG_EVENT, c->log, 0,
                       "recv: eof:%d, avail:%d",
                       rev->pending_eof, rev->available);

        if (rev->available == 0 && !rev->pending_eof) {
            rev->ready = 0;
            return NGX_AGAIN;
        }
    }

#endif

    do {
        n = recv(c->fd, buf, size, 0);

        ngx_log_debug3(NGX_LOG_DEBUG_EVENT, c->log, 0,
                       "recv: fd:%d %z of %uz", c->fd, n, size);

        if (n == 0) {
            rev->ready = 0;
            rev->eof = 1;

#if (NGX_HAVE_KQUEUE)

            /*
             * on FreeBSD recv() may return 0 on closed socket
             * even if kqueue reported about available data
             */

            if (ngx_event_flags & NGX_USE_KQUEUE_EVENT) {
                rev->available = 0;
            }

#endif

            return 0;
        }

        if (n > 0) {

#if (NGX_HAVE_KQUEUE)

            if (ngx_event_flags & NGX_USE_KQUEUE_EVENT) {
                rev->available -= n;

                /*
                 * rev->available may be negative here because some additional
                 * bytes may be received between kevent() and recv()
                 */

                if (rev->available <= 0) {
                    if (!rev->pending_eof) {
                        rev->ready = 0;
                    }

                    rev->available = 0;
                }

                return n;
            }

#endif

#if (NGX_HAVE_FIONREAD)

            if (rev->available >= 0) {
                rev->available -= n;

                /*
                 * negative rev->available means some additional bytes
                 * were received between kernel notification and recv(),
                 * and therefore ev->ready can be safely reset even for
                 * edge-triggered event methods
                 */

                if (rev->available < 0) {
                    rev->available = 0;
                    rev->ready = 0;
                }

                ngx_log_debug1(NGX_LOG_DEBUG_EVENT, c->log, 0,
                               "recv: avail:%d", rev->available);

            } else if ((size_t) n == size) {

                if (ngx_socket_nread(c->fd, &rev->available) == -1) {
                    n = ngx_connection_error(c, ngx_socket_errno,
                                             ngx_socket_nread_n " failed");
                    break;
                }

                ngx_log_debug1(NGX_LOG_DEBUG_EVENT, c->log, 0,
                               "recv: avail:%d", rev->available);
            }

#endif

#if (NGX_HAVE_EPOLLRDHUP)

            if ((ngx_event_flags & NGX_USE_EPOLL_EVENT)
                && ngx_use_epoll_rdhup)
            {
                if ((size_t) n < size) {
                    if (!rev->pending_eof) {
                        rev->ready = 0;
                    }

                    rev->available = 0;
                }

                return n;
            }

#endif

            if ((size_t) n < size
                && !(ngx_event_flags & NGX_USE_GREEDY_EVENT))
            {
                rev->ready = 0;
            }

            return n;
        }

        err = ngx_socket_errno;

        if (err == NGX_EAGAIN || err == NGX_EINTR) {
            ngx_log_debug0(NGX_LOG_DEBUG_EVENT, c->log, err,
                           "recv() not ready");
            n = NGX_AGAIN;

        } else {
            n = ngx_connection_error(c, err, "recv() failed");
            break;
        }

    } while (err == NGX_EINTR);

    rev->ready = 0;

    if (n == NGX_ERROR) {
        rev->error = 1;
    }

    return n;
}
----

=== close
==== close()
int close(int fd);
https://manpages.ubuntu.com/manpages/bionic//man2/close.2.html

Q: close之前本端发送的数据，能成功发送出去吗？
场景	本端 close() 前发送的数据能否成功送达？	说明
默认关闭	很可能可以	内核在后台保证。但可能因网络问题或对端问题在极少数情况下失败。
SO_LINGER (l_linger=0)	确定不能	发送缓冲区被立即清空，连接被重置。
需要绝对保证	需要应用层协议	使用 shutdown() 等待对端的应用层确认，是唯一可靠的方法。

Q: close会阻塞吗？
https://unix.stackexchange.com/questions/145526/can-close-block

Q: close之后本端能发送数据吗？对端能发送数据吗？
对于调用 close() 的本端程序：不能再发送数据，应将此 socket 视为完全失效。
对于对端程序：在收到 FIN（即 read() 返回0）后、自己调用 close() 前，TCP连接从它到本端的发送通道在理论上仍然是通的。但这是一个短暂的、依赖于应用层协议设计的状态，并非通用的数据发送时机。

Q: close之后本端能接收数据吗？对端能接收数据吗？
本端：
协议栈可能还在接收数据（如果对方还在发送），但这些数据不会被传递给应用程序。
应用程序无法读取这些数据（socket已关闭）。
对端：
在收到FIN之前：能正常接收。
在收到FIN之后：recv() 返回0，表示不能再接收应用数据，但TCP控制信息还能处理。
在B自己调用 close() 之前：B还能发送数据到A（虽然A的应用层收不到）。

==== 异常释放
发送一个复位(RST)报文段而不是FIN来释放一个连接，称为异常释放(ahortive release)。
异常终止一个连接对应用程序来说有两个优点:
(1) 丢弃任何待发数据并立即发送复位报文段;
(2) RST的接收方会区分另一端执行的是异常关闭还是正常关闭。
Socket API通过lingeronclose选项(即SO_LINGER)提供了这种异常关闭的能力: 停留时间设为0。

    SO_LINGER
        struct linger {
            int l_onoff;    /* linger active */
            int l_linger;   /* how many seconds to linger for */
        };
        l_onoff = 0
            默认模式
            close() 立即返回
            内核接管套接字，尝试发送缓冲区中的所有数据
            执行正常的 TCP 关闭序列（发送 FIN）
        l_onoff != 0, l_linger = 0
            close() 在以下情况下会阻塞：
                a) 发送缓冲区中还有数据
                b) 对端没有确认收到数据
            阻塞时间最多为 l_linger 秒
            如果在此期间所有数据都被确认，close() 提前返回
            如果超时，close() 返回 EWOULDBLOCK 错误，未发送数据被丢弃
        l_onoff !=0, l_linger > 0
            close() 立即返回
            发送缓冲区中的所有未发送数据被丢弃
            发送 RST 报文段（而非 FIN）给对端
            跳过正常的关闭序列，不进入 TIME_WAIT 状态
            对端会收到 ECONNRESET 错误

[source, c]
.https://github.com/torvalds/linux/blob/master/net/ipv4/tcp.c
----
    void tcp_close(struct sock *sk, long timeout)
        else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)
            /* Check zero linger _after_ checking for unread data. */
            sk->sk_prot->disconnect(sk, 0);
            NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
----

[source, c]
.https://github.com/torvalds/linux/blob/master/net/ipv4/af_inet.c
----
int inet_release(struct socket *sock)
    /* If linger is set, we don't return until the close
        * is complete.  Otherwise we return immediately. The
        * actually closing is done the same either way.
        *
        * If the close is due to the process exiting, we never
        * linger..
        */
    timeout = 0;
    if (sock_flag(sk, SOCK_LINGER) &&
        !(current->flags & PF_EXITING))
        timeout = sk->sk_lingertime;
    sk->sk_prot->close(sk, timeout);
    sock->sk = NULL;
----

==== vs. shutdown
特性	close()	    shutdown()
主要目的	释放套接字描述符	    关闭连接的一个方向
影响对象	进程级（文件描述符）	    连接级（TCP 连接）
缓冲区处理	发送缓冲区数据尽量发送	    立即停止指定方向
描述符计数	减少引用计数	    不影响引用计数
其他进程	其他进程可能仍能使用连接	    所有进程的连接方向都被关闭
FIN 发送	引用计数为 0 时发送	    立即发送（SHUT_WR/SHUT_RDWR）
RST 发送	可通过 SO_LINGER 控制	    从不发送 RST
SO_LINGER	有影响	    无影响

▪ 参考
https://stackoverflow.com/questions/4160347/close-vs-shutdown-socket
https://blog.codingnow.com/2021/02/skynet_tcp_halfclose.html

==== 实现
https://github.com/orientye/understanding-the-linux-kernel/blob/main/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Linux%E5%86%85%E6%A0%B8/%E7%BD%91%E7%BB%9C/%E7%9B%B8%E5%85%B3%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8.asc#close[linux-kernel-network-close()]

=== shutdown
==== shutdown()
int shutdown(int sockfd, int how);
https://manpages.ubuntu.com/manpages/bionic//man2/shutdown.2.html

    how-The constants SHUT_RD, SHUT_WR, SHUT_RDWR have the value 0, 1, 2

    SHUT_RD
        关闭读通道：套接字不再接收数据（内核丢弃已接收但未读取的数据，后续收数据直接丢弃），对方发数据会收到 ACK 但本地无感知。
    SHUT_WR
        关闭写通道：套接字不再发送数据（内核会将已缓冲的写数据发送完毕，然后发送 FIN 包），对方读端会收到 EOF（读返回 0）。
    SHUT_RDWR
        同时关闭读、写通道（等价于先 SHUT_RD 再 SHUT_WR）。

==== 场景
1. 当想要确保所有写好的数据已经发送成功时。如果在发送数据的过程中，网络意外断开或者出现异常，系统不一定会返回异常，这时可能以为对端已经接收到数据了。此时需要用shutdown()来确定数据是否发送成功，因为调用shutdown()时只有在缓存中的数据全部发送成功后才会返回。
2. 当程序使用了fork()或者使用多线程时，想防止其他线程或进程访问到该资源，又或者想立刻关闭这个socket，那么可以用shutdown()来实现。

Given that we can close a socket, why is shutdown needed?
There are several reasons.
First, close will deallocate the network endpoint only when the last active reference is closed. If we duplicate the socket (with dup, for example), the socket won’t be deallocated until we close the last file descriptor referring to it. The shutdown function allows us to deactivate a socket independently of the number of active file descriptors referencing it.
Second, it is sometimes convenient to shut a socket down in one direction only. For example, we can shut a socket down for writing if we want the process we are communicating with to be able to tell when we are done transmitting data, while still allowing us to use the socket to receive data sent to us by the process.
参考: 《Advanced.Programming.in.the.UNIX.Environment.3rd.Edition》16.2

==== 示例
redis:

    ./src/socket.c:    shutdown(conn->fd, SHUT_RDWR);

nginx:

    ./src/os/unix/ngx_socket.h:  #define NGX_RDWR_SHUTDOWN  SHUT_RDWR
    ./src/os/win32/ngx_socket.h: #define NGX_RDWR_SHUTDOWN  SD_BOTH

=== select/poll
- select
- poll
- 场景: 跨平台，对效率要求不高的场景

pselect vs select:

    超时精度
        select使用的是struct timeval结构体，精确到微秒
        pselect使用的是struct timespec结构体，精确到纳秒
    超时参数
        select可能会更新其超时参数timeout
        pselect6系统调用可能会更新其超时参数，glibc的封装pselect不会更新其超时参数
    信号掩码
        pselect可以设置信号掩码，若其为NULL，则行为与select相同

ppoll vs poll:

    超时精度
        poll使用的超时精度为毫秒
        ppoll使用的是struct timespec结构体，精确到纳秒
    信号掩码
        ppoll可以设置信号掩码，若其为NULL，则行为与poll相同

=== epoll/kqueue/evport
- epoll_create/epollcreate1
https://man7.org/linux/man-pages/man2/epoll_create.2.html

- epoll_ctrl
https://man7.org/linux/man-pages/man2/epoll_ctl.2.html

- epoll_wait
https://man7.org/linux/man-pages/man2/epoll_wait.2.html

- LT/ET

- example

[source, c]
.https://github.com/redis/redis/blob/unstable/src/ae_epoll.c
----
    if (e->events & EPOLLIN) mask |= AE_READABLE;
    if (e->events & EPOLLOUT) mask |= AE_WRITABLE;
    if (e->events & EPOLLERR) mask |= AE_WRITABLE|AE_READABLE;
    if (e->events & EPOLLHUP) mask |= AE_WRITABLE|AE_READABLE;
----

Q: why EPOLLOUT

- epoll惊群
EPOLLEXCLUSIVE(since Linux 4.5): https://man7.org/linux/man-pages/man2/epoll_ctl.2.html
参考: https://zhuanlan.zhihu.com/p/359774959

=== iocp
Input/Output Completion Port
https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports
https://zhuanlan.zhihu.com/p/693266969
https://www.cnblogs.com/tanguoying/p/8439701.Html

vs. io_uring
https://www.zhihu.com/question/519485278

示例:
https://github.com/microsoft/Windows-classic-samples/tree/main/Samples/Win7Samples/netds/winsock/iocp

=== io_uring

=== Q&A
Q: linux中每个TCP连接最少占用多少内存？
A: 3k多一点 https://zhuanlan.zhihu.com/p/25241630
理论绝对最小值：对于一个空闲的、几乎没有数据的连接，内核优化后可能在 ~3KB 左右。
典型/实际值：对于一个有正常数据收发的连接，通常在 10KB ~ 30KB 之间，甚至更高。这包括了读写缓冲区占用的内存。
一个TCP连接在内核中占用的内存主要由三部分组成:

    1. 连接元数据（基础开销）
        这是内核用于维护连接状态（如IP、端口、状态机、定时器等）的数据结构，主要是 struct sock 和 struct tcp_sock。
        在64位系统上，这部分的基础开销大约在 1.5KB ~ 2KB 之间。
    2. 读缓冲区（Receiver Buffer）
        用于缓存从网络对端接收过来，但应用程序还未读取的数据。其大小由 net.ipv4.tcp_rmem 内核参数控制。
        $ sysctl net.ipv4.tcp_rmem
        net.ipv4.tcp_rmem = 4096    87380   6291456   #对应 最小值、默认值、最大值（单位：字节）。
        即使连接空闲，内核也会为它分配一个最小值（例如上面的4KB）。
        当数据涌入时，缓冲区会根据TCP的流控和窗口大小，动态地在最小值和最大值之间调整。
    3. 写缓冲区（Sender Buffer）
        用于缓存应用程序已经写入，但尚未被对端确认接收的数据。其大小由 net.ipv4.tcp_wmem 内核参数控制。
        $ sysctl net.ipv4.tcp_wmem
        net.ipv4.tcp_wmem = 4096    16384   4194304   #同样包含最小、默认、最大值。
        同样，即使没有数据要发送，也会分配一个最小值。
    
    估算:
        如果每个连接占用20KB:
            10,000连接 ≈ 200 MB
            100,000连接(十万连接) ≈ 2 GB
            1,000,000连接(百万连接) ≈ 20 GB

Q: 一台服务器最多可以支撑多少条TCP连接？假设所有的TCP连接都是空连接，那么一台服务器上最多可以支撑多少条TCP连接？一台机器有可能撑起100W的并发长连接吗？
主要受限于内存，按 10KB/连接计算，百万连接需要: 1,000,000 * 10KB ≈ 10 GB
另外要设置文件描述符限制等

Q: 一台客户端最多可以支撑多少条TCP连接？
客户端的理论连接总数 = 客户端IP数 × 客户端端口数 × 服务端IP数 × 服务端端口数
如果是固定的服务器IP和服务器端口，客户端IP数通常是 1个（除非主机有多个IP），因此理论上最多 65,535 个

Q: 在端口不足的情况下，connect系统调用的CPU消耗会大幅度增加？原因是？
1. 系统调用上下文的频繁切换
2. 内核中的查找开销
3. 应用程序层面的“重试风暴”