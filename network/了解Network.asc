了解Network

Ver 0.0.2

2020-08-14,  orient

blog:            http://orientye.com
微信公众号:   深入理解计算机系统

了解Network

●  概念

●  链路层

●  IP  

●  UDP

●  TCP

●  应用层

●  API

●  库/框架

●  性能优化

●  安全

概念 - IP地址

●  loopback addr:   
     127.0.0.1     (Q: ping 127.0.0.1 与 ping 本机IP 区别?)
        ipv6  0000:0000:0000:0000:0000:0000:0000:0001(::1)
 
●  私有网段,有A,B,C三个地址段：
        10.0.0.0/8: 10.0.0.0-10.255.255.255
        172.16.0.0/12: 172.16.0.0-172.31.255.255
        192.168.0.0/16: 192.168.0.0-192.168.255.255

●  子网寻址/子网掩码
        IPv6地址/n（0 <= n<= 128）表示前n位组成子网前缀

●  任何地址：0.0.0.0

概念 - 连接

●  (Src IP,  Src Port,  Dst IP,  Dst Port)

●  TCP和UDP采用16 bit的端口号来识别应用程序
       Q: Ping调用的ICMP协议，没有端口号，那么同一内网的两台机器同时ping同一个外网ip, 那么如何区分的

●  NAT ：Network address translation
    https://en.wikipedia.org/wiki/Network_address_translation

●  IPv6

概念 - 分层

Q: 浏览器里输入:   www.taobao.com  然后返回了结果, 这个过程中发生了什么?

二层帧(Frame)   三层包(Packet)   四层段(Segment)

----------------------------------------------------------------------------------------------
MTU:  Maximum Transfer Unit 链路层的帧中数据部分最大字节数，以太网一般1500字节
----------------------------------------------------------------------------------------------
TTL:   Time To Live
----------------------------------------------------------------------------------------------
MSL:  Maximum Segment Lifetime
MSS:  Maximum Segment Size 一般MSS=1500-20-20字节
RTT:   Round-Trip Time
RTO:  Retransmission Timeout 
----------------------------------------------------------------------------------------------

概念 - 分层

链路层

● 主要三个用途: 
        为IP模块发送和接受IP数据报; 
        为ARP模块发送ARP请求和接受ARP应答；
        为RARP发送RARP请求和接受RARP应答

● 链路层协议
        链路层有多种，取决于网络所使用的硬件

● MAC地址
        也称硬件地址通常6个字节

● ARP和RARP
        解决MAC地址与IP地址的映射

● MTU（最大传输单元）
        netstat –in/ifconfig| grep -i MTU命令可以查看
        IP层数据如果超过，就要进行分片(fragmentation)

● 路径MTU
       两台通信主机路径中最小的MTU

● 实际互联网MTU在576字节？（RFC1122）

IP

●  header长度： IP 协议头部的长度，单位32bit 需要这个值是因为任选字段的长度是可
变的， 这个字段占4bit（最多能表示15个32bit的的字，即4*15=60个字节的首部长度），
因此IP 头部最多有60字节长度。正常的长度是20字节； 如果有额外的 IP 的 options 选
项，还得加上 option 的长度。

●  TTL(time-to-live)
生存时间字段设置了数据报可以经过的最多路由器数。它指定了数据报的生存时间。TTL的初始值由源主机设置(通常为32或64)，一旦经过一个
处理它的路由器，它的值就减去1。当该字段的值为0时，数据报就被丢弃，并发送ICMP报文通知源主机。

●  网络字节序(big endian)

●  IP路由选择

UDP

如何使用UDP

如何实现RUDP
根据场景可以分为： 尽力可靠(允许缺失)， 无序可靠， 有序可靠
如何实现可靠？ 重传！

基于UDP的P2P穿透

应用： QUIC   WebRTC

UDP

Q：  UDP数据包理想长度？
The maximum safe UDP payload is 508 bytes. This is a packet size of 576, minus the maximum 60-byte IP header and the 8-byte UDP header. 
Any UDP payload this size or smaller is guaranteed to be deliverable over IP (though not guaranteed to be delivered). Anything larger is 
allowed to be outright dropped by any router for any reason. Except on an IPv6-only route, where the maximum payload is 1,212 bytes. As 
others have mentioned, additional protocol headers could be added in some circumstances. A more conservative value of around 300-400 
bytes may be preferred instead.

Q：  UDP数据包最大长度？

根据 UDP 协议，从 UDP 数据包的包头可以看出，UDP 的最大包长度是2^16-1的个字节。由于UDP包头占8个字节，而在IP层进行封装后的IP包头占去20
字节，所以这个是UDP数据包的最大理论长度是2^16 - 1 - 8 - 20 = 65507字节

Q：  UDP会发生粘包吗？
UDP不存在粘包问题，是由于UDP发送的时候，没有经过Negal算法优化，不会将多个小包合并一次发送出去。另外，在UDP协议的接收端，采用了链式结
构来记录每一个到达的UDP包，这样接收端应用程序一次recv只能从socket接收缓冲区中读出一个数据包。也就是说，发送端send了几次，接收端必须recv
几次（无论recv时指定了多大的缓冲区）。

Q:    UDP存在发送缓冲区吗？存在接受缓冲区吗？sendto 和recvfrom如何实现的？

Q：  UDP阻塞模式下，sendto和recvfrom的表现？

Q：  UDP非阻塞模式下，sendto和recvfrom的表现？

Q：  UDP接收buffer如果小了呢？

Q：  UDP丢包如何解决？
查看网卡ifconfig eth0/ethtool -S eth0/netstat -s -u  查看有无drop, error, tx_, rx_等信息
报文错误、防火墙、UDP buffer size 不足、系统负载过高

UDP - P2P

 一, 使用STUN协议(RFC 3489)
    STUN协议即常说的UDP打洞，基本思想通过公网上安装的一个STUN server(一般有2个IP地址)充当联系人，client
首先与公网上的STUN server联系，判断自己处于内网还是公网，若处于内网，则继续与STUN server联系，判断
NAT是哪种类型，若是Cone nat则支持STUN，若是Symmetic nat则不支持以STUN形式的P2P.
    STUN最成功的案例要属skype,但STUN方案也有不足之处,主要有2点：
    1)需要额外的STUN服务器.
    2)symmetic nat无法穿透.

  二, 使用UPNP端口映射技术
    UPNP直接把一个内网端口映射到一个网关设备的外网IP的一个端口上,任何发送到外网IP的这个端口的数据,都会被
自动转发到内网映射的端口上,只要做了端口映射,就无须关心NAT地址转换所带来的麻烦,对于应用程序从端口映射过的
内网地址发出的数据,NAT会把从这个内网地址发出的数据IP头改为相应的公网IP和端口(简称IPPORT1),对方响应的数
据被发送到IPPORT1,NAT设备又会把数据传给绑定IPPORT1的内网地址. 相比于STUN,UPNP有着以下几点优势：
    1)无需额外的服务器.
    2)开发相比STUN简单.
    使用UPNP需要网关支持UPNP(现在的网关设备还没见过不支持UPNP的).
    windows系列的操作系统已经内置了UPNP的支持,并且为了方便应用程序的调用,提供了一组UPNP的com组件形式.
    emule开源项目是一个P2P共享的文件传输软件，其UPNP模块有两种形式，一是使用操作系统提供的UPNP接口，
二是使用miniupnpc(一个upnp开源项目)提供的接口，用于没有提供UPNP功能的操作系统,miniupnpc项目主页上有
支持的NAT设备清单，emule优先选择操作系统提供的UPNP接口，若失败，则尝试miniupnpc提供的接口.

UDP - QUIC

https://www.chromium.org/quic

https://en.wikipedia.org/wiki/QUIC

TCP

●  格式

●  状态

●  流控

●  拥塞

●  超时与重传

●  keep alive

●  API

TCP - 格式

TCP - 格式

TCP Flags:

SYN - The SYN, or Synchronisation flag, is used as a first step in establishing a 3-way handshake between two 
hosts. Only the first packet from both the sender and receiver should have this flag set. The following diagram 
illustrates a 3-way handshake process.

ACK - The ACK flag, which stands for “Acknowledgment”, is used to acknowledge the successful receipt of a 
packet. As we can see from the diagram above, the receiver sends an ACK as well as a SYN in the second step of 
the 3-way handshake process to tell the sender that it received its initial packet.

FIN - The FIN flag, which stands for “Finished”, means there is no more data from the sender. Therefore, it is used 
in the last packet sent from the sender.

URG - The URG flag is used to notify the receiver to process the urgent packets before processing all other 
packets. The receiver will be notified when all known urgent data has been received. See RFC 6093 for more 
details.

PSH - The PSH flag, which stands for “Push”, is somewhat similar to the URG flag and tells the receiver to 
process these packets as they are received instead of buffering them.

RST - The RST flag, which stands for “Reset”, gets sent from the receiver to the sender when a packet is sent to a 
particular host that was not expecting it.  

ECE - This flag is responsible for indicating if the TCP peer is ECN capable. See RFC 3168 for more details.

CWR - The CWR flag, which stands for Congestion Window Reduced, is used by the sending host to indicate it 
received a packet with the ECE flag set. See RFC 3168 for more details.

NS (experimental) - The NS flag, which stands for Nonce Sum, is still an experimental flag used to help protect 
against accidental malicious concealment of packets from the sender. See RFC 3540 for more details.

TCP - 格式

● Inital Sequence Number(SYN，全称Synchronize Sequence 
Numbers )

RFC793中说，ISN会和一个假的时钟绑在一起，这个时钟会在每4微秒对ISN做加一操作，直
到超过2^32，又从0开始。这样，一个ISN的周期大约是4.55个小时。因为，假设TCP 
Segment在网络上的存活时间不会超过Maximum Segment Lifetime（MSL），所以，只
要MSL的值小于4.55小时，那么，我们就不会重用到ISN。

一般来说，各个OS实现不一样， It‘s a random number between 0 and 4,294,967,295。

Wireshark为了显示更友好，使用了Relative SeqNum——相对序号，只要在右键菜单中的
protocol preference 中取消掉就可以看到“Absolute SeqNum”了.

Sequence Number的增加可能也不一样。

TCP - 状态

TCP

TCP - 状态

TCP - 状态

TCP-状态-sync

●  建立连接时SYN超时
如果server端接到了clien发的SYN后回了SYN-ACK后，client掉线了，server端没有收到client回来的ACK，那么，这个连接处于一个中间状
态，即没成功，也没失败。于是，server端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，5次的重试
时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 
2^6 -1 = 63s，TCP才会把断开这个连接。

● SYN Flood攻击
一些恶意的人就为此制造了SYN Flood攻击——给服务器发了一个SYN后，就下线了，于是服务器需要默认等63s才会断开连接，这样，攻击
者就可以把服务器的syn连接的队列耗尽，让正常的连接请求不能处理。于是，Linux下给了一个叫tcp_syncookies的参数来应对这个事——当
SYN队列满了后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击
者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）。请注意，
请先千万别用tcp_syncookies来处理正常的大负载的连接的情况。因为，synccookies是妥协版的TCP协议，并不严谨。对于正常的请求，你
应该调整三个TCP参数可供你选择，第一个是：tcp_synack_retries用来减少重试次数；第二个是：tcp_max_syn_backlog，可以增大SYN连

接数；第三个是：tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了。 

参考： https://coolshell.cn/articles/11564.html

TCP-状态-time_wait

●  Why TIME_WAIT state

    To implement TCP's full-duplex connection termination reliably
    To allow old duplicate segments to expire in the network

The first reason can be explained by looking at Figure 2.5 and assuming that the final ACK is lost. The server will 
resend its final FIN, so the client must maintain state information, allowing it to resend the final ACK. If it did not 
maintain this information, it would respond with an RST (a different type of TCP segment), which would be 
interpreted by the server as an error. If TCP is performing all the work necessary to terminate both directions of 
data flow cleanly for a connection (its full-duplex close), then it must correctly handle the loss of any of these four 
segments. This example also shows why the end that performs the active close is the end that remains in the 
TIME_WAIT state: because that end is the one that might have to retransmit the final ACK.

To understand the second reason for the TIME_WAIT state, assume we have a TCP connection between 
12.106.32.254 port 1500 and 206.168.112.219 port 21. This connection is closed and then sometime later, we 
establish another connection between the same IP addresses and ports: 12.106.32.254 port 1500 and 
206.168.112.219 port 21. This latter connection is called an incarnation of the previous connection since the IP 
addresses and ports are the same. TCP must prevent old duplicates from a connection from reappearing at some 
later time and being misinterpreted as belonging to a new incarnation of the same connection. To do this, TCP will 
not initiate a new incarnation of a connection that is currently in the TIME_WAIT state. Since the duration of the 
TIME_WAIT state is twice the MSL, this allows MSL seconds for a packet in one direction to be lost, and another 
MSL seconds for the reply to be lost. By enforcing this rule, we are guaranteed that when we successfully establish 
a TCP connection, all old duplicates from previous incarnations of the connection have expired in the network.

●  Why 2MSL

参考： 《Unix Networking Programming Volume 1, 3rd》2.7

TCP-状态-time_wait

如果存在大量的time_wait连接，会影响新的连接的创建：
该socket需2MSL即2*(30s-2min)才会完全关闭释放

●  问题：

●  解决：

首选方法： 尽量让客户端主动关闭

方法二： There's another way to terminate a TCP connection and that's 
by aborting the connection and sending an RST rather than a FIN. This 
is usually achieved by setting the SO_LINGER socket option to 0. 

https://stackoverflow.com/questions/3757289/when-is-tcp-option-so-linger-0-required

不太好的方式:
tcp_to_recycle    for reduce MSL?
tcp_to_reuse       or SO_REUSEADDR(Q: vs. VS. SO_REUSEPORT)
tcp_max_tw_buckets(控制并发的TIME_WAIT的数量，默认值是180000)
net.ipv4.tcp_fin_timeout

TCP-状态-close_wait

●  问题
          如果存在大量的close_wait连接

●  解决方式
             通常是代码问题,即被动关闭方未关闭socket造成

TCP - RST

● 什么情况下发生

1、目标端口未打开 / 目的主机或者网络路径中防火墙拦截：目标会向对方发送RST

2、socket接收缓冲取Recv-Q中的数据未完全被应用程序读取时关闭该socket： 会向对方发送RST

3、向对端已关闭的socket发送数据： 对端会向发送方发送RST
 
4、使用SO_LINGER规定close()行为是发送RST，而不是发送FIN
 
5、向对端已经消逝的连接中发送数据
消逝连接指的是，当前这个连接状态操作系统已经不再维护，其数据结构内核已经注销。
比如对端FIN_WAIT2超时后，其实该连接已经不存在；
比如半打开(Half Open)连接的对端，由于某种原因已经不存在；
比如服务器重启，端口号不变，此时客户端没有检测到服务器重启仍向服务器发送数据，则收到服务器发来的
RST；
比如客户端断网，重新连接网络，但是没有连接服务器，此时服务器没有检测到客户端断网重连仍向客户端发
送数据，则收到客户端发来的RST；

......

另: https://stackoverflow.com/questions/251243/what-causes-a-tcp-ip-reset-rst-flag-to-be-sent

TCP - RST

● 处理 :

在TCP协议中，rst段标识复位，用来异常的关闭连接。

1. 发送RST包关闭连接时，不必等缓冲区的包都发出去，直接就丢弃缓冲区中的包，发送RST。
2. 收到RST包后，也不必发送ACK包来确认。接收端收到RST知道发送端是异常关闭。

RFC793(page36, section3.4)

Reset Processing
In all states except SYN-SENT, all reset (RST) segments are validated by checking their SEQ-fields.  A 
reset is valid if its sequence number is in the window.  In the SYN-SENT state (a RST received in 
response to an initial SYN), the RST is acceptable if the ACK field acknowledges the SYN.

The receiver of a RST first validates it, then changes state.  If the receiver was in the LISTEN state, it 
ignores it.  If the receiver was in SYN-RECEIVED state and had previously been in the LISTEN state,
then the receiver returns to the LISTEN state, otherwise the receiver aborts the connection and goes 
to the CLOSED state.  If the receiver was in any other state, it aborts the connection and advises the 
user and goes to the CLOSED state.

● RST攻击与防御

TCP - 异常行为

● 对端程序异常结束

崩溃/断电/重启

一段时间内重连          本端有/无操作分别会发生什么
一段时间内未重连       本端有/无操作分别会发生什么
始终没有重连              本端有/无操作分别会发生什么

● 对端程序超时（程序未结束）

拔掉网线/中间路径超时

一段时间内                  本端有/无操作分别会发生什么
超过一段时间               本端有/无操作分别会发生什么

TCP - 超时与重传

●  ARQ(Automatic Repeat Request)

●  基于计时器的重传

●  快速重传
             

●  带选择确认的重传(SACK)

●  伪超时重传
             DSACK-重复SACK
             Eifel检测算法
             Forward-RTO Recovery
             Eifel响应算法

TCP-流控与拥塞

流量控制（点对点通信控制）

让发送方发送速率不要太快，使接收方来得及接收。

基于滑动窗口的流量控制机制：接收方根据接收缓存的大小动态控制发送方的发送窗口大小（调整TCP首部“窗口”字段值），限制发送方网络注入报文的速率，同
时根据网络拥塞程度估计窗口值。

拥塞控制（全局，涉及全网络主机、路由器等）

防止过多的数据注入网络，可以使网络中的路由器或链路不致于过载。
算法：慢开始、拥塞避免、快重传、快恢复

发送方维持一个拥塞窗口（cwnd）的状态变量，该大小取决于网络拥塞程度，并动态变化
变化原则：网络无拥塞，窗口增大；网络拥塞则减小

新建立的连接不能够一开始就大量发送数据包，而是根据网络情况逐步增加每次发送的数量。
为防止cwnd增长过大引起网络阻塞，设置慢开始门限ssthresh状态变量选择慢开始算法与拥塞避免算法

拥塞窗口缓慢增长，每经过一个往返时间RTT就把发送发的拥塞窗口+1，而不是加倍。
（以上两个阶段若出现拥塞，门限设置为出现拥塞时发送窗口的一半，窗口值设为1，执行慢开始算法）

慢开始与拥塞避免

慢开始（cwnd指数型增长）

拥塞避免（cwnd线性增长）

快重传与快恢复

快重传
接收方在收到一个失序的报文段后立即发出重复确认，发送方只要收到三个重复确认就应当立即重传尚未收到的报文段，不必等待设置的重传计时器时间到期。

快恢复
发送方收到三个重复确认时，ssthresh门限减半，然后执行拥塞避免算法。

TCP - 流控

●  Flow Control解决的问题

●  延时确认(Delayed Acknowledgments)

The Nagle algorithm says that when a TCP connection has outstanding data that has not yet been acknowledged, small 
segments (those smaller than the SMSS) cannot be sent until all outstanding data is acknowledged. Instead, small 
amounts of data are collected by TCP and sent in a single segment when an acknowledg- ment arrives. 

●  Nagle算法

禁用Nagle算法: TCP_NODELAY

vs TCP_CORK aggressively accumulates data. If TCP_CORK is enabled in a socket, it will not send data until the buffer fills 
to a fixed limit. Similar to Nagle's algorithm, it also accumulates data from user but until the buffer fills to a fixed limit 
not until receiving ACK. This will be useful while sending multiple blocks of data. But you have to be more careful while 
using TCP_CORK.
Until 2.6 kernel, both of these options are mutually exclusive. But in later kernel, both of them can exist together. In such 
case, TCP_CORK will be given more preference.

TCP_CORK (or TCP_NOPUSH in FreeBSD)
If set, don't send out partial frames. All queued partial frames are sent when the option is cleared again. This is useful for 
prepending headers before calling sendfile(2), or for throughput optimization. As currently implemented, there is a ** 
200-millisecond ceiling** on the time for which output is corked by TCP_CORK. If this ceiling is reached, then queued 
data is automatically transmitted. This option can be combined with TCP_NODELAY only since Linux 2.5.71. This option 
should not be used in code intended to be portable.

tcp_nodelay on;  （nginx， go等默认）
tcp_nopush off;  （nginx默认）

TCP - 流控

●  窗口大小
TCP头部的窗口大小表明接收端可以缓存空间的大小，该字段为16位，也就是64K, RFC 1323扩展之后，就可
以使用32位的值来表示窗口的大小了

●  滑动窗口

●  零窗口

●  糊涂窗口综合症(Silly Window Syndrome)

●  Large Buffers and Auto-Tuning:
       net.core.rmem_max = 131071
       net.core.wmem_max = 131071
       net.core.rmem_default = 110592
       net.core.wmem_default = 110592
       In addition, the auto-tuning parameters are given by the following variables:
       net.ipv4.tcp_rmem = 4096 87380 174760
       net.ipv4.tcp_wmem = 4096 16384 131072

TCP - 拥塞

●  Congestion
when a router is forced to discard data because it cannot handle the arriving traffic rate, is called 
congestion. 

●  拥塞检测
      丢包可能由拥塞(主要是有线网络)引起，也可能由传输和接受错误(主要是无线网络)引起

●  减缓TCP发送
      The sender’s actual (usable) window W is then written as the minimum of the receiver’s advertised 
window awnd and the congestion window:    W = min(cwnd, awnd) 

● 
1988年TCP Tahoe 提出了1）慢启动，2）拥塞避免，3）拥塞发生时的快速重传 1990年
TCP Reno 在Tahoe的基础上增加了4）快速恢复

● 对标准算法的改进
New Reno     SACK        FACK  限制传输   CWV（Congestion Window Validation）拥塞窗口校验

● 其它
HSTCP     BIC和CUBIC(linux2.6.18起默认)  Vegas  FAST CTCP(compound tcp 复合tcp)

●  慢启动(slow start)
1）连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据(准确来说是smss  发送方最大段大小，大部分情况下，smss为接收方mss和路径mtu两者中较小值)
2）每当收到一个ACK，cwnd++; 呈线性上升
3）每当过了一个RTT，cwnd = cwnd*2; 呈指数让升
4）上限ssthresh（slow start threshold），当cwnd >= ssthresh时，就会进入拥塞避免算法
所以，如果网速很快的话，ACK也会返回得快，RTT也会短，那么，这个慢启动就一点也不慢。

TCP – 拥塞

Google的论文《An Argument for Increasing TCP’s Initial Congestion Window》Linux 3.0后采用了这篇论文的建议——把cwnd 初始化成了 10个MSS。 而Linux 3.0以前，比如2.6，Linux采用
了RFC3390，cwnd是跟MSS的值来变的，如果MSS< 1095，则cwnd = 4；如果MSS>2190则cwnd=2；其它情况下，则是3。

●  拥塞避免(Congestion Avoidance)
一般来说ssthresh的值是65535，单位是字节，当cwnd达到这个值时后，算法如下：
1）收到一个ACK时，cwnd = cwnd + 1/cwnd
2）当每过一个RTT时，cwnd = cwnd + 1
这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。很明显，是一个线性上升的算法。
注：ssthresh是变化的        ssthresh = max(flight size/2, 2*SMSS)
In Microsoft’s most recent (“Next Generation”) TCP/IP stack, this equation is reportedly changed to the somewhat more conservative relationship: ssthresh = max(min(cwnd, 
awnd)/2, 2*SMSS) 

●  拥塞发生时的快速重传(Congestion Avoidance)
当丢包的时候，会有两种情况：
1）等到RTO超时，重传数据包。TCP认为这种情况太糟糕，反应也很强烈。
sshthresh =  cwnd /2    cwnd 重置为 1   进入慢启动过程
2）Fast Retransmit算法，也就是在收到3个duplicate ACK时就开启重传，而不用等到RTO超时。
TCP Tahoe的实现和RTO超时一样。
TCP Reno的实现是：    cwnd = cwnd /2        sshthresh = cwnd     进入快速恢复算法——Fast Recovery

●快速恢复算法(Fast Recovery)
TCP Reno
这个算法定义在RFC5681。快速重传和快速恢复算法一般同时使用。快速恢复算法是认为，你还有3个Duplicated Acks说明网络也不那么糟糕，所以没有必要像RTO超时那么强烈。 注意，正如前面所
说，进入Fast Recovery之前，cwnd 和 sshthresh已被更新：
cwnd = cwnd /2
sshthresh = cwnd
然后，真正的Fast Recovery算法如下：
cwnd = sshthresh  + 3 * MSS （3的意思是确认有3个数据包被收到了）
重传Duplicated ACKs指定的数据包
如果再收到 duplicated Acks，那么cwnd = cwnd +1
如果收到了新的Ack，那么，cwnd = sshthresh ，然后就进入了拥塞避免的算法了。
如果你仔细思考一下上面的这个算法，你就会知道，上面这个算法也有问题，那就是——它依赖于3个重复的Acks。注意，3个重复的Acks并不代表只丢了一个数据包，很有可能是丢了好多包。但这个
算法只会重传一个，而剩下的那些包只能等到RTO超时，于是，进入了恶梦模式——超时一个窗口就减半一下，多个超时会超成TCP的传输速度呈级数下降，而且也不会触发Fast Recovery算法了。

通常来说，正如我们前面所说的，SACK或D-SACK的方法可以让Fast Recovery或Sender在做决定时更聪明一些，但是并不是所有的TCP的实现都支持SACK（SACK需要两端都支持），所以，需要一
个没有SACK的解决方案。而通过SACK进行拥塞控制的算法是FACK（后面会讲）

TCP New Reno
于是，1995年，TCP New Reno（参见 RFC 6582 ）算法提出来，主要就是在没有SACK的支持下改进Fast Recovery算法的——
当sender这边收到了3个Duplicated Acks，进入Fast Retransimit模式，开发重传重复Acks指示的那个包。如果只有这一个包丢了，那么，重传这个包后回来的Ack会把整个已经被sender传输出去的
数据ack回来。如果没有的话，说明有多个包丢了。我们叫这个ACK为Partial ACK。
一旦Sender这边发现了Partial ACK出现，那么，sender就可以推理出来有多个包被丢了，于是乎继续重传sliding window里未被ack的第一个包。直到再也收不到了Partial Ack，才真正结束Fast 
Recovery这个过程
我们可以看到，这个“Fast Recovery的变更”是一个非常激进的玩法，他同时延长了Fast Retransmit和Fast Recovery的过程

TCP – 拥塞

●  常见的拥塞算法可以分为三类：
● 基于路径时延（如 Vegas、Westwood）

● 基于丢包（如 Cubic、NewReno）

将路径时延上升作为发生拥塞的信号，在单一的网络环境下（所有连接都使用基于路径时延的拥塞算法）是可行的，但是在复杂的网络环境下，带
宽容易被其他算法抢占，带宽利用率最低。

将丢包作为发生拥塞的信号，其背后的逻辑是路由器、交换机的缓存都是有限的，拥塞会导致缓存用尽，进而队列中的一些报文会被丢弃。

拥塞会导致丢包，但是丢包却不一定拥塞导致的。事实上，丢包可以分为两类，一类是拥塞丢包，另一类是噪声丢包，特别是在无线网络环境中，
数据以无线电的方式进行传递，无线路由器信号干扰、蜂窝信号不稳定等都会导致信号失真，最终数据链路层 CRC 校验失败将报文丢弃。

基于丢包的拥塞算法容易被噪声丢包干扰，在高丢包率高延迟的环境中带宽利用率较低。

● 基于带宽时延探测（如 BBR）

既然无法区分拥塞丢包和噪声丢包，那么就不以丢包作为拥塞信号，而是通过探测最大带宽和最小路径时延来确定路径的容量。抗丢包能力强，带
宽利用率高。

三种类型的拥塞算法没有谁好谁坏，都是顺应当时的网络环境的产物，随着路由器、交换机缓存越来越大，无线网络的比例越来越高，基于路径时
延和基于丢包的的拥塞算法就显得不合时宜了。对于流媒体、文件上传等对带宽需求比较大的场景，BBR 成为更优的选择。

参考：https://www.infoq.cn/article/SY0KFJ2pyJomB6sAkqls

●  Google BBR(Bottleneck Bandwidth and Round-trip propagation time)

Linux kernel 4.9

经典的拥塞控制算法比如reno/newReno/Cubic无一例外都是将丢包作为拥塞的信号，然后降低发送速率。而在该算法中，不考虑丢包，而是基于
这样一个定义：当网络上的包数大于BDP(带宽时延乘积)时，就认为出现了拥塞。所以重点就在于如何准确地测量出瓶颈链路的带宽和整个链路的
传播时延。

TCP BBR 解决带宽和延迟无法同时测准的方法是：交替测量带宽和延迟；用一段时间内的带宽极大值(max bandwidth)和延迟极小值(min RTT)作
为估计值。

TCP-keepalive

● 为什么，优缺点

●    设置
setsockopt(sock, SOL_SOCKET, SO_KEEPALIVE, (char*)&value, sizeof(long));
setsockopt(sock, SOL_TCP, TCP_KEEPCNT, (char*)&value, sizeof(long));
setsockopt(sock, SOL_TCP, TCP_KEEPIDLE, (char*)&value, sizeof(long));
setsockopt(sock, SOL_TCP, TCP_KEEPINTVL, (char*)&value, sizeof(long));

应用层

常见协议:

● HTTP/S  参考《了解HTTP》

● MQTT,  AMPQ, DDS等

● Protobuf

API

●  socket
●  setsockopt/getsockopt
●  bind
●  listen
●  accept
●  getaddrinfo
●  connect
●  write/writev/send/sendto/sendmsg
●  read/readv/recv/recvfrom/recvmsg
●  close
●  shutdown

●  select
●  poll
●  epoll/kqueue/evport

API - socket    setsockopt/getsockopt

● man socket

● https://linux.die.net/man/7/tcp

● https://linux.die.net/man/7/udp

API - bind

●  int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

●  bind之前一般都会setsockopt(fd, SOL_SOCKET, 
SO_REUSEADDR，…)

SO_REUSEADDR vs. SO_REUSEPORT
https://stackoverflow.com/questions/14388706/how-do-so-reuseaddr-and-so-reuseport-differ

API - listen

●  int listen(int sockfd, int backlog);

●  server建立连接会维护两个队列：

一个存放 SYN 的队列（半连接队列）- 对应SYN_REVD状态（pending socket queue）
一个存放完成连接的队列（全连接队列）- 对应ESTABELLISHED状态(established socket 

queue)

全连接队列长度=min(backlog, 内核参数 net.core.somaxconn(默认为128));

半连接队列长度=min(backlog, 内核参数net.core.somaxconn，内核参数
tcp_max_syn_backlog), 当使用SYNCookie时(即内核参数 net.ipv4.tcp_syncookies=1)，
tcp_max_syn_backlog无效

●  example:

Redis:  #define CONFIG_DEFAULT_TCP_BACKLOG       511    /* TCP listen backlog. */
LibUV: 大部分是128
Nginx: #define NGX_LISTEN_BACKLOG  -1

API - accept

●  int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
      int accept4(int sockfd, struct sockaddr *addr, socklen_t *addrlen, int flags);

●  非阻塞accept：
    If the socket is marked nonblocking and no pending connections are present on the queue, 
accept() fails with the error EAGAIN or EWOULDBLOCK
Tips: https://stackoverflow.com/questions/7003234/which-systems-define-eagain-and-ewouldblock-as-
different-values
●  example:

redis:
 856     while(max--) {
 857         cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &cport);
 858         if (cfd == ANET_ERR) {
 859             if (errno != EWOULDBLOCK)
 860                 serverLog(LL_WARNING,
 861                     "Accepting client connection: %s", server.neterr);
 862             return;
 863         }
nginx:
或者accept4
 68         if (s == (ngx_socket_t) -1) {
 69             err = ngx_socket_errno;
 70 
 71             if (err == NGX_EAGAIN) {//EAGAIN
 72                 ngx_log_debug0(NGX_LOG_DEBUG_EVENT, ev->log, err,
 73                                "accept() not ready");
 74                 return;
 75             }

API - getaddrinfo

●  问题

getaddrinfo_a

●  example:
libuv

API - connect

● int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); 

● 非阻塞Connect：
        linux:  EINPROGRESS
              The socket is non-blocking and the connection cannot be completed immediately.  It is 
possible to select(2) or poll(2) for completion by  selecting the  socket  for  writing.   After  
select(2) indicates writability, use getsockopt(2) to read the SO_ERROR option at level 
SOL_SOCKET to determine whether connect() completed successfully (SO_ERROR is zero) or 
unsuccessfully (SO_ERROR is one of the usual error codes listed here, explaining the reason for 
the failure).
             其它类型的失败需要close socket

●  example:
redis:
if (connect(s,(struct sockaddr*)&sa,sizeof(sa)) == -1) {                                                                                                                     
         if (errno == EINPROGRESS &&
390             flags & ANET_CONNECT_NONBLOCK)
391             return s;
392 
393         anetSetError(err, "connect: %s", strerror(errno));
394         close(s);
395         return ANET_ERR;
396     }

nginx: core/ngx_resolver.c/ngx_tcp_connect

API - send相关

● write/writev/send/sendto/sendmsg

       ssize_t write(int fd, const void *buf, size_t count);
       ssize_t writev(int fd, const struct iovec *iov, int iovcnt);
       ssize_t send(int sockfd, const void *buf, size_t len, int flags);
       ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);
       ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags);

● 阻塞/非阻塞

send实质上仅仅是把数据放入发送缓冲区而已。
阻塞和非阻塞区别在于是否等待发送缓冲区腾出足够的空间。

Q: 阻塞下的write/read表现

Q: why sendmsg/recvmsg
A few things recvmsg and sendmsg can do:
You can do scatter/gather buffers. For example, let's day you want to receive exactly 1MB of data, but you only have 10 buffers that are each 100KB, 
then you can fill up each in a single recvmsg call.
Access to Control flags, ancillary data, and IP packet header fields. For example, for UDP, you can get the destination IP/port address that the packet 
was addressed by enumerating the control data (with certain ioctls enabled) returned from recvmsg.

●  example:
redis: networking.c/writeToClient
nginx: src/os/unix/ngx_unix_send

● SIGPIPE一般为什么要忽略
A SIGPIPE is sent to a process if it tried to write to a socket that had been shutdown for writing or isn't connected (anymore).
UNNP   5.13：When a process writes to a socket that has received an RST, the SIGPIPE signal is sent to the process

API - recv相关

● read/readv/recv/recvfrom/recvmsg 

       ssize_t read(int fd, void *buf, size_t count);
       ssize_t readv(int fd, const struct iovec *iov, int iovcnt);
       ssize_t recv(int sockfd, void *buf, size_t len, int flags);
       ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);
       ssize_t recvmsg(int sockfd, struct msghdr *msg, int flags);

● 阻塞/非阻塞

●  example:

redis:networking.c/readQueryFromClient

    if (nread == -1) {
        if (errno == EAGAIN) {
            return;
        } else {
            serverLog(LL_VERBOSE, "Reading from client: %s",strerror(errno));
            freeClient(c);
            return;
        }
    } else if (nread == 0) {
        serverLog(LL_VERBOSE, "Client closed connection");
        freeClient(c);
        return;
    } else

nginx:os/unix/ngx_send.c/ngx_recv.c

API - blocking

https://stackoverflow.com/questions/5407182/blocking-sockets-when-exactly-does-
send-return

API - close

● int close(int fd); 

● how to do
https://github.com/torvalds/linux/blob/master/net/ipv4/tcp.c        tcp_close

● Q： close之前本端发送的数据，能成功发送出去吗？

● Q： close之后本端能发送数据吗？对端呢？

● Q： close之后本端能接受数据吗？对端呢？

● 异常释放：

　　但也有可能发送一个复位报文段而不是FIN来释放一个连接，这成为异常释放(ahortive release)。异常终止一个连接对应用程序来说有两个优点:(1)丢弃任何待发
数据并立即发送复位报文段;(2)RST的接收方会区分另一端执行的是异常关闭还是正常关闭。应用程序使用的API必须提供产生异常关闭而不是正常关闭的手段。
SocketAPI通过“lingeronclose”选项(SO_LINGER)提供了这种异常关闭的能力:停留时间设为0。

API - close

● SO_LINGER

struct linger
{
     int l_onoff;
     int l_linger;
};
注意： 需要考虑平台，(以及阻塞/非阻塞?)
l_onoff = 0
l_onoff != 0，l_linger = 0
l_onoff !=0，l_linger > 0

https://github.com/torvalds/linux/blob/master/net/ipv4/tcp.c        void tcp_close(struct sock *sk, long timeout)
} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {

/* Check zero linger _after_ checking for unread data. */
sk->sk_prot->disconnect(sk, 0);
NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);

https://github.com/torvalds/linux/blob/master/net/ipv4/af_inet.c        int inet_release(struct socket *sock)

/* If linger is set, we don't return until the close
 * is complete.  Otherwise we return immediately. The
 * actually closing is done the same either way.
 *
 * If the close is due to the process exiting, we never
 * linger..
 */
timeout = 0;
if (sock_flag(sk, SOCK_LINGER) &&
    !(current->flags & PF_EXITING))

timeout = sk->sk_lingertime;

sk->sk_prot->close(sk, timeout);
sock->sk = NULL;

● vs shutdown：

API - close

　　close-----关闭本进程的socket id，但链接还是开着的，用这个socket id的其它进程还能用这个链接，能读或写这个socket id。
　　shutdown--破坏了socket 链接，读的时候可能侦探到EOF结束符，写的时候可能会收到一个SIGPIPE信号，这个信号可能直到socket buffer
被填充了才收到，shutdown有一个关闭方式的参数，0 不能再读，1不能再写，2 读写都不能。

socket 多进程中的 shutdown、close 的使用
　　当所有的数据操作结束以后，你可以调用close()函数来释放该socket，从而停止在该socket上的任何数据操作：close(sockfd);使用close中止
一个连接，但它只是减少描述符的参考数，并不直接关闭连接，只有当描述符的参考数为0时才关闭连接。所以在多进程/线程程序中，close只是确
保了对于某个特定的进程或线程来说，该连接是关闭的。使用 client_fd = accept() 后 fork() 以在子进程中处理请求，此时在父进程中使用 close() 
关闭该连接，子进程仍可以继续使用该连接。

　　也可以调用shutdown()函数来关闭该socket。该函数允许你只停止在某个方向上的数据传输，而一个方向上的数据传输继续进行。如你可以关
闭某socket的写操作而允许继续在该socket上接受数据，直至读入所有数据。int shutdown(int sockfd,int how);shutdown可直接关闭描述符，
不考虑描述符的参考数，可选择中止一个方向的连接。

注意
　　1.如果有多个进程共享一个套接字，close每被调用一次，计数减1，直到计数为0时，也就是所用进程都调用了close，套接字将被释放。

　　2.在多进程中如果一个进程中shutdown(sfd, SHUT_RDWR)后其它的进程将无法进行通信。如果一个进程close(sfd)将不会影响到其它进程，
得自己理解引用计数的用法了。

更多关于close和shutdown的说明
　　1.只要TCP栈的读缓冲里还有未读取（read）数据，则调用close时会直接向对端发送RST。

　　2.shutdown与socket描述符没有关系，即使调用shutdown(fd, SHUT_RDWR)也不会关闭fd，最终还需close(fd)。

　　3.可以认为shutdown(fd, SHUT_RD)是空操作，因为shutdown后还可以继续从该socket读取数据，这点也许还需要进一步证实。在已发送
FIN包后write该socket描述符会引发EPIPE/SIGPIPE。

　　4.当有多个socket描述符指向同一socket对象时，调用close时首先会递减该对象的引用计数，计数为0时才会发送FIN包结束TCP连接。
shutdown不同，只要以SHUT_WR/SHUT_RDWR方式调用即发送FIN包。

　　5.SO_LINGER与close，当SO_LINGER选项开启但超时值为0时，调用close直接发送RST（这样可以避免进入TIME_WAIT状态，但破坏了TCP
协议的正常工作方式），SO_LINGER对shutdown无影响。

　　6.TCP连接上出现RST与随后可能的TIME_WAIT状态没有直接关系，主动发FIN包方必然会进入TIME_WAIT状态，除非不发送FIN而直接以发
送RST结束连接。

API - shutdown

● int shutdown(int sockfd, int how); 
     how-The constants SHUT_RD, SHUT_WR, SHUT_RDWR have the value 0, 1, 2

● 场景：
　　　　1.当你想要确保所有写好的数据已经发送成功时。如果在发送数据的过程中，网络意外断开或者出现
异常，系统不一定会返回异常，这是你可能以为对端已经接收到数据了。这时需要用shutdown()来确定数据
是否发送成功，因为调用shutdown()时只有在缓存中的数据全部发送成功后才会返回。
　　　　2.想用一种方法来捕获程序潜在的错误，这错误可能是因为往一个不能写的socket上写数据，也有可
能是在一个不该读操作的socket上读数据。当程序尝试这样做时，将会捕获到一个异常，捕获异常对于程序排
错来说是相对简单和省劲的。
　　　　3.当您的程序使用了fork()或者使用多线程时，你想防止其他线程或进程访问到该资源，又或者你想
立刻关闭这个socket，那么可以用shutdown()来实现。

●  example:
redis:
 927         /* In the case of diskless replication the fork is writing to the
 928          * sockets and just closing the fd isn't enough, if we don't also
 929          * shutdown the socket the fork will continue to write to the slave
 930          * and the salve will only find out that it was disconnected when
 931          * it will finish reading the rdb. */
 932         if ((c->flags & CLIENT_SLAVE) &&
 933             (c->replstate == SLAVE_STATE_WAIT_BGSAVE_END)) {
 934             shutdown(c->fd, SHUT_RDWR);
 935         }

API - select/poll

●  select

●  poll 

● 场景：
　　　　跨平台, 对效率要求不高的场景

API - epoll/kqueue/evport

● epoll_create/epollcreate1            epoll_ctrl                 epoll_wait            LT/ET

API - epoll/kqueue/evport

●  example:

Redis/src/ae_poll.c
            if (e->events & EPOLLIN) mask |= AE_READABLE;
            if (e->events & EPOLLOUT) mask |= AE_WRITABLE;
            if (e->events & EPOLLERR) mask |= AE_WRITABLE|AE_READABLE;
            if (e->events & EPOLLHUP) mask |= AE_WRITABLE|AE_READABLE;

● Q： why EPOLLOUT

库/框架 - TCP网络编程本质

三个半事件：

– 连接建立
server的accept,  client的connect

– 连接断开
主动断开: close, shutdown    被动断开:read返回值

– 消息到达，fd可读

– 消息发送完毕

– （实际中的网络库为了方便可能还有timer事件，不属于此范畴）

库/框架 - linux网络并发服务模型

•
•
•
•

(based) one conn(req) one thread/process
reactor (主线程 IO)(libuv,nodejs)
reactor + thread(主线程 IO+工作线程)
reactors + thread pool

          Half-Sync/Half-Asynchttps://www.dre.vanderbilt.edu/~schmidt/PDF/PLoP-95.pdf

reactors in threads(one loop per thread)
reactors in processes(nginx)

•
•
• coroutine,  goroutine(golang), actor(erlang)

性能优化

-----------------------------------------------------------------
指标                                         工具
-----------------------------------------------------------------
b(bit)ps吞吐量                          sar, nethogs, iftop                     
p(package)ps                           sar, /proc/net/dev
连接数                                      netstat, ss
延迟                                         ping hping3
连接跟踪                                  conntrack
路由                                         mtr, route, traceroute
DNS                                         dig, nslookup
防火墙                                      iptables, firewall-cmd
网卡功能
丢包                                          netstat, ifconfig, ethtool
抓包
内核协议栈追踪                         bcc, systemtap
内核协议栈剖析                         perf

                  tcpdump, wireshark

   ethtool

性能优化

●  内核层的优化

参数设置

●  应用层的优化
    例如unix domain sockets

io_uringhttps://blog.csdn.net/csdnnews/article/details/108848646

        例如QUIC
        
        例如https://www.infoq.cn/article/netty-million-level-push-service-design-points

●  C10M(单机1000万连接？)

跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序
目前有两种机制: DPDK 与 XDP
    

安全

● 各层的攻击与防御

● DDOS攻击

● ... ...

just for fun
  
 谢谢大家

