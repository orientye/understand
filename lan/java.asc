= java
:revnumber: 0.0.1
:author: orient
:homepage: http://orientye.com
:toc:
:toclevels: 5
:hardbreaks-option:
<<<

== 概念
- 历史
    https://en.wikipedia.org/wiki/Java_version_history

- 分层编译(TieredCompilation)

- JIT
    JIT是指将热点代码以方法为单位转换成机器码，直接运行在底层硬件之上。它采用了多种优化方式，包括静态编译器可以使用的如方法内联、逃逸分析，也包括基于程序运行profile的投机性优化(speculative/optimistic optimization)。
    

- AOT(Ahead-of-Time Compilation)
    提前编译是相对于即时编译的概念
    直接将字节码编译成机器代码，避免了JIT预热等各方面的开销
    Oracle JDK 9就引入了实验性的AOT特性，并且增加了新的jaotc工具
    https://openjdk.org/jeps/295
    优点:
        Java虚拟机加载这些已经预编译成二进制库之后就能够直接调用，而无须再等待即时编译器在运行时将其编译成二进制机器码;
        提前编译可以减少即时编译带来的预热时间
    缺点:
        破坏了Java一次编写，到处运行的承诺;
        降低了Java链接过程的动态性，要求加载的代码在编译期就是全部已知的，而不能在运行期才确定

== 基础
=== 核心
- String、StringBuffer、StringBuilder

    String是final class，且所有属性也都是final的
    由于它的不可变性，类似拼接、裁剪字符串等操作，均会产生新的String对象
    String str1 = "abc123"; //通过直接量赋值方式，放入字符串常量池
    String str2 = new String("abc123"); //通过new方式赋值方式，不放入字符串常量池
    在字符串内容不经常发生变化的场景优先使用String类，如常量声明、少量的字符串拼接操作等

    StringBuffer线程安全

    StringBuilder非线程安全，性能较好，优先考虑

- final、finally、finalize

    final可以用来修饰类、方法、变量
    final的类不可以继承
    final的方法不可以override
    final的变量不可以修改
        https://en.wikipedia.org/wiki/Java_concurrency#Final_fields

    finally是保证代码一定要被执行的一种机制
    使用try-finally或者try-catch-finally来进行类似关闭连接、unlock锁等动作
    Q: 一定能被执行吗？
    A: 不一定，例如try块里调用了System.exit(1)

    finalize是java.lang.Object的一个方法
    保证对象在被垃圾收集前完成特定资源的回收
    从JDK 9起被标记为deprecated:
    https://docs.oracle.com/javase%2F9%2Fdocs%2Fapi%2F%2F/java/lang/Object.html
    推荐使用java.lang.ref.Cleaner, 不过也存在问题

- 强引用、软引用、弱引用、幻象引用

    不同的引用类型，主要体现的是对象不同的可达性(reachable)状态和对垃圾收集的影响。

    强引用Strong Reference，即最常见的普通对象引用
    只要还有强引用指向一个对象，就能表明对象还活着，垃圾收集器不会碰这种对象。
    对于一个普通的对象，如果没有其他的引用关系，
    只要超过了引用的作用域或者显式地将相应(强)引用赋值为null，就是可以被垃圾收集的了，
    当然具体回收时机还是要看垃圾收集策略。

    软引用SoftReference，是一种相对强引用弱化一些的引用，可以让对象豁免一些垃圾收集
    只有当JVM认为内存不足时，才会去试图回收软引用指向的对象。
    JVM会确保在抛出OutOfMemoryError之前，清理软引用指向的对象。
    软引用通常用来实现内存敏感的缓存，
    如果还有空闲内存，就可以暂时保留缓存，
    当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。

    弱引用WeakReference并不能使对象豁免垃圾收集，仅仅是提供一种访问在弱引用状态下对象的途径。
    可以用来构建一种没有特定约束的关系，比如，维护一种非强制性的映射关系
    如果试图获取时对象还在，就使用它，否则重现实例化。
    它同样是很多缓存实现的选择。

    幻象引用Phantom Reference，也叫虚引用，不能通过它访问对象。
    幻象引用仅仅是提供了一种确保对象被finalize以后，做某些事情的机制
    比如，通常用来做所谓的Post-Mortem清理机制，Java平台自身的Cleaner机制等
    也可以利用幻象引用监控对象的创建和销毁

- 反射机制

    赋予程序在运行时自省(introspect)的能力
    通过反射可以直接操作类或者对象，例如:
    获取某个对象的类定义，获取类声明的属性和方法，调用方法或者构造对象，甚至可以运行时修改类定义
    https://docs.oracle.com/javase/tutorial/reflect/index.html

- 动态代理

    一种方便运行时动态构建代理、动态处理代理方法调用的机制
    使用场景: RPC调用、面向切面(AOP)的编程等
    实现动态代理的方式很多，例如JDK自身提供的动态代理，其主要利用了反射机制
    其它的实现方式，例如ASM、cglib(基于ASM)、Javassist等

- int和Integer有何区别

    8个原始数据类型(Primitive Types): boolean、byte、short、char、int、float、double、long
    Integer是int对应的包装类，其它同理
    Java 5中引入了自动装箱和自动拆箱功能(boxing/unboxing)，可以根据上下文自动进行转换

    使用原始数据类型在性能极度敏感的场景往往具有优势
    原始数据类型操作非线程安全
    原始数据类型和Java泛型并不能配合使用
        因为Java的泛型某种程度上可以算作伪泛型，是一种编译期的技巧
        Java编译期会自动将类型转换为对应的特定类型
    对象类型无法高效地表达数据，例如vector
        Java的对象都是引用类型，如果是一个原始数据类型数组，它在内存里是一段连续的内存
        而对象数组则不然，数据存储的是引用，对象往往分散地存储在堆的不同位置
        这种设计虽然带来了极大灵活性，但是也导致了数据操作的低效，尤其是无法充分利用现代CPU缓存机制

- Vector、ArrayList、LinkedList有何区别

    Vector是Java早期提供的线程安全的动态数组，如果不需要线程安全，并不建议选择
    Vector内部是使用对象数组来保存数据，可以根据需要自动的增加容量
    当数组已满时，会创建新的数组，并拷贝原有数组数据

    ArrayList是应用更加广泛的动态数组，非线程安全，因此性能要好很多
    Vector在扩容时会提高1倍，而ArrayList则是增加50%

    LinkedList是双向链表，非线程安全

- Hashtable、HashMap、LinkedHashMap、TreeMap有何区别

    Hashtable是早期Java类库提供的一个哈希表实现，本身是同步的，不支持null键和值
    由于同步导致的性能开销，已经很少被推荐使用

    HashMap基于哈希表，通常是首选
    与HashTable主要区别在于HashMap不是同步的，支持null键和值等
    通常情况下，put或者get操作可以达到常数时间的性能

    LinkedHashMap使用了双链表，能维护插入顺序

    TreeMap基于红黑树，提供顺序访问
    和HashMap不同，get、put、remove之类操作都是O(log(n))的时间复杂度

- HashMap

    数组(Node<K,V>[] table)和链表结合组成的复合结构
    数组被分为一个个桶(bucket)，通过哈希值决定了键值对在这个数组的寻址
    哈希值相同的键值对，则以链表形式存储

    rehashed: that is, internal data structures are rebuilt
    https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html
    如果链表大小超过阈值(TREEIFY_THRESHOLD, 8)，链表就会被改造为树形结构(即树化)。
    树化是为了性能与安全:
        如果对象哈希冲突都被放置到同一个桶里，则会形成一个链表
        链表查询是线性的，会严重影响存取的性能
        恶意制造哈希冲突，导致服务器端CPU大量占用，构成了哈希碰撞拒绝服务攻击
    
    树化改造，对应逻辑主要在putVal()和treeifyBin()中:
    如果容量小于MIN_TREEIFY_CAPACITY(默认64)，只会进行简单的扩容；
    否则，进行树化改造。

    扩容: resize()方法
    https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/HashMap.java

    hashCode和equals的一些基本约定:
        equals相等，hashCode一定要相等
        重写了hashCode也要重写equals
        hashCode需要保持一致性，状态改变返回的哈希值仍然要一致
        equals的对称、反射、传递等特性

    同步:
    Map m = Collections.synchronizedMap(new HashMap(...));

- ConcurrentHashMap

    Hashtable比较低效，其实现基本就是将put、get、size等各种方法加上synchronized
    HashMap非线程安全的，而Collections提供的同步包装器同样低效:
    https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/Collections.java
    private static class SynchronizedMap<K,V> implements Map<K,V>, Serializable {
        //...
        @SuppressWarnings("serial") // Conditionally serializable
        private final Map<K,V> m;     // Backing Map
        @SuppressWarnings("serial") // Conditionally serializable
        final Object      mutex;        // Object on which to synchronize
        //...
        public V get(Object key) {
            synchronized (mutex) {return m.get(key);}
        }
        //
        public V put(K key, V value) {
            synchronized (mutex) {return m.put(key, value);}
        }
        //...
    }

    https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/concurrent/ConcurrentHashMap.java

    Java8:
    依然是桶(bucket)数组 + 链表结构(bin)，但同步的粒度要更细
    内部仍然有Segment定义，但仅仅是为了保证序列化时的兼容性
    由于不再使用Segment，初始化操作得以大大简化，修改为lazy-load形式，避免初始化开销
    利用volatile来保证可见性
    使用CAS等操作，在特定场景进行无锁并发操作
    使用Unsafe、LongAdder等底层手段，进行极端情况的优化

- Sort
原始数据类型:
https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/DualPivotQuicksort.java
对象数据类型:
https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/TimSort.java
参考: https://mail.openjdk.org/pipermail/core-libs-dev/2018-January/051000.html
参考: https://en.wikipedia.org/wiki/Timsort
vs. C++ https://en.wikipedia.org/wiki/Introsort

- Direct vs. non-direct buffers
The contents of direct buffers may reside outside of the normal garbage-collected heap, and so their impact upon the memory footprint of an application might not be obvious. It is therefore recommended that direct buffers be allocated primarily for large, long-lived buffers that are subject to the underlying system's native I/O operations. In general it is best to allocate direct buffers only when they yield a measureable gain in program performance.

- SPI
Java SPI实际上是"基于接口的编程＋策略模式＋约定配置文件"的组合实现的动态加载机制，在JDK中提供了java.util.ServiceLoader来实现服务查找。

=== 异常
==== 分类
Exception: https://docs.oracle.com/javase/8/docs/api/java/lang/Exception.html
Error: https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html

Exception和Error都继承了Throwable类。

Exception是程序正常运行中，可以预料的意外情况，可能并且应该被捕获。

Error是指在正常情况下，不大可能出现的情况，绝大部分的Error都会导致程序(比如JVM自身)处于非正常的、不可恢复状态。Error是非正常情况，不便于也不需要捕获，常见的Error如OutOfMemoryError。

Exception又分为可检查(checked)异常和不检查(unchecked)异常:
可检查异常必须显式地进行捕获处理，是编译期检查的一部分。
不检查异常即运行时异常，例如NullPointerException、ArrayIndexOutOfBoundsException，通常是编码可以避免的逻辑错误，根据需要决定是否进行捕获，不会在编译期强制要求。

关于Checked Exception:
反对Java语言的Checked Exception的原因:
    Checked Exception假设捕获了异常，然后恢复程序，然而大多数情况下是不能恢复的；
    Checked Exception不兼容functional编程。
很多开源项目，已经采纳了这种实践，如Spring、Hibernate等，以及新的编程语言如Scala等。
参考: http://literatejava.com/exceptions/checked-exceptions-javas-biggest-mistake/
另一方面，有一些异常，比如和环境相关的IO、网络等，其实是存在可恢复性的。
参考: https://v.qq.com/x/page/d0635rf5x0o.html[Failing at Failing: How and Why We’ve Been Nonchalantly Moving Away From Exception Handling]

==== 实践
- 尽量不要捕获类似Exception这样的通用异常，而是应该捕获特定异常

- 不要生吞(swallow)异常，否则可能导致非常难以诊断的诡异情况
e.printStackTrace()也不合适，这是因为它prints this throwable and its backtrace to the standard error stream，而生产环境中，标准错误(stderr)不是一个合适的输出选项，很难判断错误输出到哪里了。

- throw early, catch late原则
在更高层面，具有更完整清晰的context，往往可以选择更合理的处理方式。

- 捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层使用者，必须处理异常。

- 事务场景中，抛出异常被catch后，如果需要回滚，一定要注意手动回滚事务。

- finally块必须对资源对象、流对象进行关闭，有异常也要做try-catch。JDK7及以上，可以使用try-with-resources方式。

- 不要在finally块中使用return
解读: try块中的return语句执行成功后，并不马上返回，而是继续执行finally块中的语句，如果此处存在return语句，则在此直接返回，将会丢弃掉try块中的返回点。
反例:
[source, java]
----
private int x = 0;
    public int checkReturn() {
    try {
        // x等于1，此处不返回
        return ++x;
    } finally {
        // 返回的结果是2
        return ++x;
    }
}
----

- 在调用RPC、二方包(一般指公司内部的依赖库)、或动态生成类的相关方法时，捕捉异常必须使用Throwable类来进行拦截。
解读: 通过反射机制来调用方法，如果找不到方法，抛出NoSuchMethodException。什么情况会抛出NoSuchMethodError呢？二方包在类冲突时，仲裁机制可能导致引入非预期的版本使类的方法签名不匹配，或者在字节码修改框架(比如ASM)动态创建或修改类时，修改了相应的方法签名。这些情况，即使代码编译期是正确的，但在代码运行期时，会抛出NoSuchMethodError。

- 防止NPE(NullPointerException)是调用者的责任。

- 对于公司外的http/api开放接口必须使用errorCode；而应用内部推荐异常抛出；跨应用间RPC调用优先考虑使用Result方式，封装isSuccess()方法、errorCode、errorMessage；而应用内部直接抛出异常即可。
解读: 关于RPC方法返回方式使用Result方式的理由:
1)使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。
2)如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。

参考: 《Java开发手册》异常处理

==== 性能
Java每实例化一个Exception，都会对当时的栈进行快照。如果发生的非常频繁，此开销不能被忽略。

建议仅捕获有必要的代码段，尽量不要一个大的try包住整段的代码；也不要利用异常控制代码流程。

=== 注解
https://www.geeksforgeeks.org/annotations-in-java/
https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/annotation/package-summary.html
https://docs.oracle.com/javase/tutorial/java/annotations/index.html

== 内存

=== 内存区域
https://docs.oracle.com/javase/specs/jvms/se21/html/jvms-2.html#jvms-2.5

==== 程序计数器(PC，Program Counter Register)
在 JVM 规范中，每个线程都有它自己的程序计数器，并且任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法。程序计数器会存储当前线程正在执行的 Java 方法的 JVM 指令地址；或者，如果是在执行本地(native)方法，则是未指定值(undefined)。
此内存区域是唯一一个在《Java虚拟机规范》中没有规定任何OutOfMemoryError情况的区域。

==== Java 虚拟机栈(Java Virtual Machine Stack)
早期也叫 Java 栈。每个线程在创建时都会创建一个虚拟机栈，其内部保存一个个的栈帧(Stack Frame)，对应着一次次的 Java 方法调用。在一个时间点，对应的只会有一个活动的栈帧，通常叫作当前帧，方法所在的类叫作当前类。如果在该方法中调用了其他方法，对应的新的栈帧会被创建出来，成为新的当前帧，一直到它返回结果或者执行结束。JVM 直接对 Java 栈的操作只有两个，就是对栈帧的压栈和出栈。栈帧中存储着局部变量表、操作数(operand)栈、动态链接、方法正常退出或者异常退出的定义等。

==== 堆(Heap)
是 Java 内存管理的核心区域，用来放置 Java 对象实例，几乎所有创建的 Java 对象实例都是被直接分配在堆上。堆被所有的线程共享，在虚拟机启动时指定的"Xmx"之类参数就是用来指定最大堆空间等指标。堆是垃圾收集器重点照顾的区域，因此堆内空间还会被不同的垃圾收集器进行进一步的细分，例如新生代、老年代的划分。
根据《Java虚拟机规范》的规定，Java堆可以处于物理上不连续的内存空间中，但在逻辑上应该被视为连续的。

==== 方法区(Method Area)
也是所有线程共享的一块内存区域，用于存储所谓的元(Meta)数据，例如类结构信息，以及对应的运行时常量池、字段、方法代码等。由于早期的 Hotspot JVM 实现，很多人习惯于将方法区称为永久代(Permanent Generation)。Oracle JDK 8 中将永久代移除，同时增加了元数据区(Metaspace)。

==== 运行时常量池(Run-Time Constant Pool)
是方法区的一部分。分析反编译的类文件结构，可以看到版本号、字段、方法、超类、接口等各种信息，还有一项信息就是常量池。Java 的常量池可以存放各种常量信息，不管是编译期生成的各种字面量，还是需要在运行时决定的符号引用，因此它比一般语言的符号表存储的信息更加宽泛。

==== 本地方法栈(Native Method Stack)
与 Java 虚拟机栈是非常相似的，支持对本地方法的调用，也是每个线程都会创建一个。在 Oracle Hotspot JVM 中，本地方法栈和Java虚拟机栈是在同一块区域，这取决于具体的实现，规范中并未作出强制。

=== 直接内存(Direct Memory)
直接内存并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。
但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现。

在JDK 1.4中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区(Buffer)的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。

显然，本机直接内存的分配不会受到Java堆大小的限制，但是既然是内存，则肯定还是会受到本机总内存(包括物理内存、 SWAP分区或者分页文件)大小以及处理器寻址空间的限制，一般服务器管理员配置虚拟机参数时，会根据实际内存去设置-Xmx等参数信息，但经常忽略掉直接内存，使得各个内存区域总和大于物理内存限制(包括物理的和操作系统级的限制)，从而导致动态扩展时出现OutOfMemoryError异常。

=== 堆外内存和堆内内存
- 堆内内存

    JVM GC自动回收
    新生代与老年代
    在JDK1.8版本废弃了永久代，替代的是元空间(MetaSpace)
    元空间与永久代上类似，最大区别是: 元空间并不在JVM中，而是使用本地内存。

- 堆外内存

    不受JVM管理，需要手动释放
    当进行网络I/O操作、文件读写时，堆内内存都需要转换为堆外内存，然后再与底层设备进行交互
    堆外内存可以实现进程之间、JVM多实例之间的数据共享
    分配: ByteBuffer#allocateDirect和Unsafe#allocateMemory
    https://docs.oracle.com/javase/8/docs/api/java/nio/ByteBuffer.html#allocateDirect-int-

    JVM参数-XX:MaxDirectMemorySize指定堆外内存的上限大小:
        当堆外内存的大小超过该阈值时，就会触发一次Full GC进行清理回收
        如果在Full GC之后还是无法满足堆外内存的分配，那么程序将会抛出OOM异常

=== HotSpot虚拟机对象
==== 对象的创建
当Java虚拟机遇到一条字节码new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。

在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务实际上便等同于把一块确定大小的内存块从Java堆中划分出来。需要考虑性能、线程安全等问题。

内存分配完成之后，虚拟机必须将分配到的内存空间(但不包括对象头)都初始化为零值。

接下来，Java虚拟机还要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到
类的元数据信息、对象的哈希码(实际上对象的哈希码会延后到真正调用Object::hashCode()方法时才计算)、对象的GC分代年龄等信息。这些信息存放在对象的对象头(Object Header)之中。

==== 对象的内存布局
对象头(Header)、实例数据(Instance Data)和对齐填充(Padding)

对象头部分包括两类信息:
第一类是用于存储对象自身的运行时数据，如哈希码(HashCode)、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机(未开启压缩指针)中分别为32个比特和64个比特，官方称它为"Mark Word"。
对象头的另外一部分是类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例。

实例数据:
对象真正存储的有效信息，即在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来。
这部分的存储顺序会受到虚拟机分配策略参数(-XX: FieldsAllocationStyle参数)和字段在Java源码中定义顺序的影响。HotSpot虚拟机默认的分配顺序为longs/doubles、ints、shorts/chars、bytes/booleans、oops(Ordinary
Object Pointers，OOPs)，从以上默认的分配策略中可以看到，相同宽度的字段总是被分配到一起存放，在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前。如果HotSpot虚拟机的+XX: CompactFields参数值为true(默认为true)，那子类之中较窄的变量也允许插入父类变量的空隙之中，以节省出一点点空间。

对齐填充:
并不是必然存在的，仅仅起着占位符的作用。
HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍。

==== 对象的定位
Java程序会通过栈上的reference数据来操作堆上的具体对象。
由于reference类型在《Java虚拟机规范》里面只规定了它是一个指向对象的引用，并没有定义这个引用应该通过什么方式去定位、访问到堆中对象的具体位置，所以对象访问方式也是由虚拟机实现而定的，主流的访问方式主要有使用句柄和直接指针两种:
用句柄来访问的最大好处就是reference中存储的是稳定句柄地址，在对象被移动(垃圾收集时移动对象是非常普遍的行为)时只会改变句柄中的实例数据指针，而reference本身不需要被修改;
使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁， 因此这类开销积少成多也是一项极为可观的执行成本。

HotSpot主要使用直接指针进行对象访问。

=== OOM
从数据区的角度，除了程序计数器，其它区域都有可能会因为可能的空间不足发生 OutOfMemoryError:

- 堆内存不足是最常见的 OOM 原因之一，抛出的错误信息是"java.lang.OutOfMemoryError:Java heap space"，原因可能千奇百怪，例如，可能存在内存泄漏问题；也很有可能就是堆的大小不合理；或者出现 JVM 处理引用不及时，导致堆积起来，内存无法释放等。

- 对于 Java 虚拟机栈和本地方法栈，如果写一段程序不断的进行递归调用，而且没有退出条件，就会导致不断地进行压栈。类似这种情况，JVM 实际会抛出 StackOverFlowError；如果 JVM 试图去扩展栈空间的的时候失败，则会抛出 OutOfMemoryError。

- 对于老版本的 Oracle JDK，因为永久代的大小是有限的，并且 JVM 对永久代垃圾回收(如，常量池回收、卸载不再需要的类型)非常不积极，所以不断添加新类型的时候，永久代出现 OutOfMemoryError 也非常多见，尤其是在运行时存在大量动态类型生成的场合；类似 Intern 字符串缓存占用太多空间，也会导致 OOM 问题。对应的异常信息，会标记出来和永久代相关: "java.lang.OutOfMemoryError: PermGen space"。

- 随着元数据区的引入，方法区内存已经不再那么窘迫，相应的 OOM 有所改观，出现 OOM，异常信息则变成了"java.lang.OutOfMemoryError: Metaspace"。

- 直接内存不足，也会导致 OOM。

=== GC
==== 内存释放的判定
===== 概要
主要是两个方面:
最主要的部分是对象实例，都是存储在堆上的；
还有就是方法区中的元数据等信息，例如类型不再使用，卸载该Java类是很合理的。

对于对象实例收集，主要是两种基本算法: 引用计数和可达性分析。
引用计数算法，为对象添加一个引用计数，用于记录对象被引用的情况，如果计数为0，即表示对象可回收。这是很多语言的资源回收选择，例如Python(Python同时支持引用计数和垃圾收集机制)。业界有大规模实践中仅保留引用计数机制，以提高吞吐量的尝试。Java并没有选择引用计数，是因为很难处理循环引用关系。
另外就是Java选择的可达性分析，Java的各种引用关系，在某种程度上，将可达性问题还进一步复杂化，这种类型的垃圾收集通常叫作追踪性垃圾收集(Tracing Garbage Collection)。其原理简单来说，就是将对象及其引用关系看作一个图，选定活动的对象作为GC Roots，然后跟踪引用链条，如果一个对象和GC Roots之间不可达，也就是不存在引用链条，那么即可认为是可回收对象。JVM会把虚拟机栈和本地方法栈中正在引用的对象、静态属性引用的对象和常量，作为GC Roots。

方法区无用元数据的回收比较复杂，一般来说初始化类加载器加载的类型是不会进行类卸载(unload)的；而普通的类型的卸载，往往是要求相应自定义类加载器本身被回收，因此大量使用动态类型的场合，需要防止元数据区(或者早期的永久代)不会OOM。

===== 可达性分析
Java、C#等主流语言的内存管理子系统，都是通过可达性分析(Reachability Analysis)算法来判定对象是否存活的。
其基本思路就是通过一系列称为"GC Roots"的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为引用链"(Reference Chain)，如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。

固定可作为GC Roots的对象包括:
· 在虚拟机栈(栈帧中的本地变量表)中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等
· 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量
· 在方法区中常量引用的对象，譬如字符串常量池(String Table)里的引用
· 在本地方法栈中JNI(即通常所说的Native方法)引用的对象
· Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象(比如NullPointExcepiton、 OutOfMemoryError)等，还有系统类加载器
· 所有被同步锁(synchronized关键字)持有的对象
· 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等

除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象"临时性"地加入，共同构成完整GC Roots集合。譬如分代收集和局部回收(Partial GC)，如果只针对Java堆中某一块区域发起垃圾收集时(如最典型的只针对新生代的垃圾收集)，必须考虑到内存区域是虚拟机自己的实现细节(在用户视角里任何内存区域都是不可见的)，更不是孤立封闭的，所以某个区域里的对象完全有可能被位于堆中其他区域的对象所引用，这时候就需要将这些关联区域的对象也一并加入GC Roots集合中去，才能保证可达性分析的正确性。

即使在可达性分析算法中判定为不可达的对象，也不是非死不可的，这时候它们暂时还处于缓刑阶段，要真正宣告一个对象死亡，至少要经历两次标记过程: 如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。假如对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为"没有必要执行"。

===== 引用计数
引用计数算法(Reference Counting)原理简单，判定效率也很高，在大多数情况下它都是一个不错的算法。也有一些比较著名的应用案例，例如微软COM技术。但是，在Java领域，至少主流的Java虚拟机里面都没有选用引用计数算法来管理内存，主要原因是有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作，譬如单纯的引用计数就很难解决对象之间相互循环引用的问题。

在JDK 1.2版之后，Java对引用的概念进行了扩充，将引用分为:
强引用Strongly Reference、软引用Soft Reference(一些还有用，但非必须的对象)、弱引用Weak Reference和虚引用Phantom Reference 4种，其强度依次逐渐减弱。

===== 回收方法区
方法区的垃圾收集主要回收两部分内容: 废弃的常量和不再使用的类型。

判定一个常量是否废弃相对简单，而要判定一个类型是否属于不再被使用的类的条件就比较苛刻了，需要同时满足下面三个条件:
· 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。
· 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。
· 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。
Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是被允许，而并不是和对象一样，没有引用了就必然会回收。关于是否要对类型进行回收，HotSpot虚拟机提供了-Xnoclassgc参数进行控制。

在大量使用反射、动态代理、CGLib等字节码框架，动态生成JSP以及OSGi这类频繁自定义类加载器的场景中，通常都需要Java虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力。

==== 垃圾收集算法
===== 分代收集理论
- 部分收集(Partial GC)

    指目标不是完整收集整个Java堆的垃圾收集，其中又分为:
        新生代收集(Minor GC/Young GC)
            指目标只是新生代的垃圾收集
        老年代收集(Major GC/Old GC)
            指目标只是老年代的垃圾收集
            目前只有CMS收集器会有单独收集老年代的行为
            另外Major GC在场合有不同所指，需按上下文区分是指老年代的收集还是整堆收集
        混合收集(Mixed GC)
            指目标是收集整个新生代以及部分老年代的垃圾收集
            目前只有G1收集器会有这种行为

- 整堆收集(Full GC)

    收集整个Java堆和方法区的垃圾收集

习惯上把新生代 GC(Young GC)叫作 Minor GC，老年代 GC 叫作 Major GC，区别于整体性的 Full GC。
但是现代 GC 中，这种概念已经不再准确，对于 G1 来说: Minor GC 仍然存在，虽然具体过程会有区别，会涉及 Remembered Set 等相关处理。老年代回收，则是依靠 Mixed GC。并发标记结束后，JVM 就有足够的信息进行垃圾收集，Mixed GC 不仅同时会清理 Eden、Survivor 区域，而且还会清理部分 Old 区域。

===== 标记-清除(Mark-Sweep)算法
最早出现也是最基础的垃圾收集算法是标记-清除算法，1960年由Lisp之父John McCarthy提出。

主要思想: 首先进行标记工作，标识出所有要回收的对象，然后进行清除。

后续的收集算法大多都是以标记-清除算法为基础，对其缺点进行改进而得到的。

主要缺点:
一、执行效率不稳定: 如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低
二、内存空间的碎片化问题

===== 标记-复制(简称复制)算法
例如新生代GC，基本都是基于复制算法，将活着的对象复制到to区域，拷贝过程中将对象顺序放置，就可以避免内存碎片化。代价是，既然要进行复制，既要提前预留内存空间，有一定的浪费；另外，对于G1这种分拆成为大量region的GC，复制而不是移动，意味着GC需要维护region之间对象引用关系，这个开销也不小，不管是内存占用或者时间开销。

标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，因此在老年代一般不能直接选用这种算法。

===== 标记-整理(Mark-Compact)
类似于标记-清除，但为了避免内存碎片化，它会在清理过程中将对象移动，以确保移动后的对象占用连续的内存空间。

标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。

是否移动对象都存在弊端，移动则内存回收时会更复杂，不移动则内存分配时会更复杂。

===== 小结
实际GC实现过程要复杂的多，目前还在发展中的前沿GC都是复合算法，并且并行和并发兼备。

==== HotSpot的实现
根节点枚举
安全点
安全区域
记忆集与卡表
写屏障
并发的可达性分析

==== 垃圾收集器
垃圾收集器(GC，Garbage Collector)与具体的JVM实现紧密相关，不同厂商不同版本的JVM，提供的选择也不同。

以Oracle JDK为例:

- Serial GC
最古老的垃圾收集器，"Serial"体现在其收集工作是单线程的，并且在进行垃圾收集过程中，会进入臭名昭著的"Stop-The-World"状态。当然，其单线程设计也意味着精简的 GC 实现，无需维护复杂的数据结构，初始化也简单，因此一直是 Client 模式下 JVM 的默认选项。从年代的角度，通常将其老年代实现单独称作 Serial Old，它采用了标记-整理(Mark-Compact)算法，区别于新生代的复制算法。
对应 JVM 参数: -XX:+UseSerialGC

- ParNew GC
新生代 GC 实现，实际是 Serial GC 的多线程版本，最常见的应用场景是配合老年代的 CMS GC 工作
对应 JVM 参数: -XX:+UseConcMarkSweepGC -XX:+UseParNewGC

- CMS(Concurrent Mark Sweep) GC
基于标记-清除(Mark-Sweep)算法，设计目标是尽量减少停顿时间，适合Web等反应时间敏感的应用，一直到今天，仍然有很多系统使用 CMS GC。但是，CMS 采用的标记-清除算法，存在着内存碎片化问题，难以避免在长时间运行等情况下发生 full GC，导致恶劣的停顿。另外，既然强调了并发(Concurrent)，CMS 会占用更多 CPU 资源，并和用户线程争抢。

- Parallel GC
在早期 JDK 8 等版本中，它是 server 模式 JVM 的默认 GC 选择，也被称作是吞吐量优先的 GC。它的算法和 Serial GC 比较相似，尽管实现要复杂的多，其特点是新生代和老年代 GC 都是并行进行的，在常见的服务器环境中更加高效。开启选项是:-XX:+UseParallelGC
另外，Parallel GC 引入了开发者友好的配置项，可以直接设置暂停时间或吞吐量等目标，JVM 会自动进行适应性调整，例如参数: 
-XX:MaxGCPauseMillis=value
-XX:GCTimeRatio=N // GC时间和用户时间比例 = 1 / (N+1)

- G1(Garbage First) GC
是一种兼顾吞吐量和停顿时间的 GC 实现，是 Oracle JDK 9 以后的默认 GC 选项。G1 可以直观的设定停顿时间的目标，相比于 CMS GC，G1 未必能做到 CMS 在最好情况下的延时停顿，但是最差情况要好很多。G1 GC 仍然存在着年代的概念，但是其内存结构并不是简单的条带式划分，而是类似棋盘的一个个 region。Region 之间是复制算法，但整体上实际可看作是标记-整理(Mark-Compact)算法，可以有效地避免内存碎片，尤其是当 Java 堆非常大的时候，G1 的优势更加明显。G1 吞吐量和停顿表现都非常不错，并且仍然在不断地完善，与此同时 CMS 已经在 JDK 9 中被标记为废弃(deprecated)。

==== 低延迟垃圾收集器
衡量垃圾收集器的三项最重要的指标是:内存占用（Footprint）、吞吐量（Throughput）和延迟
（Latency），三者共同构成了一个“不可能三角”。

- Shenandoah收集器

- ZGC收集器

==== 命令
查看: jinfo -flags $PID
查看GC基本信息 在JDK 9之前使用-XX:+PrintGC，JDK 9后使用-Xlog:gc
查看GC详细信息 在JDK 9之前使用-XX:+PrintGCDetails，JDK 9后使用-X-log:gc*
查看GC前后的堆、方法区可用容量变化，在JDK 9之前使用-XX:+PrintHeapAtGC，JDK 9之后使用-Xlog:gc+heap=debug
查看GC过程中用户线程并发时间以及停顿的时间，在JDK 9之前使用-XX:+PrintGCApplicationConcurrentTime以及-XX:+PrintGCApplicationStoppedTime，JDK 9之后使用-Xlog:safepoint
查看收集器Ergonomics机制（自动设置堆空间各分代区域大小、收集目标等内容，从Parallel收
集器开始支持）自动调节的相关信息。在JDK 9之前使用-XX:+PrintAdaptive-SizePolicy，JDK 9之后
使用-Xlog:gc+ergo*=trace
查看熬过收集后剩余对象的年龄分布信息，在JDK 9前使用-XX:+PrintTenuring-Distribution，
JDK 9之后使用-Xlog:gc+age=trace

==== 实战:内存分配与回收策略
- 对象优先在Eden分配
大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起
一次Minor GC。

- 大对象直接进入老年代

- 长期存活的对象将进入老年代

- 动态对象年龄判定

- 空间分配担保

==== 参考
https://openjdk.org/jeps/291
https://openjdk.org/jeps/304

== concurrency
=== 介绍
Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling.

Benefits of threads:
1. Exploiting multiple processors
2. Simplicity of modeling
A complicated, asynchronous workflow can be decomposed into a number of simpler, synchronous workflows each running in a separate thread, interacting only with each other at specific synchronization points.
3. Simplified handling of asynchronous events
4. More responsive user interfaces

Risks of threads:
1. Safety hazards
Thread safety can be unexpectedly subtle because, in the absence of sufficient synchronization, the ordering of operations in multiple threads is unpredictable and sometimes surprising.
2. Liveness hazards
safety means "nothing bad ever happens"
liveness concerns the complementary goal that "something good eventually happens"
For example, if thread A is waiting for a resource that thread B holds exclusively, and B never releases it, A will wait forever.
3. Performance hazards
When threads share data, they must use synchronization mechanisms that can inhibit compiler optimizations, flush or invalidate memory caches, and create synchronization traffic on the shared memory bus. All these factors introduce additional performance costs.

Threads are everywhere:
When the JVM starts, it creates threads for JVM housekeeping tasks (garbage collection, finalization) and a main thread for running the main method.
The AWT (Abstract Window Toolkit) and Swing user interface frameworks create threads for managing user interface events.
Timer creates threads for executing deferred tasks.
Component frameworks, such as servlets and RMI create pools of threads and invoke component methods in these threads.

=== 线程安全
==== 概念
If multiple threads access the same mutable state variable without appropriate synchronization, your program is broken. There are three ways to fix it:
• Don’t share the state variable across threads;
• Make the state variable immutable; or
• Use synchronization whenever accessing the state variable.

什么是线程安全:
当多个线程同时访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其它的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。

Thread-safe classes encapsulate any needed synchronization so that clients need not provide their own.
Example: a stateless servlet
[source, java]
----
@ThreadSafe
public class StatelessFactorizer implements Servlet {
    public void service(ServletRequest req, ServletResponse resp) {
        BigInteger i = extractFromRequest(req);
        BigInteger[] factors = factor(i);
        encodeIntoResponse(resp, factors);
    }
}
----
Stateless objects are always thread-safe.

Atomicity原子性:
[source, java]
----
@NotThreadSafe
public class UnsafeCountingFactorizer implements Servlet {
    private long count = 0;
    public long getCount() { return count; }
    public void service(ServletRequest req, ServletResponse resp) {
        BigInteger i = extractFromRequest(req);
        BigInteger[] factors = factor(i);
        ++count; //not thread safe
        encodeIntoResponse(resp, factors);
    }
}
----
race conditions:
[source, java]
----
@NotThreadSafe
public class LazyInitRace {
    private ExpensiveObject instance = null;
    public ExpensiveObject getInstance() {
        if (instance == null)
        instance = new ExpensiveObject();
        return instance;
    }
}
----

[source, java]
----
@ThreadSafe
public class CountingFactorizer implements Servlet {
    private final AtomicLong count = new AtomicLong(0);
    public long getCount() { return count.get(); }
    public void service(ServletRequest req, ServletResponse resp) {
        BigInteger i = extractFromRequest(req);
        BigInteger[] factors = factor(i);
        count.incrementAndGet();
        encodeIntoResponse(resp, factors);
    }
}
----
Where practical, use existing thread-safe objects, like AtomicLong, to manage your class’s state.

Locking:
[source, java]
----
public class Widget {
    public synchronized void doSomething() {
        ...
    }
}
public class LoggingWidget extends Widget {
    public synchronized void doSomething() {
        System.out.println(toString() + ": calling doSomething");
        super.doSomething();
    }
}
----
Code that would deadlock if intrinsic locks were not reentrant.

Guarding state with locks

Liveness and performance:
There is frequently a tension between simplicity and performance. When implementing a synchronization policy, resist the temptation to prematurely sacrifice simplicity (potentially compromising safety) for the sake of performance.
Avoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O.

==== Java语言中的线程安全
可以将Java语言中各种操作共享的数据分为以下五类:
不可变、绝对线程安全、相对线程安全、线程兼容和线程对立。

=== Sharing Objects
==== Visibility
可见性
always use the proper synchronization whenever data is shared across threads.
Locking is not just about mutual exclusion; it is also about memory visibility. To ensure that all threads see the most up-to-date values of shared mutable variables, the reading and writing threads must synchronize on a common lock.

Volatile variables:
Use volatile variables only when they simplify implementing and verifying your synchronization policy; avoid using volatile variables when veryfing correctness would require subtle reasoning about visibility. Good uses of volatile variables include ensuring the visibility of their own state, that of the object they refer to, or indicating that an important lifecycle event (such as initialization or shutdown) has occurred.
Locking can guarantee both visibility and atomicity; volatile variables can only guarantee visibility.
You can use volatile variables only when all the following criteria are met:
• Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value;
• The variable does not participate in invariants with other state variables;
and
• Locking is not required for any other reason while the variable is being accessed.

==== Publication and escape
Publishing an object means making it available to code outside of its current scope, such as by storing a reference to it where other code can find it, returning it from a nonprivate method, or passing it to a method in another class ...
An object that is published when it should not have been is said to have escaped.

==== Thread confinement
线程约束
• Ad-hoc thread confinement(特定目的线程约束)
The decision to use thread confinement is often a consequence of the decision to implement a particular subsystem, such as the GUI, as a single-threaded subsystem.
A special case of thread confinement applies to volatile variables.
建议:
Because of its fragility, ad-hoc thread confinement should be used sparingly; if possible, use one of the stronger forms of thread confinment (stack confinement or ThreadLocal) instead.
• Stack confinement
Stack confinement is a special case of thread confinement in which an object can only be reached through local variables.
Stack confinement (also called within-thread or thread-local usage, but not to be confused with the ThreadLocal library class) is simpler to maintain and less fragile than ad-hoc thread confinement.
• ThreadLocal
ThreadLocal provides get and set accessor methods that maintain a separate copy of the value for each thread that uses it, so a get returns the most recent value passed to set from the currently executing thread.
[source, java]
----
private static ThreadLocal<Connection> connectionHolder
    = new ThreadLocal<Connection>() {
        public Connection initialValue() {
            return DriverManager.getConnection(DB_URL);
        }
    };

public static Connection getConnection() {
    return connectionHolder.get();
}
----

==== Immutability
An object is immutable if:
• Its state cannot be modified after construction;
• All its fields are final;
• It is properly constructed (the this reference does not escape during construction).
[source, java]
.Immutable class built out of mutable underlying objects.
----
@Immutable
public final class ThreeStooges {
    private final Set<String> stooges = new HashSet<String>();
        public ThreeStooges() {
        stooges.add("Moe");
        stooges.add("Larry");
        stooges.add("Curly");
    }

    public boolean isStooge(String name) {
        return stooges.contains(name);
    }
}
----

Final fields:
Even if an object is mutable, making some fields final can still simplify reasoning about its state, since limiting the mutability of an object restricts its set of possible states. An object that is "mostly immutable" but has one or two mutable state variables is still simpler than one that has many mutable variables.
it is a good practice to make all fields private unless they need greater visibility, it is a good practice to make all fields final unless they need to be mutable.

==== Safe publication
Immutable objects can be used safely by any thread without additional synchronization, even when synchronization is not used to publish them.
To publish an object safely, both the reference to the object and the object’s state must be made visible to other threads at the same time. A properly constructed object can be safely published by:
• Initializing an object reference from a static initializer;
• Storing a reference to it into a volatile field or AtomicReference;
• Storing a reference to it into a final field of a properly constructed object; or
• Storing a reference to it into a field that is properly guarded by a lock.

Effectively immutable objects:
Objects that are not technically immutable, but whose state will not be modified after publication, are called effectively immutable.
Safely published effectively immutable objects can be used safely by any thread without additional synchronization.

Mutable objects:
The publication requirements for an object depend on its mutability:
• Immutable objects can be published through any mechanism;
• Effectively immutable objects must be safely published;
• Mutable objects must be safely published, and must be either thread-safe or guarded by a lock.

=== Composing Objects
Designing a thread-safe class
Instance confinement
Delegating thread safety
Adding functionality to existing thread-safe classes
Documenting synchronization policies

=== Building Blocks
==== Synchronized collections
java.util.Collections.synchronizedXxx factory methods

Problems with synchronized collections
Iterators and ConcurrentModificationException
Hidden iterators

==== Concurrent collections
Java 5.0 improves on the synchronized collections by providing several concurrent collection classes. Synchronized collections achieve their thread safety by serializing all access to the collection’s state. The cost of this approach is poor concurrency; when multiple threads contend for the collection-wide lock, throughput suffers.

Replacing synchronized collections with concurrent collections can offer dramatic scalability improvements with little risk.

==== Blocking queues and the producer-consumer pattern
Work stealing can be more scalable than a traditional producer-consumer design because workers don’t contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another’s queue, it does so from the tail rather than the head, further reducing contention.
Work stealing is well suited to problems in which consumers are also producers—when performing a unit of work is likely to result in the identification of more work. For example, processing a page in a web crawler usually results in the identification of new pages to be crawled. Similarly, many graph-exploring algorithms, such as marking the heap during garbage collection, can be efficiently parallelized using work stealing. When a worker identifies a new unit of work, it places it at the end of its own deque (or alternatively, in a work sharing design, on that of another worker); when its deque is empty, it looks for work at the end of someone else’s deque, ensuring that each worker stays busy.

==== Blocking and interruptible methods
Interruption is a cooperative mechanism.

When your code calls a method that throws InterruptedException, then your method is a blocking method too, and must have a plan for responding to interruption. For library code, there are basically two choices:
1. Propagate the InterruptedException. This is often the most sensible policy if you can get away with it—just propagate the InterruptedException to your caller. This could involve not catching InterruptedException, or catching it and throwing it again after performing some brief activity-specific cleanup.
2. Restore the interrupt. Sometimes you cannot throw InterruptedException, for instance when your code is part of a Runnable. In these situations, you must catch InterruptedException and restore the interrupted status by calling interrupt on the current thread, so that code higher up the call stack can see that an interrupt was issued, as demonstrated:
[source, java]
.Restoring the interrupted status so as not to swallow the interrupt.
----
public class TaskRunnable implements Runnable {
    BlockingQueue<Task> queue;
    ...
    public void run() {
        try {
            processTask(queue.take());
        } catch (InterruptedException e) {
            // restore interrupted status
            Thread.currentThread().interrupt();
        }
    }
}
----
But there is one thing you should not do with InterruptedException—catch it and do nothing in response. This deprives code higher up on the call stack of the opportunity to act on the interruption, because the evidence that the thread was interrupted is lost. The only situation in which it is acceptable to swallow an interrupt is when you are extending Thread and therefore control all the code higher up on the call stack.

==== Synchronizers
CountDownLatch: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html
FutureTask: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/FutureTask.html
Semaphore: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Semaphore.html
CyclicBarrier: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CyclicBarrier.html
Phaser: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Phaser.html
Exchanger: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Exchanger.html

=== Task Execution
==== Executing tasks in threads
Ideally, tasks are independent activities: work that doesn’t depend on the state, result, or side effects of other tasks.

==== The Executor framework
Executor may be a simple interface, but it forms the basis for a flexible and powerful framework for asynchronous task execution that supports a wide variety of task execution policies. It provides a standard means of decoupling task submission from task execution, describing tasks with Runnable. The Executor implementations also provide lifecycle support and hooks for adding statistics gathering, application management, and monitoring.
Executor is based on the producer-consumer pattern, where activities that submit tasks are the producers (producing units of work to be done) and the threads that execute tasks are the consumers (consuming those units of work).

Execution policies:
• In what thread will tasks be executed?
• In what order should tasks be executed (FIFO, LIFO, priority order)?
• How many tasks may execute concurrently?
• How many tasks may be queued pending execution?
• If a task has to be rejected because the system is overloaded, which task should be selected as the victim, and how should the application be notified?
• What actions should be taken before or after executing a task?

Whenever you see code of the form: new Thread(runnable).start()
and you think you might at some point want a more flexible execution policy, seriously consider replacing it with the use of an Executor.

Thread pools

Executor lifecycle

Delayed and periodic tasks

Executors提供了5种不同的线程池创建配置:
newCachedThreadPool()，它是一种用来处理大量短时间工作任务的线程池:它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过60秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。
newFixedThreadPool(int nThreads)，重用指定数目nThreads的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目nThreads。
newSingleThreadExecutor()，工作线程数目被限制为1，操作一个无界的工作队列，它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目。
newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。
newWorkStealingPool(int parallelism)，Java 8才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。

==== Finding exploitable parallelism
Result-bearing tasks: Callable and Future

CompletionService: Executor meets BlockingQueue

Placing time limits on tasks

=== Cancellation and Shutdown
Java does not provide any mechanism for safely forcing a thread to stop what it is doing. Instead, it provides interruption, a cooperative mechanism that lets one thread ask another to stop what it is doing.

==== Task cancellation
There are a number of reasons why you might want to cancel an activity: user-requested, cancellation, time-limited activities, application events, errors, shutdown.

There is no safe way to preemptively stop a thread in Java, and therefore no safe way to preemptively stop a task. There are only cooperative mechanisms, by which the task and the code requesting cancellation follow an agreed-upon protocol.

=== Applying Thread Pools
==== Implicit couplings between tasks and execution policies
While the Executor framework offers substantial flexibility in specifying and modifying execution policies, not all tasks are compatible with all execution policies. Types of tasks that require specific execution policies include:
    Dependent tasks
    Tasks that exploit thread confinement
    Response-time-sensitive tasks
    Tasks that use ThreadLocal

Thread pools work best when tasks are homogeneous and independent.

==== Sizing thread pools
Sizing thread pools is not an exact science, but fortunately you need only avoid the extremes of "too big" and "too small".

    Given these definitions:
        Ncpu = number of CPUs
        Ucpu = target CPU utilization, 0 ≤ Ucpu ≤ 1
        W/C = ratio of wait time to compute time
    The optimal pool size for keeping the processors at the desired utilization is:
    Nthreads = Ncpu ∗ Ucpu ∗ (1 + W/C)
    int N_CPUS = Runtime.getRuntime().availableProcessors();

==== Configuring ThreadPoolExecutor
==== Extending ThreadPoolExecutor
==== Parallelizing recursive algorithms

=== GUI Applications
==== Why are GUIs single-threaded?
Single-threaded GUI frameworks are not unique to Java; Qt, NextStep, MacOS Cocoa, X Windows, and many others are also single-threaded. This is not for lack of trying; there have been many attempts to write multithreaded GUI frameworks, but because of persistent problems with race conditions and deadlock, they all eventually arrived at the single-threaded event queue model in which a dedicated thread fetches events off a queue and dispatches them to applicationdefined event handlers. (AWT originally tried to support a greater degree of multithreaded access, and the decision to make Swing single-threaded was based largely on experience with AWT.)

==== Short-running GUI tasks
For simple, short-running tasks, the entire action can stay in the event thread; for longer-running tasks, some of the processing should be offloaded to another thread.

==== Long-running GUI tasks

==== Shared data models

==== Other forms of single-threaded subsystems
Thread confinement is not restricted to GUIs: it can be used whenever a facility is implemented as a single-threaded subsystem. Sometimes thread confinement is forced on the developer for reasons that have nothing to do with avoiding synchronization or deadlock. For example, some native libraries require that all access to the library, even loading the library with System.loadLibrary, be made from the same thread.

=== Avoiding Liveness Hazards
==== Deadlock
Lock-ordering deadlocks
Dynamic lock order deadlocks
Deadlocks between cooperating objects
Open calls
Resource deadlocks
==== Avoiding and diagnosing deadlocks
Timed lock attempts
Deadlock analysis with thread dumps

==== Other liveness hazards
Starvation
Poor responsiveness
Livelock:
Livelock is a form of liveness failure in which a thread, while not blocked, still cannot make progress because it keeps retrying an operation that will always fail.

=== Performance and Scalability
==== Thinking about performance
Some of these (service time, latency) are measures of “how fast” a given unit of work can be processed or acknowledged; others (capacity, throughput) are measures of “how much” work can be performed with a given quantity of computing resources.

Avoid premature optimization. First make it right, then make it fast - if it is not already fast enough.

Measure, don’t guess.

==== Amdahl’s law
==== Costs introduced by threads
Context switching
Memory synchronization
Blocking

==== Reducing lock contention
serialization hurts scalability and that context switches hurt performance. Contended locking causes both, so reducing lock contention can improve both performance and scalability.

There are three ways to reduce lock contention:
• Reduce the duration for which locks are held;
• Reduce the frequency with which locks are requested; or
• Replace exclusive locks with coordination mechanisms that permit greater concurrency.

===== Narrowing lock scope (“Get in, get out”)
===== Reducing lock granularity
The other way to reduce the fraction of time that a lock is held (and therefore the likelihood that it will be contended) is to have threads ask for it less often.
This can be accomplished by lock splitting and lock striping, which involve using separate locks to guard multiple independent state variables previously guarded by a single lock. These techniques reduce the granularity at which locking occurs, potentially allowing greater scalability—but using more locks also increases the risk of deadlock.

===== Lock striping
锁分段，例如ConcurrentHashMap

===== Avoiding hot fields
ConcurrentHashMap avoids this problem by having size enumerate the stripes and add up the number of elements in each stripe, instead of maintaining a global count. To avoid enumerating every element, ConcurrentHashMap maintains a separate count field for each stripe, also guarded by the stripe lock.

===== Alternatives to exclusive locks
These include using the concurrent collections, read-write locks, immutable objects and atomic variables.

===== Monitoring CPU utilization
Insufficent load
I/O-bound
Externally bound
Lock contention

===== Just say no to object pooling
Allocating objects is usually cheaper than synchronizing.

==== Example: Comparing Map performance

==== Reducing context switch overhead

=== Testing Concurrent Programs
Throughput: the rate at which a set of concurrent tasks is completed;
Responsiveness: the delay between a request for and completion of some action(also called latency); or
Scalability: the improvement in throughput (or lack thereof) as more resources(usually CPUs) are made available.

==== Testing for correctness
Basic unit tests
Testing blocking operations
Testing safety
Testing resource management
Using callbacks
Generating more interleavings

==== Testing for performance

==== Avoiding performance testing pitfalls

==== Complementary testing approaches
Code review
Static analysis tools
Aspect-oriented testing techniques
Profilers and monitoring tools

=== Explicit Locks
==== Lock and ReentrantLock
Why create a new locking mechanism that is so similar to intrinsic locking? Intrinsic locking works fine in most situations but has some functional limitations—it is not possible to interrupt a thread waiting to acquire a lock, or to attempt to acquire a lock without being willing to wait for it forever. Intrinsic locks also must be released in the same block of code in which they are acquired; this simplifies coding and interacts nicely with exception handling, but makes non-blockstructured locking disciplines impossible. None of these are reasons to abandon synchronized, but in some cases a more flexible locking mechanism offers better liveness or performance.

==== Performance considerations
Performance and scalability are sensitive to platform factors such as CPU, processor count, cache size, and JVM characteristics, all of which can change over time.

==== Fairness

==== Choosing between synchronized and ReentrantLock
ReentrantLock is an advanced tool for situations where intrinsic locking is not practical. Use it if you need its advanced features: timed, polled, or interruptible lock acquisition, fair queueing, or non-block-structured locking. Otherwise, prefer synchronized.

==== Read-write locks

=== Building Custom Synchronizers
But if the library classes do not provide the functionality you need, you can also build your own synchronizers using the low-level mechanisms provided by the language and libraries, including intrinsic condition queues, explicit Condition objects, and the AbstractQueuedSynchronizer framework.

==== Managing state dependence
==== Using condition queues
==== Explicit condition objects
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/Condition.html
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/Lock.html

==== Anatomy of a synchronizer
In actuality, they are both implemented using a common base class, AbstractQueuedSynchronizer (AQS)—as are many other synchronizers. AQS is a framework for building locks and synchronizers, and a surprisingly broad range of synchronizers can be built easily and efficiently using it. Not only are ReentrantLock and Semaphore built using AQS, but so are CountDownLatch, ReentrantReadWriteLock, SynchronousQueue,12 and FutureTask.

==== AbstractQueuedSynchronizer

==== AQS in java.util.concurrent synchronizer classes
ReentrantLock
Semaphore and CountDownLatch
FutureTask
ReentrantReadWriteLock

=== Atomic Variables and Nonblocking Synchronization
==== Disadvantages of locking
For lock-based classes with fine-grained operations (such as the synchronized collections classes, where most methods contain only a few operations), the ratio of scheduling overhead to useful work can be quite high when the lock is frequently contended.

When a thread is waiting for a lock, it cannot do anything else. If a thread holding a lock is delayed (due to a page fault, scheduling delay, or the like), then no thread that needs that lock can make progress. This can be a serious problem if the blocked thread is a high-priority thread but the thread holding the lock is a lower-priority thread—a performance hazard known as priority inversion.

locking is simply a heavyweight mechanism for fine-grained operations such as incrementing a counter.

==== Hardware support for concurrency
Today, nearly every modern processor has some form of atomic readmodify-write instruction, such as compare-and-swap or load-linked/store-conditional.

===== Compare and swap
[source, java]
----
@ThreadSafe
public class SimulatedCAS {
    @GuardedBy("this") private int value;
    
    public synchronized int get() { return value; }
    
    public synchronized int compareAndSwap(int expectedValue, int newValue) {
        int oldValue = value;
        if (oldValue == expectedValue)
            value = newValue;
        return oldValue;
    }

    public synchronized boolean compareAndSet(int expectedValue, int newValue) {
        return (expectedValue == compareAndSwap(expectedValue, newValue));
    }
}
----

===== A nonblocking counter
[source, java]
----
@ThreadSafe
public class CasCounter {
    private SimulatedCAS value;

    public int getValue() {
        return value.get();
    }

    public int increment() {
        int v;
        do {
        v = value.get();
        }
        while (v != value.compareAndSwap(v, v + 1));
        return v + 1;
    }
}
----

===== CAS support in the JVM
since java 5.0

==== Atomic variable classes
The fast (uncontended) path for updating an atomic variable is no slower than the fast path for acquiring a lock, and usually faster; the slow path is definitely faster than the slow path for locks because it does not involve suspending and rescheduling threads.

https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-summary.html

===== Atomics as "better volatiles"
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html

===== Performance comparison: locks versus atomic variables
With low to moderate contention, atomics offer better scalability; with high contention, locks offer better contention avoidance.

==== Nonblocking algorithms
An algorithm is called nonblocking if failure or suspension of any thread cannot cause failure or suspension of another thread; an algorithm is called lock-free if, at each step, some thread can make progress.

Algorithms that use CAS exclusively for coordination between threads can, if constructed correctly, be both nonblocking and lock-free. An uncontended CAS always succeeds, and if multiple threads contend for a CAS, one always wins and therefore makes progress.

Nonblocking algorithms are also immune to deadlock or priority inversion (though they can exhibit starvation or livelock because they can involve repeated retries).

===== A nonblocking stack
Nonblocking stack using Treiber’s algorithm (Treiber, 1986):
[source, java]
----
@ThreadSafe
public class ConcurrentStack <E> {
    AtomicReference<Node<E>> top = new AtomicReference<Node<E>>();
    
    public void push(E item) {
        Node<E> newHead = new Node<E>(item);
        Node<E> oldHead;
        do {
            oldHead = top.get();
            newHead.next = oldHead;
        } while (!top.compareAndSet(oldHead, newHead));
    }

    public E pop() {
        Node<E> oldHead;
        Node<E> newHead;
        do {
            oldHead = top.get();
            if (oldHead == null)
                return null;
            newHead = oldHead.next;
        } while (!top.compareAndSet(oldHead, newHead));
        return oldHead.item;
    }

    private static class Node <E> {
        public final E item;
        public Node<E> next;
        public Node(E item) {
            this.item = item;
        }
    }
}
----

===== A nonblocking linked list
the basic pattern of using CAS to update a value speculatively, retrying if the update fails.

Insertion in the Michael-Scott nonblocking queue algorithm (Michael and Scott, 1996):

[source, java]
----
@ThreadSafe
public class LinkedQueue <E> {
    private static class Node <E> {
        final E item;
        final AtomicReference<Node<E>> next;
        
        public Node(E item, Node<E> next) {
            this.item = item;
            this.next = new AtomicReference<Node<E>>(next);
        }
    }
    
    private final Node<E> dummy = new Node<E>(null, null);
    private final AtomicReference<Node<E>> head
        = new AtomicReference<Node<E>>(dummy);
    private final AtomicReference<Node<E>> tail
        = new AtomicReference<Node<E>>(dummy);
    
    public boolean put(E item) {
        Node<E> newNode = new Node<E>(item, null);
        while (true) {
            Node<E> curTail = tail.get();
            Node<E> tailNext = curTail.next.get();
            if (curTail == tail.get()) {
                if (tailNext != null) {
                    // Queue in intermediate state, advance tail
                    tail.compareAndSet(curTail, tailNext);
                } else {
                    // In quiescent state, try inserting new node
                    if (curTail.next.compareAndSet(null, newNode)) {
                        // Insertion succeeded, try advancing tail
                        tail.compareAndSet(curTail, newNode);
                        return true;
                    }
                }
            }
        }
    }
}
----

===== Atomic field updaters
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicIntegerFieldUpdater.html
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicLongFieldUpdater.html
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReferenceFieldUpdater.html

===== The ABA problem
AtomicStampedReference (and its cousin AtomicMarkableReference) provide atomic conditional update on a pair of variables. AtomicStampedReference updates an object reference-integer pair, allowing “versioned” references that are immune8 to the ABA problem. Similarly, AtomicMarkableReference updates an object reference-boolean pair that is used by some algorithms to let a node remain in a list while being marked as deleted.

=== Java-Memory-Model-and-Thread
==== 概念
- 内存模型
可以理解为在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象。

- JSR-133
Java Memory Model and Thread Specification Revision(Java内存模型和线程规范修订)
https://download.oracle.com/otndocs/jcp/memory_model-1.0-prd-oth-G-F/

==== Java内存模型
===== 主内存与工作内存
Java内存模型规定了所有的变量都存储在主内存(Main Memory)中(此处的主内存与介绍物理硬件时提到的主内存名字一样，两者也可以类比，但物理上它仅是虚拟机内存的一部分)。

每个线程还有自己的工作内存(Working Memory，可与处理器高速缓存类比)，线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。

===== 内存间交互操作

===== volatile
====== 作用
volatile是Java虚拟机提供的最轻量级的同步机制，但是它并不容易被正确、完整地理解。

- 保证变量的可见性
线程读变量能读到它在内存中的最新的值，也就是说不同的线程看到的一个变量的值是相同的。CPU都是有行缓存的，volatile能让行缓存无效，因此能读到内存中最新的值。

- 保证赋值操作的原子性
有些变量的赋值本身就是原子性的，例如boolean，int，但是对于long或者double则不一定，例如32位的处理器，对于64位的变量的操作可能会被分解成为高32位和低32位，从而导致线程不安全。如果变量声明为volatile，那么虚拟机会保证赋值是原子的，是不可被打断的。

- 禁止指令重排
正常情况下，虚拟机会对指令进行重排，当然是在不影响程序结果的正确性的前提下。volatile能够在一定程度上禁止虚拟机进行指令重排。还有就是对于volatile变量的写操作，保证是在读操作之前完成，假设线程A来读变量，刚好线程B正在写变量，那么虚拟机会保证写在读之前完成。

====== 注意事项
volatile变量只能保证可见性，在不符合以下两条规则的运算场景中，仍然要通过加锁来保证原子性:

    运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值
    变量不需要与其它的状态变量共同参与不变约束

    例如volatile int race，执行race++操作，并不线程安全

====== 实现
[source, cpp]
.https://github.com/openjdk/jdk/blob/master/src/hotspot/os_cpu/linux_x86/orderAccess_linux_x86.hpp
----
// A compiler barrier, forcing the C++ compiler to invalidate all memory assumptions
static inline void compiler_barrier() {
  __asm__ volatile ("" : : : "memory");
}

inline void OrderAccess::loadload()   { compiler_barrier(); }
inline void OrderAccess::storestore() { compiler_barrier(); }
inline void OrderAccess::loadstore()  { compiler_barrier(); }
inline void OrderAccess::storeload()  { fence();            }

inline void OrderAccess::acquire()    { compiler_barrier(); }
inline void OrderAccess::release()    { compiler_barrier(); }

inline void OrderAccess::fence() {
   // always use locked addl since mfence is sometimes expensive
#ifdef AMD64
  __asm__ volatile ("lock; addl $0,0(%%rsp)" : : : "cc", "memory");
#else
  __asm__ volatile ("lock; addl $0,0(%%esp)" : : : "cc", "memory");
#endif
  compiler_barrier();
}

inline void OrderAccess::cross_modify_fence() {
  int idx = 0;
#ifdef AMD64
  __asm__ volatile ("cpuid " : "+a" (idx) : : "ebx", "ecx", "edx", "memory");
#else
  // On some x86 systems EBX is a reserved register that cannot be
  // clobbered, so we must protect it around the CPUID.
  __asm__ volatile ("xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx " : "+a" (idx) : : "esi", "ecx", "edx", "memory");
#endif
}
----

===== 针对long和double型变量的特殊规则
对于64位的数据类型(long和double)，在模型中特别定义了一条宽松的规定:允许虚拟机将没有
被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现自行选择是否要保证64位数据类型的load、store、read和write这四个操作的原子性，这就是所谓的"long和double的非原子性协定"(Non-Atomic Treatment of double and long Variables)。

如果有多个线程共享一个并未声明为volatile的long或double类型的变量，并且同时对它们进行读取和修改操作，那么某些线程可能会读取到一个既不是原值，也不是其他线程修改值的代表了"半个变量"的数值。不过这种读取到"半个变量"的情况是非常罕见的，经过实际测试，在目前主流平台下商用的64位Java虚拟机中并不会出现非原子性访问行为，但是对于32位的Java虚拟机，譬如比较常用的32位x86平台下的HotSpot虚拟机，对long类型的数据确实存在非原子性访问的风险。

在实际开发中，除非该数据有明确可知的线程竞争，否则在编写代码时一般不需要因为这个原因刻意把用到的long和double变量专门声明为volatile。

===== 原子性、可见性与有序性
====== 原子性
基本数据类型的访问、读写都是具备原子性的(例外就是long和double的非原子性协定)。
更大范围的原子性保证，Java内存模型提供了lock和unlock操作，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了更高层次的字节码指令monitorenter和monitorexit来隐式地使用这两个操作，反映到代码中就是同步块-synchronized关键字，即在synchronized块之间的操作也具备原子性。

====== 可见性
可见性就是指当一个线程修改了共享变量的值时，其它线程能够立即得知这个修改。

Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是volatile变量都是如此。普通变量与volatile变量的区别是，volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。因此可以说volatile保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。

除了volatile之外，Java还有两个关键字能实现可见性，它们是synchronized和final。同步块的可见性是由对一个变量执行unlock操作之前，必须先把此变量同步回主内存中(执行store、write操作)的规则获得的。而final关键字的可见性是指:被final修饰的字段在构造器中一旦被初始化完成，并且构造器没有把"this"的引用传递出去(this引用逃逸是一件很危险的事情，其它线程有可能通过这个引用访问到"初始化了一半"的对象)，那么在其它线程中就能看见final字段的值。
参考: https://en.wikipedia.org/wiki/Java_concurrency#Final_fields

====== 有序性
Java程序中天然的有序性可以总结为一句话:如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指"线程内似表现为串行的语义"(Within-Thread As-If-SerialSemantics)，后半句是指"指令重排序"现象和"工作内存与主内存同步延迟"现象。

Java提供了volatile和synchronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由"一个变量在同一个时刻只允许一条线程对其进行lock操作"这条规则获得的，这个规则决定了持有同一个锁的两个同步块只能串行地进入。

===== 先行发生(Happens-Before)
happen-before关系，是Java内存模型中保证多线程操作可见性的机制，也是对早期语言规范中含糊的可见性概念的一个精确定义。

先行发生是Java内存模型中定义的两项操作之间的偏序关系，比如说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响能被操作B观察到，"影响"包括修改了内存中共享变量的值、发送了消息、调用了方法等。

Java语言无须任何同步手段保障就能成立的先行发生规则有且只有:

- 程序次序规则(Program Order Rule)
在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。注意，这里说的是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。

- 管程锁定规则(Monitor Lock Rule)
一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是"同一个锁"，而"后面"是指时间上的先后。

- volatile变量规则(Volatile Variable Rule)
对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的"后面"同样是指时间上的先后。

- 线程启动规则(Thread Start Rule)
Thread对象的start()方法先行发生于此线程的每一个动作。

- 线程终止规则(Thread Termination Rule)
线程中的所有操作都先行发生于对此线程的终止检测，可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。

- 线程中断规则(Thread Interruption Rule)
对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::interrupted()方法检测到是否有中断发生。

- 对象终结规则(Finalizer Rule)
一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。

- 传递性(Transitivity)
如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。

// 以下操作在同一个线程中执行
int i = 1;
int j = 2;
两条赋值语句在同一个线程之中，根据程序次序规则，int i = 1的操作先行发生于int j = 2，但是int j = 2的代码完全可能先被处理器执行，这并不影响先行发生原则的正确性，因为在这条线程之中没有办法感知到这一点。

注意，时间先后顺序与先行发生原则之间基本没有因果关系，衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。

===== What is a memory model, and why would I want one?
- Platform memory models
- Reordering
- happens-before
- Piggybacking on synchronization

===== Publication
====== Unsafe publication
====== Safe publication
====== Safe initialization idioms
====== Double-checked locking

===== Initialization safety
Initialization safety makes visibility guarantees only for the values that are reachable through final fields as of the time the constructor finishes. For values reachable through nonfinal fields, or values that may change after construction, you must use synchronization to ensure visibility.

==== Java与线程

===== 实现
实现线程主要有三种方式:

    使用内核线程实现(1: 1实现)
    使用用户线程实现(1: N实现)
    使用用户线程加轻量级进程混合实现(N: M实现)

早期的Classic虚拟机上(JDK 1.2以前)，是基于一种被称为绿色线程(Green Threads)的用户线程实现的
从JDK 1.3起，主流平台上的商用Java虚拟机的线程模型普遍都被替换为基于操作系统原生线程模型来实现，即采用1:1的线程模型。以HotSpot为例，其每一个Java线程都是直接映射到一个操作系统原生线程来实现的，中间没有额外的间接结构，调度等都是由操作系统全权决定。

===== 调度
线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种:
协同式Cooperative Threads-Scheduling线程调度与抢占式Preemptive Threads-Scheduling线程调度。

Java线程调度是系统自动完成的，应用可以设置线程优先级，通常优先级越高的线程越容易被系统选择执行。

===== 状态
https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.State.html

    NEW
        A thread that has not yet started is in this state.
    RUNNABLE
        A thread executing in the Java virtual machine is in this state.
    BLOCKED
        A thread that is blocked waiting for a monitor lock is in this state.
    WAITING
        A thread that is waiting indefinitely for another thread to perform a particular action is in this state.
    TIMED_WAITING
        A thread that is waiting for another thread to perform an action for up to a specified waiting time is in this state.
    TERMINATED
        A thread that has exited is in this state.

Q: 一个线程两次调用start()方法会出现什么情况？
A: Java的线程是不允许启动两次的，第二次调用必然会抛出IllegalThreadStateException(运行时异常)。因为在第二次调用start()方法的时候，线程可能处于终止或者其它(非NEW)状态。

守护线程:

    Thread daemonThread = new Thread();
    daemonThread.setDaemon(true);
    daemonThread.start();

==== virtual thread
===== 概念
since JDK19, JDK 21(September 2023) release.
https://openjdk.org/jeps/444 JEP 444: Virtual Threads
https://openjdk.org/jeps/436 JEP 436: Virtual Threads (Second Preview)
https://openjdk.org/jeps/425 JEP 425: Virtual Threads (Preview)

https://mail.openjdk.org/pipermail/loom-dev/2019-November/000876.html
https://www.zhihu.com/question/332042250

They exist to provide scale (higher throughput), not speed (lower latency).
https://docs.oracle.com/en/java/javase/21/core/virtual-threads.html

===== 调度
JDK 的虚拟线程调度器是一个在 FIFO 模式下运行的类似ForkJoinPool的线程池。

调度器的并行数量取决于调度器虚拟线程的平台线程数量。默认情况下是 CPU 可用核心数量，但可以使用系统属性jdk.virtualThreadScheduler.parallelism进行调整。

注意，这里的ForkJoinPool与ForkJoinPool.commonPool()不同，ForkJoinPool.commonPool()用于实现并行流，并在 LIFO 模式下运行。ForkJoinPool和ExecutorService的工作方式不同，ExecutorService有一个等待队列来存储它的任务，其中的线程将接收并处理这些任务。而ForkJoinPool的每一个线程都有一个等待队列，当一个由线程运行的任务生成另一个任务时，该任务被添加到该线程的等待队列中，当运行Parallel Stream，一个大任务划分成两个小任务时就会发生这种情况。

为了防止线程饥饿问题，当一个线程的等待队列中没有更多的任务时，ForkJoinPool还实现了另一种模式，称为任务窃取， 也就是说:饥饿线程可以从另一个线程的等待队列中窃取一些任务。这和 Go G-M-P 模型中 work stealing 机制有异曲同工之妙。

在以下两种情况下，虚拟线程会被固定到运行它的平台线程，在阻塞操作期间无法卸载虚拟线程:
    当在synchronized块或方法中执行代码时。
    当执行native方法或foreign function时。
虚拟线程被固定不会影响程序运行的正确性，但它可能会影响系统的并发度和吞吐量。如果虚拟线程在被固定时执行 I/O或BlockingQueue.take()等阻塞操作，则负责运行它的平台线程在操作期间会被阻塞。(如果虚拟线程没有被固定，那会执行 I/O 等阻塞操作时会从平台线程上卸载)

参考: https://www.zhihu.com/question/332042250/answer/2683420106

===== 实现
https://blog.rockthejvm.com/ultimate-guide-to-java-virtual-threads/#8-some-virtual-threads-internals
https://www.cnblogs.com/throwable/p/16758997.html

===== misc
https://github.com/electronicarts/ea-async
https://github.com/openjdk/loom
https://github.com/Tencent/TencentKona-8/wiki/KonaFiber%E7%94%A8%E6%88%B7%E6%96%87%E6%A1%A3-282

===== 参考
https://mail.openjdk.org/pipermail/loom-dev/2019-November/000876.html

==== 参考
https://en.wikipedia.org/wiki/Java_concurrency#Memory_model
《深入理解Java虚拟机: JVM高级特性与最佳实践(第3版)》第12章 Java内存模型与线程
https://download.oracle.com/otndocs/jcp/memory_model-1.0-prd-oth-G-F/
https://en.wikipedia.org/wiki/Java_memory_model
http://gee.cs.oswego.edu/dl/jmm/cookbook.html

=== API
- java.util.concurrent
https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/concurrent/package-summary.html

- java.util.concurrent.locks
https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/concurrent/locks/package-summary.html

- java.util.concurrent.atomic
https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/concurrent/atomic/package-summary.html

并发包也就是java.util.concurrent及其子包，集中了Java并发的各种基础工具类，主要包括:
1. 提供了比synchronized更加高级的各种同步结构，包括CountDownLatch、CyclicBarrier、Semaphore等，可以实现更加丰富的多线程操作。
2. 各种线程安全的容器，例如ConcurrentHashMap、有序的ConcurrentSkipListMap，或者通过类似快照机制，实现线程安全的动态数组CopyOnWriteArrayList等。
3. 各种并发队列实现，如各种BlockingQueue实现，比较典型的ArrayBlockingQueue、SynchronousQueue或针对特定场景的PriorityBlockingQueue等。
4. Executor框架，可以创建各种不同类型的线程池，调度任务运行等，绝大部分情况下，不再需要自己实现线程池和任务调度器。

Q: 为什么并发容器里面没有ConcurrentTreeMap？
A: 基于红黑树的TreeMap要实现高效的线程安全是非常困难的。

Q: ConcurrentLinkedQueue与LinkedBlockingQueue有什么区别？
A: ConcurrentLinkedQueue等Concurrent类型基于lock-free，在常见的多线程访问场景，一般可以提供较高吞吐量。而LinkedBlockingQueue则是基于锁，并提供了BlockingQueue的等待性方法。

java.util.concurrent包提供的容器Queue、List、Set、Map，从命名上可以大概区分为Concurrent、CopyOnWrite和Blocking三类，同样是线程安全容器，Concurrent类型没有类似CopyOnWrite之类容器相对较重的修改开销，但是Concurrent只能提供较低的遍历一致性，即所谓的弱一致性，例如，当利用迭代器遍历时，如果容器发生修改，迭代器仍然可以继续进行遍历。与弱一致性对应的，就是同步容器常见的fail-fast，即当检测到容器在遍历过程中发生了修改，则抛出ConcurrentModificationException，不再继续遍历。弱一致性还有其它的体现，例如size等操作的准确性是有限的。

=== Q&A
==== synchronized与ReentrantLock
synchronized是Java内建的同步机制，也称Intrinsic Locking，它提供了互斥的语义和可见性，当一个线程已经获取当前锁时，其他试图获取的线程只能等待或者阻塞在那里。Java 5以前synchronized是仅有的同步手段。

Java 5提供的ReentrantLock，语义与synchronized基本相同。
ReentrantLock能够实现更多细节控制，比如可以控制fairness，或者利用定义条件等。
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html

早期版本的synchronized在很多场景下与ReentrantLock性能相差较大，在后续版本进行了较多改进，在低竞争场景中表现可能优于ReentrantLock。

关于公平性:
指在竞争场景中，当公平性为真时，会倾向于将锁赋予等待时间最久的线程。公平性会引入额外开销，只有确实有公平性需要的时候，才有必要指定。
指定方式: ReentrantLock(boolean fair)

==== synchronized
synchronized代码块是由monitorenter/monitorexit指令实现的，Monitor对象是同步的基本实现单元。

Java 6之前，Monitor的实现完全依靠操作系统内部的互斥锁，需要进行用户态到内核态的切换，是一个重量级操作。

现在JVM提供了三种不同的Monitor实现，即: 偏斜锁(Biased Locking)、轻量级锁和重量级锁。

锁的升级、降级，就是JVM优化synchronized运行的机制，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现:
没有竞争出现时，默认会使用偏斜锁。JVM会利用CAS操作(compare and swap)，在对象头上的Mark Word部分设置线程ID，表示该对象偏向于当前线程，因此并不涉及真正的互斥锁。其假设是基于在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。
如果有另外的线程试图锁定某个已经被偏斜过的对象，JVM就需要撤销(revoke)偏斜锁，并切换到轻量级锁实现。轻量级锁依赖CAS操作Mark Word来试图获取锁，如果重试成功，就使用普通的轻量级锁；否则，进一步升级为重量级锁。

==== 条件变量
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/Condition.html

==== StampedLock
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/StampedLock.html
https://stackoverflow.com/questions/26094200/what-is-stampedlock-in-java

==== ThreadLocal
https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html

==== 死锁
https://github.com/orientye/understand/blob/main/high/concurrency/lock.asc#dead-lock

==== BlockingQueue
是否有界(Bounded、Unbounded)
ArrayBlockingQueue是典型的的有界队列，其内部以final的数组保存数据，数组的大小就决定了队列的边界。
LinkedBlockingQueue容易被误解为无边界，但其实是基于有界的逻辑实现的，如果没有在创建队列时就指定容量，则默认为Integer.MAX_VALUE，变成了无界队列。
SynchronousQueue是一个非常奇葩的队列实现，每个删除操作都要等待插入操作，反之每个插入操作也都要等待删除动作。其队列的容量是0。
PriorityBlockingQueue是无边界的优先队列。
DelayedQueue和LinkedTransferQueue同样是无边界的队列。
对于无边界的队列，put操作永远也不会发生其它BlockingQueue的那种等待情况。

参考: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/BlockingQueue.html

==== AbstractQueuedSynchronizer
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/AbstractQueuedSynchronizer.html

AbstractQueuedSynchronizer(AQS)是Java并发包中实现各种同步结构与部分其它组成单元(如线程池中的Worker)的基础。
AbstractQueuedSynchronizer(AQS) provides common underlying functionality:
- Expressed in terms of acquire/release operations
    Implements a concrete synch scheme
- Structured using a variant of GoF template-method pattern
    Synchronizer classes define only the code expressing rules for when it is permitted to acquire and release.
- Doesn't try to work for all possible synchronizers, but enough to be both efficient and widely useful
    Phasers, Exchangers don't use AQS

参考: http://www.sti.uniurb.it/events/sfm15mp/slides/lea.2.pdf[The Design and Engineering of Concurrency Libraries]

实现:
https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/concurrent/locks/AbstractQueuedSynchronizer.java

AQS内部数据和方法，可以简单拆分为:
状态，private volatile int state;
一个等待线程队列，以实现多线程间竞争和等待，这是AQS机制的核心之一;
各种基于CAS的基础操作方法，以及各种期望具体同步结构去实现的acquire/release方法。

==== servlet
https://stackoverflow.com/questions/3106452/how-do-servlets-work-instantiation-sessions-shared-variables-and-multithreadi

=== 最佳实践

=== 参考
《Java Concurrency in Practice》

== JVM
=== 类文件结构
class文件魔数: 0xCAFEBABE

=== 虚拟机类加载机制
加载阶段将字节码数据从不同的数据源读取到JVM中，并映射为JVM认可的数据结构(Class 对象)，这里的数据源可能是各种各样的形态，如jar文件、class文件，甚至是网络数据源等；如果输入数据不是ClassFile的结构，则会抛出ClassFormatError。加载阶段是用户参与的阶段，可以自定义类加载器，去实现自己的类加载过程。

链接是把原始的类定义信息平滑地转化入JVM运行的过程中。可进一步细分为三个步骤:验证(Verification)，防止恶意信息或者不合规的信息危害JVM的运行，验证阶段有可能触发更多class的加载。准备(Preparation)，创建类或接口中的静态变量，并初始化静态变量的初始值。但这里的"初始化"和下面的显式初始化阶段是有区别的，侧重点在于分配所需要的内存空间，不会去执行更进一步的JVM指令。解析(Resolution)，在这一步会将常量池中的符号引用(symbolic reference)替换为直接引用。

初始化真正去执行类初始化的代码逻辑，包括静态字段赋值的动作，以及执行类定义中的静态初始化块内的逻辑，编译器在编译阶段就会把这部分逻辑整理好，父类型的初始化逻辑优先于当前类型的逻辑。

https://docs.oracle.com/javase/specs/jvms/se21/html/jvms-5.html

双亲委派机制: Parents Delegation Model
JDK8:
启动类加载器(Bootstrap Class-Loader)，加载jre/lib下面的jar文件，如rt.jar
扩展类加载器(Extension or Ext Class-Loader)，加载jre/lib/ext/目录下面的jar包
应用类加载器(Application or App Class-Loader)，加载classpath的内容
JDK9:
由于Jigsaw项目引入了Java平台模块化系统(JPMS)，Java SE的源代码被划分为一系列模块，类加载器等都发生了非常大的变化。

=== 虚拟机字节码执行引擎

=== 前端编译与优化
==== 概念
前端编译器: JDK的Javac、Eclipse JDT中的增量式编译器(ECJ)
即时编译器: HotSpot虚拟机的C1、C2编译器，Graal编译器
提前编译器: JDK的Jaotc、GNU Compiler for the Java(GCJ)、Excelsior JET

==== Javac编译器
在JDK 6以前，Javac并不属于标准Java SE API的一部分，其实现单独存放在tools.jar中。
JDK 6通过了JSR 199编译器API的提案，使得Javac编译器的实现代码晋升成为标准Java类库之一，源码就改为放在JDK_SRC_HOME/langtools/src/share/classes/com/sun/tools/javac中。
到了JDK 9时，整个JDK所有的Java类库都采用模块化进行重构划分，Javac编译器就被挪到了jdk.compiler模块(JDK_SRC_HOME/src/jdk.compiler/share/classes/com/sun/tools/javac)里面。

==== 语法糖
- 泛型
Java选择的泛型实现方式叫作类型擦除式泛型(Type Erasure Generics)，而C#选择的泛型实现方式是具现化式泛型(Reified Generics)。
对于C#，List<int>与List<string>是两个不同的类型，它们由系统在运行期生成，有着自己独立的虚方法表和类型数据。而Java语言中的泛型则不同，它只在程序源码中存在，在编译后的字节码文件中，全部泛型都被替换为原来的裸类型(Raw Type)，并在相应的地方插入了强制
转型代码，因此对于运行期的Java语言来说，ArrayList<int>与ArrayList<String>其实是同一个类型。

=== 后端编译与优化
==== 即时编译器
===== 解释器与编译器
HotSpot虚拟机中内置了两个(或三个)即时编译器:
客户端编译器(Client Compiler)和服务端编译器(Server Compiler)，或者简称为C1编译器和C2编译器(部分资料和JDK源码中C2也叫Opto编译器)，第三个是在JDK 10时才出现的、长期目标是代替C2的Graal编译器。

===== 编译对象与触发条件
基于采样的热点探测(Sample Based Hot Spot Code Detection):
    周期性地检查各个线程的调用栈顶
基于计数器的热点探测(Counter Based Hot Spot Code Detection):
    为每个方法甚至是代码块建立计数器

两种探测手段在商用Java虚拟机中都有使用到，譬如J9用过第一种采样热点探测，而HotSpot中使用的是第二种基于计数器的热点探测方法，为了实现热点计数，HotSpot为每个方法准备了两类计数器: 方法调用计数器(Invocation Counter)和回边计数器(Back Edge Counter, 指在循环边界往回跳转)，计数器阈值一旦溢出，就会触发即时编译。

===== 编译过程
在默认条件下，无论是方法调用产生的标准编译请求，还是栈上替换编译请求，虚拟机在编译器还未完成编译之前，都仍然将按照解释方式继续执行代码，而编译动作则在后台的编译线程中进行。
用户可以通过参数-XX：-BackgroundCompilation来禁止后台编译，后台编译被禁止后，当达到触发即时编译的条件时，执行线程向虚拟机提交编译请求以后将会一直阻塞等待，直到编译过程完成再开始执行编译器输出的本地代码。

服务端编译器则是专门面向服务端的典型应用场景，并为服务端的性能配置针对性调整过的编译器，也是一个能容忍很高优化复杂度的高级编译器。

===== 查看分析即时编译结果
-XX: +PrintCompilation
-XX: +PrintInlining
https://wiki.openjdk.org/display/HotSpot/PrintAssembly

==== 提前编译器
提前编译产品和对其的研究有着两条明显的分支:
一条分支是做与传统C、 C++编译器类似的，在程序运行之前把程序代码编译成机器码的静态翻译工作；
另外一条分支是把原本即时编译器在运行时要做的编译工作提前做好并保存下来，下次运行到这些代码(譬如公共库代码在被同一台机器其它Java进程使用)时直接把它加载进来使用。

https://openjdk.org/jeps/295

=== GraalVM
https://www.graalvm.org/latest/docs/

=== 参考
https://docs.oracle.com/javase/specs/

== 性能优化

=== GC优化
GC调优通常关注三个方面: 内存占用(footprint)、延时(latency)和吞吐量(throughput)，大多数情况下调优会侧重于其中一个或者两个方面的目标，很少有情况可以兼顾三个不同的角度。也可能需要考虑其它GC相关的场景，例如，OOM/应用启动速度等。

参考:
https://docs.oracle.com/en/java/javase/21/gctuning/garbage-first-garbage-collector-tuning.html
https://tech.meituan.com/2017/12/29/jvm-optimize.html

=== JIT

==== 参考
https://tech.meituan.com/2020/10/22/java-jit-practice-in-meituan.html

=== 参考
《Java性能优化权威指南》

== 热更新
- Javassist(Java Programming Assistant)
https://github.com/jboss-javassist/javassist

- Byte Buddy
https://github.com/raphw/byte-buddy

- java.lang.instrument.Instrumentation redefineClasses()

- 参考
https://www.zhihu.com/question/61040749

== 工具
=== lombok
https://projectlombok.org/
https://projectlombok.org/features/

=== maven
https://maven.apache.org/

- 插件
build-helper-maven-plugin
maven-dependency-plugin

=== Java Agent
https://docs.oracle.com/javase/8/docs/api/java/lang/instrument/package-summary.html

=== 监控与分析
自带命令: jps、jstat、jstack、jmap
visualvm: https://visualvm.github.io/download.html

第三方在线监控工具:
arthas: https://github.com/alibaba/arthas/blob/master/README_CN.md
skywalking: https://github.com/apache/skywalking

jmc:
https://www.oracle.com/java/technologies/jdk-mission-control.html
一般不建议生产系统进行Profiling，大多数是在性能测试阶段进行。当生产系统确实存在这种需求时，建议使用JFR配合JMC来做Profiling，因为它是从Hotspot JVM内部收集底层信息，并经过了大量优化，性能开销非常低，通常低于2%。
https://github.com/openjdk/jmc

jfr(Java Flight Recorder):
https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm

=== 代码分析
https://github.com/pmd/pmd

=== jlink
https://docs.oracle.com/en/java/javase/11/tools/jlink.html

=== 反编译
https://github.com/skylot/jadx

== 实战
=== 日志
https://tech.meituan.com/2022/07/29/tips-for-avoiding-log-blocking-threads.html

=== docker
- 一些问题
对于 Java 来说，Docker 是一个较新的环境，例如，其内存、CPU 等资源限制是通过 CGroup(Control Group)实现的，早期的 JDK 版本(8u131 之前)并不能识别这些限制，进而会导致一些基础问题:如果未配置合适的 JVM 堆和元数据区、直接内存等参数，Java 就有可能试图使用超过容器限制的内存，最终被容器 OOM kill，或者自身发生 OOM。错误判断了可获取的 CPU 资源，例如，Docker 限制了 CPU 的核数，JVM 就可能设置不合适的 GC 并行线程数等。
从应用打包、发布等角度来看，JDK 自身就比较大，生成的镜像就更为臃肿，镜像非常多的时候，镜像的存储等开销就比较明显了。
在微服务、Serverless 等新的架构和场景，Java 自身的大小、内存占用、启动速度，都存在一定局限性，因为 Java 早期的优化大多是针对长时间运行的大型服务器端应用。

- 解决

    尽量升级到新版本
    老版本的JDK:
        明确设置堆、元数据区等内存区域大小，保证 Java 进程的总大小可控
            限制容器内存:
                $ docker run -it --rm --name yourcontainer -p 8080:8080 -m 800M repo/your-java-container:openjdk
            可以额外配置环境变量，直接指定 JVM 堆大小:
                -e JAVA_OPTIONS='-Xmx300m'
        明确配置 GC 和 JIT 并行线程数目，以避免二者占用过多计算资源
            -XX:ParallelGCThreads
            -XX:CICompilerCount
        意外使用Swap可能也是因为 Ergonomics 机制失效导致的，建议:
            明确告知 JVM 系统内存限额:
                -XX:MaxRAM=`cat /sys/fs/cgroup/memory/memory.limit_in_bytes`
            也可以指定 Docker 运行参数，例如:--memory-swappiness=0

    容器镜像大小的问题:
        使用jlink工具定制最小依赖的Java运行环境(since JDK9)，将JDK裁剪为几十M的大小

== 参考
https://github.com/akullpp/awesome-java
https://docs.oracle.com/en/java/javase/index.html
极客时间《Java 核心技术面试精讲》
《深入理解Java虚拟机: JVM高级特性与最佳实践(第3版)》