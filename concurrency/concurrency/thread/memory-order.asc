:toc:
:toclevels: 5
:hardbreaks-option:

=== memory order
==== reorder
in-order: 顺序
out-of-order: 乱序
reorder: 重排
memory barrier: 字面意思是内存栅栏，不准确的粗暴理解是，隔开了在其前面和后面的指令。
memory barrier约束了CPU的行为，同时也约束了编译器的行为，即memory barrier也隐含了compiler barrier语义。

考虑如下情形:
(1) 编译器在编译程序的过程中，对代码会进行调整(其实除了reorder，还会有些invent，remove之类的优化)；
(2) CPU在执行指令的过程中，对指令会进行重排。
编译器重排序: 在将源代码编译成机器码时，编译器会进行优化，调整指令顺序，前提是保证在单线程上下文中的最终结果不变。
CPU指令级重排序: CPU在执行时，动态地调整指令的执行顺序。这是最核心的重排序。由于存在多级缓存，一个CPU核心对数据的修改，在最终写入主内存并被其他核心看到之前，其顺序可能对其他核心来说是被打乱的。
显然，有时候这些优化并不符合预期(第一种情况可能发生编译乱序，第二种情况可能发生执行乱序)，为了防止这两种情况，这就需要compiler barrier和memory barrier。

Q: 单线程会发生指令重排吗(同理，运行在单个CPU core上的多线程)
A: 会，但不会影响结果

Q: 什么情况下肯定不会重排?
处理器必须能正确处理指令依赖情况保证程序能得出正确的执行结果。
例如指令1把地址A中的值加100，指令2把地址A中的值乘以7，指令3把地址B中的值减去60，则指令1和指令2是有依赖的，它们之间的顺序不能重排: (A+100)*7与A*7+100显然不相等，但指令3可以重排到指令1或指令2之前。

==== compile-time memory ordering
https://en.wikipedia.org/wiki/Memory_ordering#Compile-time_memory_ordering
编译器重排序是指编译器在将源代码翻译成目标代码（如汇编或机器码）的过程中，为了优化性能，在不改变程序单线程语义的前提下，重新排列指令的执行顺序。

==== runtime memory ordering
https://en.wikipedia.org/wiki/Memory_ordering#Runtime_memory_ordering

CPU在什么情况下会reorder呢？

对于有前后有依赖的指令，CPU一般不会reorder(Alpha架构除外)。
例如: a = 5; b = a + 1; 这两条指令存在依赖关系，不会被cpu重排顺序。

==== Shared-Variable Shenanigans(共享变量陷阱)
Given code that does plain loads and stores, the compiler is within its rights to assume that the affected variables are neither accessed nor modified by any other thread. This assumption allows the compiler to carry out a large number of transformations, including load tearing, store tearing, load fusing, store fusing, code reordering, invented loads, invented stores, store-to-load transformations, and deadcode elimination, all of which work just fine in singlethreaded code. But concurrent code can be broken by each of these transformations, or shared-variable shenanigans, as described below.
对于执行普通加载和存储操作的代码，编译器有权假设受影响的变量不会被任何其他线程访问或修改。这一假设允许编译器执行大量的代码转换优化，包括：加载拆分、存储拆分、加载融合、存储融合、代码重排、凭空加载、凭空存储、存储到加载转换以及死代码消除。这些转换在单线程代码中都能良好运作。
然而，并发代码可能因上述任何一种转换（或称共享变量陷阱）而失效，具体描述如下。

- Load tearing(加载拆分)
加载拆分是指编译器将单个内存读取操作拆分成多个较小的加载指令。例如，在读取一个多字节变量（如指针或长整型）时，编译器可能会生成多条指令来分别读取其各个字节或字。这种优化在单线程环境下是安全的，但在并发场景中，如果另一个线程在拆分加载的过程中修改了该变量，就可能导致读取到不一致的中间状态（例如部分旧值、部分新值），从而引发数据损坏、野指针或程序崩溃等错误。

- Store tearing(存储拆分)
存储拆分是指编译器将单个内存写入操作拆分成多个较小的存储指令。例如，一个线程可能向一个四字节整数变量存储值 0x12345678，而另一个线程同时存储 0xabcdef00。如果编译器对任一写入操作使用了 16 位的存储指令，结果可能变成 0x1234ef00。这对于从该整数加载数据的代码来说，可能会造成意料之外的混乱。
这并非纯粹的理论问题。例如，有些 CPU 的立即数字段较小，在这种 CPU 上，编译器为了减少在寄存器中显式构造 64 位常量的开销，可能会将一个 64 位的存储拆分成两个 32 位的存储，即使在 64 位 CPU 上也可能如此。
当然，考虑到存在需要在 32 位系统上运行的使用 64 位整数的代码，编译器在某些情况下除了拆分存储之外别无选择。但对于正确对齐的、与机器字长匹配的存储，使用 WRITE_ONCE() 可以防止存储拆分。

- Load fusing(加载融合)
Load fusing occurs when the compiler uses the result of a prior load from a given variable instead of repeating the load.

- Store fusing(存储融合)
Store fusing can occur when the compiler notices a pair of successive stores to a given variable with no intervening loads from that variable. In this case, the
compiler is within its rights to omit the first store.

==== 体系结构的差异性
对于没有前后依赖关系的指令，CPU就有可能对这些指令进行重排(除非使用memory barrier进行一些显示控制)，具体的力度则与CPU体系结构相关:

    x86是一种strong order(也叫TSO，total store order):
        同一CPU执行的load指令后接load指令(L-L)，store指令后接store指令(S-S)，load指令后接store指令(L-S):
            均不能交换指令的执行顺序
        仅store指令后接load指令(S-L)才可以

    ARM则是一种weak order:
        只要没有依赖关系，load指令和store指令就可任意交换。

==== Why Memory Barriers
memory barriers are a necessary evil that is required to enable good performance and scalability, an evil that stems from the fact that CPUs are orders of magnitude faster than are both the interconnects between them and the memory they are attempting to access.
- 《perfbook》 Appendix C Unknown Why Memory Barriers?

===== Cache Structure
https://github.com/orientye/understand/blob/main/co-ca/cpu.asc#cache-structure

===== Cache-Coherence Protocols
https://github.com/orientye/understand/blob/main/co-ca/cpu.asc#cache-coherence-protocols

===== Stores Result in Unnecessary Stalls
====== 背景
its performance for the first write to a given cache line is quite poor.
使无效与使无效应答之间存在延迟。

====== Store Buffers
One way to prevent this unnecessary stalling of writes is to add “store buffers” between each CPU and its cache.

Q: But then why do uniprocessors also have store buffers?
A: Because the purpose of store buffers is not just to hide acknowledgement latencies in multiprocessor cache coherence protocols, but to hide memory latencies in
general. Because memory is much slower than is cache on uniprocessors, store buffers on uniprocessors can help to hide write-miss memory latencies.
问: 那为何单处理器系统也需要存储缓冲区？
答: 因为存储缓冲区的作用不仅限于掩盖多处理器缓存一致性协议中的确认延迟，更在于普遍掩盖内存访问延迟。在单处理器系统中，由于内存速度远低于缓存速度，存储缓冲区有助于掩盖写入未命中时的内存访问延迟。

Q: So store-buffer entries are variable length? Isn’t that difficult to implement in hardware?
A: Here are two ways for hardware to easily handle variable length stores.
First, each store-buffer entry could be a single byte wide. Then an 64-bit store would consume eight store-buffer entries. This approach is simple and flexible, but one disadvantage is that each entry would need to replicate much of the address that was stored to.
Second, each store-buffer entry could be double the size of a cache line, with half of the bits containing the values stored, and the other half indicating which bits had been stored to. So, assuming a 32-bit cache line, a single-byte store of 0x5a to the low-order byte of a given cache line would result in 0xXXXXXX5a for the first half and 0x000000ff for the second half, where the values labeled X are arbitrary because they would be ignored. This approach allows multiple consecutive stores corresponding to a given cache line to be merged into a single store-buffer entry, but is space-inefficient for random stores of single bytes.
Much more complex and efficient schemes are of course used by actual hardware designers.
问：那么存储缓冲区的条目是可变长度的？这在硬件上实现起来不是很难吗？
答：硬件可以通过以下两种方式轻松处理可变长度的存储操作：
首先，每个存储缓冲区条目可以设计为单字节宽度。这样，一次64位存储操作会占用八个存储缓冲区条目。这种方法简单灵活，但缺点在于每个条目都需要重复存储大量与目标地址相关的信息。
其次，每个存储缓冲区条目可以设为缓存行大小的两倍：其中一半比特位用于存储具体数值，另一半则用作位掩码来标识哪些位已被写入。假设采用32位缓存行，当向某缓存线低字节存储单字节数据0x5a时，前半部分会记录为0xXXXXXX5a，后半部分位掩码则记为0x000000ff（标有X的数值可任意，因为它们会被忽略）。这种方法允许将针对同一缓存行的多个连续存储操作合并到单个缓冲区条目中，但对于随机单字节存储操作则存在空间效率低下的问题。
实际硬件设计人员当然会采用更为复杂高效的实现方案。

These store buffers are local to a given CPU or, on systems with hardware multi threading, local to a given core.

====== Store Forwarding
Breaking this guarantee is violently counter-intuitive to software types, so much so that the hardware guys took pity and implemented “store forwarding”, where each CPU refers to (or “snoops”) its store buffer as well as its cache when performing loads, as shown in Figure C.6. In other words, a given CPU’s stores are directly forwarded to its subsequent loads, without having to pass through the cache.

当CPU执行一个读取操作时，它需要从内存中获取数据。但如果在它之前刚刚执行了一个写入操作到同一个内存地址，那么这个最新的数据可能还在CPU的存储缓冲区中，还没来得及被写入到缓存/内存里。
存储转发/前向存储机制就是：在这种情况下，CPU会绕过缓存，直接从自己的存储缓冲区中把这个“尚未提交”的数据转发给后续的读取操作。

作用范围：存储转发通常只在同一个CPU核心内部有效。一个核心的存储操作不能被转发到另一个核心。
存储缓冲区：这是实现该技术的关键硬件结构，用于临时存放已执行但尚未提交到缓存层的写入数据。
存储转发停顿：虽然存储转发是优化，但在某些复杂情况下它也可能导致性能下降。例如，当一次读取操作的部分数据来自存储缓冲区，而另一部分数据需要从缓存中获取时（比如读写对象地址没对齐），CPU可能不得不等待存储操作完全提交到缓存后，才能从缓存执行一个完整的读取，这个过程被称为“存储转发停顿”。

====== Store Buffers and Memory Barriers
[source, c]
----
// variables “a” and “b” initially zero
void foo(void)
{
    a=1;
    smp_mb();
    b=1;
}

void bar(void)
{
    while (b == 0) continue;
    assert(a == 1);
}
----

The memory barrier smp_mb() will cause the CPU to flush its store buffer before applying each subsequent store to its variable’s cache line. The CPU could either simply stall until the store buffer was empty before proceeding, or it could use the store buffer to hold subsequent stores until all of the prior entries in the store buffer had been applied.

===== Store Sequences Result in Unnecessary Stalls
====== 背景
Unfortunately, each store buffer must be relatively small, which means that a CPU executing a modest sequence of stores can fill its store buffer (for example, if all of them result in cache misses). At that point, the CPU must once again wait for invalidations to complete in order to drain its store buffer before it can continue executing. This same situation can arise immediately after a memory barrier, when all subsequent store instructions must wait for invalidations to complete, regardless of whether or not these stores result in cache misses.
遗憾的是，每个存储缓冲区的容量都相对有限，这意味着即使CPU仅执行数量适中的存储指令序列，也可能将其存储缓冲区填满（例如，如果所有这些存储操作均导致缓存未命中）。此时，CPU将不得不再次等待无效化操作完成，以便清空其存储缓冲区，之后才能继续执行后续指令。同样的情况会立即发生在内存屏障之后——届时所有后续存储指令都必须等待无效化操作完成，无论这些存储操作是否会引起缓存未命中。
This situation can be improved by making invalidate acknowledge messages arrive more quickly. One way of accomplishing this is to use per-CPU queues of invalidate messages, or “invalidate queues”.

====== Invalidate Queues
invalidate acknowledge不能尽快回复的主要原因:
One reason that invalidate acknowledge messages can take so long is that they must ensure that the corresponding cache line is actually invalidated, and this invalidation can be delayed if the cache is busy, for example, if the CPU is intensively loading and storing data, all of which resides in the cache. In addition, if a large number of invalidate messages arrive in a short time period, a given CPU might fall behind in processing them, thus possibly stalling all the other CPUs.

However, the CPU need not actually invalidate the cache line before sending the acknowledgement. It could instead queue the invalidate message with the understanding that the message will be processed before the CPU sends any further messages regarding that cache line.
但事实上，CPU其实不需要完成invalidate操作就可以回送acknowledgement消息。该CPU完全可以将无效化消息暂存于队列中，其前提条件是必须保证在后续发送涉及该缓存行的任何相关消息之前，这些暂存的invalidate消息都已被处理完毕。

====== Invalidate Queues and Invalidate Acknowledge
A CPU with an invalidate queue may acknowledge an invalidate message as soon as it is placed in the queue, instead of having to wait until the corresponding line is actually invalidated. Of course, the CPU must refer to its invalidate queue when preparing to transmit invalidation messages—if an entry for the corresponding cache line is in the invalidate queue, the CPU cannot immediately transmit the invalidate message; it must instead wait until the invalidate-queue entry has been processed.
当然，在准备发送无效化消息时，CPU 必须查询自身的无效化队列——如果队列中已存在针对该缓存行的条目，则 CPU 不能立即发送无效化消息，而必须等待队列中的对应条目被处理完毕。

Placing an entry into the invalidate queue is essentially a promise by the CPU to process that entry before transmitting any MESI protocol messages regarding that cache line. As long as the corresponding data structures are not highly contended, the CPU will rarely be inconvenienced by such a promise.
However, the fact that invalidate messages can be buffered in the invalidate queue provides additional oppor
tunity for memory-misordering, as discussed in the next section.
将条目加入无效化队列，本质上是 CPU 做出的一项承诺：在发送任何涉及该缓存行的 MESI 协议消息之前，必须先处理该队列条目。只要对应的数据结构没有处于高度竞争状态，这种承诺通常不会对 CPU 造成性能影响。
然而，正如下一节将要讨论的，无效化消息能在无效化队列中缓冲的特性，为内存乱序创造了新的可能性。

====== Invalidate Queues and Memory Barriers
Let us suppose that CPUs queue invalidation requests, but respond to them immediately. This approach minimizes the cache-invalidation latency seen by CPUs doing stores, but can defeat memory barriers, as seen in the following example.
[source, c]
----
// variables “a” and “b” initially zero
// “a” is replicated read-only (MESI “shared” state)
// “b” is owned by CPU 0 (MESI “exclusive” or “modified” state)
void foo(void) // cpu0 execute
{
    a=1;
    smp_mb();
    b=1;
}

void bar(void) // cpu1 execute
{
    while (b == 0) continue;
    assert(a == 1);
}
----

Then the sequence of operations might be as follows:

(1) CPU 0 executes a = 1. The corresponding cache line is read-only in CPU 0’s cache, so CPU 0 places the new value of “a” in its store buffer and transmits an “invalidate” message in order to flush the corresponding cache line from CPU 1’s cache.
CPU 0 执行 a = 1。对应的缓存行在 CPU 0 的缓存中处于只读状态，因此 CPU 0 将“a”的新值放入store buffer，并发送一个“invalidate即使无效”消息，以便从 CPU 1 的缓存中清除对应的缓存行。

(2) CPU 1 executes while (b == 0)continue, but the cache line containing “b” is not in its cache. It therefore transmits a “read” message.
CPU 1 执行 while (b == 0) continue，但包含“b”的缓存行不在其缓存中。因此它发送一个“读”消息。

(3) CPU 1 receives CPU 0’s “invalidate” message, queues it, and immediately responds to it.
CPU 1 收到 CPU 0 的“无效”消息，将其加入队列，并立即回复/响应该消息。

(4) CPU 0 receives the response from CPU 1, and is therefore free to proceed past the smp_mb() on line 4 above, moving the value of “a” from its store buffer to its cache line.
CPU 0 收到来自 CPU 1 的回复，因此可以继续执行第 4 行的 smp_mb() 之后的指令，将“a”的值从其存储缓冲区移动到其缓存行中。

(5) CPU 0 executes b = 1. It already owns this cache line (in other words, the cache line is already in either the “modified” or the “exclusive” state), so it stores the new value of “b” in its cache line.
CPU 0 执行 b = 1。它已经拥有该缓存行（即该缓存行已处于“修改”或“独占”状态），因此它将“b”的新值存储到其缓存行中。

(6) CPU 0 receives the “read” message, and transmits the cache line containing the now-updated value of “b” to CPU 1, also marking the line as “shared” in its own cache.
CPU 0 收到“读”消息，并将包含已更新值“b”的缓存行发送给 CPU 1，并标记该cacheline为shared状态。

(7) CPU 1 receives the cache line containing “b” and installs it in its cache.
CPU 1 收到包含“b”的缓存行，并将其将其应用到本地缓存。

(8) CPU 1 can now finish executing while (b ==0) continue, and since it finds that the value of “b” is 1, it proceeds to the next statement.
CPU 1 现在可以完成 while (b == 0) continue 的执行，由于它发现“b”的值为 1，于是继续执行下一条语句。

(9) CPU 1 executes the assert(a == 1), and, since the old value of “a” is still in CPU 1’s cache, this assertion fails.
CPU 1 执行 assert(a == 1)，由于“a”的旧值仍在 CPU 1 的缓存中，该断言失败。

(10) Despite the assertion failure, CPU 1 processes the queued “invalidate” message, and (tardily) invalidates the cache line containing “a” from its own cache.
尽管断言失败，CPU 1 仍处理队列中的“使无效”消息，并（延迟地）从其自己的缓存中使包含“a”的缓存行无效。

There is clearly not much point in accelerating invalidation responses if doing so causes memory barriers to effectively be ignored. However, the memory-barrier instructions can interact with the invalidate queue, so that when a given CPU executes a memory barrier, it marks all the entries currently in its invalidate queue, and forces any subsequent load to wait until all marked entries have been applied to the CPU’s cache.
如果加速无效响应会导致内存屏障失效，那么这样做显然没有意义。然而，内存屏障指令可以与无效队列交互，使得当某个 CPU 执行内存屏障时，它会标记当前在其无效队列中的所有条目，这些被标注的项次被称为marked entries，而随后CPU执行的任何的load操作都需要等到Invalidate Queue中所有marked entries完成对cacheline的操作之后才能进行。

因此，要想保证程序逻辑正确，需要给bar函数增加内存屏障的操作，具体如下:
[source, c]
----
// variables “a” and “b” initially zero
// “a” is replicated read-only (MESI “shared” state)
// “b” is owned by CPU 0 (MESI “exclusive” or “modified” state)
void foo(void) // cpu0 execute
{
    a=1;
    smp_mb();
    b=1;
}

void bar(void) // cpu1 execute
{
    while (b == 0) continue;
    smp_mb();
    assert(a == 1);
}
----
同上(bar函数没有加内存屏障)，前面7个步骤是一样的，到了第8步，执行开始不同了:

(8) CPU 1 can now finish executing while (b ==0) continue, and since it finds that the value of “b” is 1, it proceeds to the next statement, which is now a memory barrier.
CPU 1 此时可完成执行 while (b ==0) continue 循环，由于检测到"b"的值为1，便继续执行下一语句，此时遇到内存屏障。

(9) CPU 1 must now stall until it processes all preexisting messages in its invalidation queue.
CPU 1 必须暂停执行，直至其处理完失效队列中的所有待处理消息。

(10) CPU 1 now processes the queued “invalidate” message, and invalidates the cache line containing “a” from its own cache.
CPU 1 开始处理队列中的"失效"消息，将包含"a"的缓存行从自身缓存中置为无效。

(11) CPU 1 executes the assert(a == 1), and, since the cache line containing “a” is no longer in CPU 1’s cache, it transmits a “read” message.
CPU 1 执行 assert(a == 1) 断言，由于包含"a"的缓存行已不在其缓存中，遂发送"读"消息。

(12) CPU 0 responds to this “read” message with the cache line containing the new value of “a”.
CPU 0 响应此"读取"消息，传回包含新值"a"的缓存行。

(13) CPU 1 receives this cache line, which contains a value of 1 for “a”, so that the assertion does not trigger.
CPU 1 接收到该缓存行，其中"a"的值为1，因此断言未触发。

===== Read and Write Memory Barriers
In the previous section, memory barriers were used to mark entries in both the store buffer and the invalidate queue. But in our code fragment, foo() had no reason to
do anything with the invalidate queue, and bar() similarly had no reason to do anything with the store buffer. Many CPU architectures therefore provide weaker memory-barrier instructions that do only one or the other of these two. Roughly speaking, a “read memory barrier” marks only the invalidate queue (and snoops entries in the
store buffer) and a “write memory barrier” marks only the store buffer, while a full-fledged memory barrier does all of the above.

The software-visible effect of these hardware mechanisms is that a read memory barrier orders only loads on the CPU that executes it, so that all loads preceding the read memory barrier will appear to have completed before any load following the read memory barrier. Similarly, a write memory barrier orders only stores, again on the CPU that executes it, and again so that all stores preceding the write memory barrier will appear to have completed before any store following the write memory barrier. A full-fledged memory barrier orders both loads and stores, but again only on the CPU executing the memory barrier.

改进后的foo()和bar():
[source, c]
----
void foo(void)
{
    a=1;
    smp_wmb();
    b=1;
}

void bar(void)
{
    while (b == 0) continue;
    smp_rmb();
    assert(a == 1);
}
----
Some computers have even more flavors of memory barriers, but understanding these three variants will provide a good introduction to memory barriers in general.

===== 存储缓冲区与失效队列
- Store Buffer(存储缓冲区/写缓冲区)
是CPU核心内部的一个小型、高速的硬件队列，用于临时存放CPU核心想要写入到缓存（Cache）中的数据。
https://developer.arm.com/documentation/ddi0489/f/memory-system/l1-caches/store-buffer
https://community.intel.com/t5/Software-Tuning-Performance/Purpose-of-Load-Buffer-in-x86/td-p/1091736

- Invalidate Queue(失效队列/无效化队列)

特性	失效队列（无效化队列/Invalidate Queue）	存储缓冲区（写缓冲区/Store Buffer）
解决的问题	优化 “读” 操作，加速对缓存失效确认的响应。	优化 “写” 操作，让CPU不必等待写入完成。
工作原理	快速接收并确认其他CPU发来的“失效”消息，将其排队，稍后再处理。	CPU将“写”指令的结果先暂存于此，然后继续执行，由后台完成写入缓存。
位于何处	CPU的缓存控制器中（更靠近缓存一致性协议逻辑）。	CPU核心与缓存之间。
主要目的	隐藏读延迟，避免CPU因等待缓存行失效而停滞。	隐藏写延迟，实现写操作的非阻塞。
可能引起的问题	读取旧数据：CPU可能从自己已失效（但还在队列中）的缓存行里读取到旧数据。	看到最新写入：其他CPU可能看不到本CPU刚刚写入的值（因为还在缓冲区里）。
需要何种内存屏障	读内存屏障 确保在处理新的读操作前，先清空失效队列。	写内存屏障 确保在处理新的写操作前，先清空存储缓冲区。

Q: 存储缓冲区与失效队列有多大
大致大小	约 10 到 64 条目	约 8 到 32 条目
确定性信息	无法公开获得，是核心商业机密。	无法公开获得，是核心商业机密。
对程序员的意义
1. 理解其存在和有限性。	1. 理解其存在和有限性。
2. 明白它是写操作乱序的主要来源。	2. 明白它是读操作看到旧数据的主要来源。
3. 知道写内存屏障会作用于它。	3. 知道读内存屏障会作用于它。

Q: 满了怎么办？
CPU核心会被强制“停滞”，直到有空闲条目可用。这被称为 “流水线停滞” 或 “内存顺序停滞” ，是性能杀手。

Q: 具体来说，存储缓冲区如果满了怎么办？
场景: CPU核心正在疯狂地执行存储指令（比如，在一个紧凑的循环中修改多个变量）。
核心只能空转，等待存储缓冲区出现空位；等待期间，存储缓冲区会持续工作。

Q: 具体来说，失效队列如果满了怎么办？
场景: 系统中有多个CPU核心正在频繁地修改共享数据，导致某个核心收到了大量的“失效”消息。
必须阻塞发送方，它会延迟对新的“失效”消息的确认，直到它能在失效队列中腾出空间来处理这个新请求；为了腾出空间，缓存控制器必须加速处理队列中现有的失效条目-即真正地查找并无效化本地缓存中对应的缓存行。

===== Example Memory-Barrier Sequences
====== Ordering-Hostile Architecture
let us insteaddesign a mythical but maximally memory-ordering-hostile computer architecture.
This hardware must obey the following ordering constraints [McK05a, McK05b]:
1. Each CPU will always perceive its own memory accesses as occurring in program order.
2. CPUs will reorder a given operation with a store only if the two operations are referencing different locations.
3. All of a given CPU’s loads preceding a read memory barrier (smp_rmb()) will be perceived by all CPUs to precede any loads following that read memory barrier.
4. All of a given CPU’s stores preceding a write memory barrier (smp_wmb()) will be perceived by all CPUs to precede any stores following that write memory barrier.
5. All of a given CPU’s accesses (loads and stores) preceding a full memory barrier (smp_mb()) will be perceived by all CPUs to precede any accesses following that memory barrier.
我们来设计一个虚构的、在内存排序上极度不友好的计算机架构。
该硬件必须遵守以下排序约束:
1. 对于每个CPU而言，从它自己的角度看，其内存访问的顺序总是符合program order的。
2. 仅当两个操作访问不同地址时，CPU 才会对某个操作与一次存储进行重排序。
3. 在某个 CPU 中，位于读内存屏障（smp_rmb()）之前的所有加载操作，在所有 CPU 看来，都必须先于该屏障之后的任何加载操作。
4. 在某个 CPU 中，位于写内存屏障（smp_wmb()）之前的所有存储操作，在所有 CPU 看来，都必须先于该屏障之后的任何存储操作。
5. 在某个 CPU 中，位于全内存屏障（smp_mb()）之前的所有访问（加载和存储），在所有 CPU 看来，都必须先于该屏障之后的任何访问。

Q: Does the guarantee that each CPU sees its own memory accesses in order also guarantee that each user-level thread will see its own memory accesses in order? Why or why not?
A: No. Consider the case where a thread migrates from one CPU to another, and where the destination CPU perceives the source CPU’s recent memory operations out of order.
To preserve user-mode sanity, kernel hackers must use memory barriers in the context-switch path. However, the locking already required to safely do a context switch
should automatically provide the memory barriers needed to cause the user-level task to see its own accesses in order. That said, if you are designing a super-optimized
scheduler, either in the kernel or at user level, please keep this scenario in mind!
问题：保证每个CPU按顺序看到自己的内存访问，是否也保证每个用户级线程按顺序看到自己的内存访问？
为什么能或为什么不能？
答：不能。考虑这样一种情况：一个线程从一个CPU迁移到另一个CPU，而目标CPU可能以乱序的方式感知源CPU最近的内存操作。
为保证用户态程序的正常运行，内核开发者人员必须在上下文切换路径中使用内存屏障。然而，安全执行上下文切换所需的锁定操作通常已经自动提供了所需的内存屏障，从而使用户级任务能够按顺序看到自己的内存访问。尽管如此，如果您正在设计一个超级优化的调度器（无论是在内核层还是用户层），请务必牢记这种情况！

===== Are Memory Barriers Forever
There have been a number of recent systems that are significantly less aggressive about out-of-order execution in general and re-ordering memory references in particular.
Will this trend continue to the point where memory barriers are a thing of the past?
近期出现了不少新系统，它们在乱序执行（尤其是内存访问重排序）方面明显不再那么激进。这一趋势是否会持续到内存屏障彻底退出历史舞台的程度？

===== Advice to Hardware Designers
1. I/O devices that ignore cache coherence
2. External busses that fail to transmit cache-coherence data
3. Device interrupts that ignore cache coherence
4. Inter-processor interrupts (IPIs) that ignore cache coherence
5. Context switches that get ahead of cache coherence
6. Overly kind simulators and emulators

===== 参考
perfbook C2
《Parallel Computer Architecture A Hardware / Software Approach》
《A Primer on Memory Consistency and Cache Coherence》2nd

==== Q&A
Q: memory order vs. cache coherence
https://course.ece.cmu.edu/~ece847c/S15/lib/exe/fetch.php?media=part2_2_sorin12.pdf

Q: memory order vs. atomic
https://stackoverflow.com/questions/15056237/which-is-more-efficient-basic-mutex-lock-or-atomic-integer

==== 可见性问题

==== cpu视角
x86: https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html

chapter8 multi-processor management
8.2 memory ordering
8.2.5 strengthening or weakening the memory-order model:

• SFENCE — Serializes all store (write) operations that occurred prior to the SFENCE instruction in the program instruction stream, but does not affect load operations.

• LFENCE — Serializes all load (read) operations that occurred prior to the LFENCE instruction in the program instruction stream, but does not affect store operations.

• MFENCE — Serializes all store and load operations that occurred prior to the MFENCE instruction in the program instruction stream.

==== kernel视角
https://github.com/orientye/understanding-the-linux-kernel/blob/main/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Linux%E5%86%85%E6%A0%B8/%E8%BF%9B%E7%A8%8B/%E5%90%8C%E6%AD%A5.asc#barrier[linux-kernel-barrier]

==== gcc atomic operations
https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html
https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html

==== llvm atomics
https://llvm.org/docs/Atomics.html

==== c++

===== 概念
https://en.cppreference.com/w/cpp/atomic/memory_order

- Sequenced-before

- Carries dependency

- Modification order

- Release sequence

- Dependency-ordered before

- Inter-thread happens-before

- Happens-before

- Visible side-effects

- Consume operation

- Acquire opertation

- Release operation

- Synchronizes-with

===== 本质
本质上两个问题:

    【1】______不可以重排到______的前面/后面
    【2】______对谁______可见

===== Q&A
Q: 如果变量是函数的参数呢？

===== relaxed
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Relaxed_ordering

===== release acquire
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Release-Acquire_ordering

windows:
https://learn.microsoft.com/en-us/windows-hardware/drivers/kernel/acquire-and-release-semantics
https://learn.microsoft.com/en-us/windows/win32/dxtecharts/lockless-programming

https://davekilian.com/acquire-release.html
A write-release guarantees that all preceding code completes before the releasing write
A read-acquire guarantees that all following code starts after the acquiring read

acquire: This is a read-acquire; block upcoming operations until this is done (but allow preceding operations to be delayed if it’s convenient).
release: This is a write-release: wait for all preceding operations to complete before doing this (but work ahead on stuff following this if it’s convenient).
acquire: 读 - 获取语义；阻止后续操作执行，直到该读取完成（但允许之前的操作根据情况延迟执行）。
release: 写 - 释放语义；等待所有之前的操作完成后再执行该写入（但允许提前处理之后的操作）。

https://preshing.com/20120913/acquire-and-release-semantics/
Acquire semantics is a property that can only apply to operations that read from shared memory, whether they are read-modify-write operations or plain loads. The operation is then considered a read-acquire. Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order.

Release semantics is a property that can only apply to operations that write to shared memory, whether they are read-modify-write operations or plain stores. The operation is then considered a write-release. Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order.

Fence Semantics
A fence semantics combines both acquire and release semantics behavior.

Q: memory_order_release 与 smp_wmb()
Q: memory_order_acquire 与 smp_rmb()
Q: memory_order_seq_cst 与 smp_mb()

===== release consume
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Release-Consume_ordering
release-consume 被废弃的根本原因是其设计复杂性（对编译器和程序员而言）与其实践中的实际收益（性能优势）严重不匹配。它为一个在大多数硬件上难以实现的、微小的优化，引入了巨大的正确性风险和维护负担。最终，C++ 标准委员会选择了安全和简单，而不是极致的、脆弱的性能。

https://preshing.com/20141124/fixing-gccs-implementation-of-memory_order_consume/

===== sequentially consistent

===== thread_fence
std::atomic_thread_fence

extern "C" void atomic_thread_fence( std::memory_order order ) noexcept;

Establishes memory synchronization ordering of non-atomic and relaxed atomic accesses, as instructed by order, without an associated atomic operation.

===== implement
以memory_order_seq_cst为例:
Ubuntu(g++, VM, Intel i5 4 core):    汇编指令mfence
Mac OSX(g++,  Intel i5 2 core):      汇编指令xchgl(= lock xchgl)
Win10(VS2015, Intel i5 4 core):      _ReadWriteBarrier, _InterlockedExchange

is_lock_free

===== 应用示例
https://github.com/facebook/rocksdb/blob/master/memtable/skiplist.h
https://github.com/apache/incubator-brpc/blob/master/src/bthread/work_stealing_queue.h

===== 参考
https://en.cppreference.com/w/cpp/language/memory_model

==== java
https://docs.oracle.com/javase/specs/jls/se21/html/jls-17.html#jls-17.4

https://github.com/orientye/understand/blob/main/lan/java.asc#Java-Memory-Model-and-Thread

==== correctness
link:../lock-free.asc#correctness[correctness]

==== 参考
https://github.com/MattPD/cpplinks/blob/master/atomics.lockfree.memory_model.md
https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/
perfbook: Chapter 15 Advanced Synchronization: Memory Ordering
https://www.hboehm.info/c++mm/
