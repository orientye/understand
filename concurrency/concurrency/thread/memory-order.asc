:toc:
:toclevels: 5
:hardbreaks-option:

=== memory order
==== Shared-Variable Shenanigans(共享变量陷阱)
Given code that does plain loads and stores, the compiler is within its rights to assume that the affected variables are neither accessed nor modified by any other thread. This assumption allows the compiler to carry out a large number of transformations, including load tearing, store tearing, load fusing, store fusing, code reordering, invented loads, invented stores, store-to-load transformations, and deadcode elimination, all of which work just fine in singlethreaded code. But concurrent code can be broken by each of these transformations, or shared-variable shenanigans, as described below.
对于执行普通加载和存储操作的代码，编译器有权假设受影响的变量不会被任何其他线程访问或修改。这一假设允许编译器执行大量的代码转换优化，包括：加载拆分、存储拆分、加载融合、存储融合、代码重排、凭空加载、凭空存储、存储到加载转换以及死代码消除。这些转换在单线程代码中都能良好运作。
然而，并发代码可能因上述任何一种转换（或称共享变量陷阱）而失效，具体描述如下:

- Load tearing(加载拆分)
加载拆分是指编译器将单个内存读取操作拆分成多个较小的加载指令。例如，在读取一个多字节变量（如指针或长整型）时，编译器可能会生成多条指令来分别读取其各个字节或字。这种优化在单线程环境下是安全的，但在并发场景中，如果另一个线程在拆分加载的过程中修改了该变量，就可能导致读取到不一致的中间状态（例如部分旧值、部分新值），从而引发数据损坏、野指针或程序崩溃等错误。

- Store tearing(存储拆分)
存储拆分是指编译器将单个内存写入操作拆分成多个较小的存储指令。例如，一个线程可能向一个四字节整数变量存储值 0x12345678，而另一个线程同时存储 0xabcdef00。如果编译器对任一写入操作使用了 16 位的存储指令，结果可能变成 0x1234ef00。这对于从该整数加载数据的代码来说，可能会造成意料之外的混乱。
这并非纯粹的理论问题。例如，有些 CPU 的立即数字段较小，在这种 CPU 上，编译器为了减少在寄存器中显式构造 64 位常量的开销，可能会将一个 64 位的存储拆分成两个 32 位的存储，即使在 64 位 CPU 上也可能如此。
当然，考虑到存在需要在 32 位系统上运行的使用 64 位整数的代码，编译器在某些情况下除了拆分存储之外别无选择。但对于正确对齐的、与机器字长匹配的存储，使用 WRITE_ONCE() 可以防止存储拆分。

- Load fusing(加载融合)
Load fusing occurs when the compiler uses the result of a prior load from a given variable instead of repeating the load.

- Store fusing(存储融合)
Store fusing can occur when the compiler notices a pair of successive stores to a given variable with no intervening loads from that variable. In this case, the compiler is within its rights to omit the first store.

- Code reordering(代码重排)
Code reordering is a common compilation technique used to combine common subexpressions, reduce register pressure, and improve utilization of the many functional units available on modern superscalar microprocessors.

- Invented loads(凭空加载)
凭空加载是指编译器为了优化，在源代码指定位置之前，创建了本不存在的内存读取指令。

- Invented stores(凭空存储)
凭空存储是指编译器在优化过程中，在源代码并未明确要求的位置，插入额外的内存写入操作。

- Store-to-load transformations(存储到加载的转换)
编译器将"可能无用的存储"转换为"检查+条件存储"

- Dead-code elimination(死代码消除)
Dead-code elimination can occur when the compiler notices that the value from a load is never used, or when a variable is stored to, but never loaded from.

==== reorder
in-order: 顺序
out-of-order: 乱序
reorder: 重排
memory barrier: 内存栅栏/内存屏障
memory barrier 约束了 CPU 的行为，同时也约束了编译器的行为，即 memory barrier 也隐含了 compiler barrier 语义。

代码顺序 ≠ 实际执行顺序 ≠ 其他线程观察到的顺序

考虑如下情形:
(1) 编译器在编译程序的过程中，对代码会进行调整(其实除了reorder，还会有些invent，remove之类的优化)；
(2) CPU在执行指令的过程中，对指令会进行重排。

编译器重排序:
compile-time memory ordering
在将源代码编译成机器码时，编译器会进行优化，调整指令顺序，前提是保证在单线程上下文中的最终结果不变。
https://en.wikipedia.org/wiki/Memory_ordering#Compile-time_memory_ordering
编译器重排序是指编译器在将源代码翻译成目标代码（如汇编或机器码）的过程中，为了优化性能，在不改变程序单线程语义的前提下，重新排列指令的执行顺序。

CPU指令级重排序:
runtime memory ordering
CPU在执行时，动态地调整指令的执行顺序。这是最核心的重排序。由于存在多级缓存，一个CPU核心对数据的修改，在最终写入主内存并被其他核心看到之前，其顺序可能对其他核心来说是被打乱的。
显然，有时候这些优化并不符合预期(第一种情况可能发生编译乱序，第二种情况可能发生执行乱序)，为了防止这两种情况，这就需要 compiler barrier 和 memory barrier 。
https://en.wikipedia.org/wiki/Memory_ordering#Runtime_memory_ordering

Q: 单线程会发生指令重排吗(同理，运行在单个CPU core上的多线程)
A: 会，但不会影响结果

Q: 什么情况下肯定不会重排?

    (1) 有数据依赖的操作
        处理器必须能正确处理指令依赖情况保证程序能得出正确的执行结果。
        对于有前后有依赖的指令，CPU一般不会reorder(Alpha架构除外)。
        例如: a = 5; b = a + 1; 这两条指令存在依赖关系，不会被cpu重排顺序。
    (2) 显式的内存屏障/隐式的内存屏障
    (3) I/O 函数通常在实践中阻止重排，但这不是语言标准保证的。
        对标准输出（如 printf 或 std::cout）或文件系统的操作通常涉及系统调用。
        这些操作具有不可逆的外部副作用，编译器和系统必须保证它们按照逻辑顺序发生。
        std::cout << "Hello";  // A
        std::cout << "World";  // B
        // A须在B之前执行，因为这是可观察的I/O操作

==== 体系结构的差异性
对于没有前后依赖关系的指令，CPU就有可能对这些指令进行重排(除非使用memory barrier进行一些显示控制)，具体的力度则与CPU体系结构相关:

    x86是一种strong order(也叫TSO，total store order):
        同一CPU执行的load指令后接load指令(L-L)，store指令后接store指令(S-S)，load指令后接store指令(L-S):
            均不能交换指令的执行顺序
        仅store指令后接load指令(S-L)才可以

    ARM则是一种weak order:
        只要没有依赖关系，load指令和store指令就可任意交换。

==== Memory Model(MM/Memory Consistency Model)

===== 概念
https://en.wikipedia.org/wiki/Memory_model_(programming)

- Most research in the area of memory models revolves around:
** Designing a memory model that allows a maximal degree of freedom for compiler optimizations while still giving sufficient guarantees about race-free and (perhaps more importantly) race-containing programs.
** Proving program optimizations that are correct with respect to such a memory model.

https://research.swtch.com/mm
Hardware Memory Models: https://research.swtch.com/hwmm
Programming Language Memory Models: https://research.swtch.com/plmm

===== data race
https://en.wikipedia.org/wiki/Race_condition#Data_race
https://en.cppreference.com/w/cpp/language/multithread.html#Data_races
https://en.wikipedia.org/wiki/Race_condition#Example_definitions_of_data_races_in_particular_concurrency_models

要发生数据竞争，必须同时满足以下条件:
(1) 多线程/进程访问同一内存位置
(2) 至少有一个线程执行写操作
(3) 没有使用同步机制（如锁、原子操作、内存屏障等）来控制访问顺序

===== DRF-SC
DRF-SC/SC-DRF (Data-Race-Free with Sequential Consistency) 无数据竞争顺序一致性:
指在没有数据竞争（Data-Race）的情况下，程序执行能表现得像顺序一致性（Sequential Consistency, SC）一样，确保能实现高效且安全的并发编程。

Data-Race-Free (DRF): 在多线程环境中，当两个线程访问同一内存位置，至少一个写操作，且没有同步机制时，就发生数据竞争。

Sequential Consistency (SC): 一种理想的内存模型，保证所有操作看起来是按程序顺序串行执行的。

DRF-SC 目标: 在保证不发生数据竞争的前提下，允许底层硬件进行宽松的优化（比如乱序执行、缓存），从而提高性能，同时又向程序员呈现出简单的顺序一致性视图。

===== c/c++
https://en.cppreference.com/w/c/language/memory_model.html
https://en.cppreference.com/w/cpp/language/memory_model.html

===== gcc
https://gcc.gnu.org/wiki/Atomic/GCCMM

===== llvm
https://llvm.org/docs/Atomics.html
https://llvm.org/docs/MemoryModelRelaxationAnnotations.html

===== linux kernel
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0124r6.html

===== java
https://github.com/orientye/understand/blob/main/lan/java.asc#java-memory-model-and-thread

===== csharp
https://learn.microsoft.com/en-us/archive/msdn-magazine/2012/december/csharp-the-csharp-memory-model-in-theory-and-practice
https://learn.microsoft.com/en-us/archive/msdn-magazine/2013/january/csharp-the-csharp-memory-model-in-theory-and-practice-part-2

===== go
https://research.swtch.com/gomm

==== Why Memory Barriers
memory barriers are a necessary evil that is required to enable good performance and scalability, an evil that stems from the fact that CPUs are orders of magnitude faster than are both the interconnects between them and the memory they are attempting to access.
- 《perfbook》 Appendix C Unknown Why Memory Barriers?

===== Cache Structure
https://github.com/orientye/understand/blob/main/co-ca/cpu.asc#cache-structure

===== Cache-Coherence Protocols
https://github.com/orientye/understand/blob/main/co-ca/cpu.asc#cache-coherence-protocols

===== Stores Result in Unnecessary Stalls
====== 背景
its performance for the first write to a given cache line is quite poor.
使无效与使无效应答之间存在延迟。

====== Store Buffers
One way to prevent this unnecessary stalling of writes is to add “store buffers” between each CPU and its cache.

Q: But then why do uniprocessors also have store buffers?
A: Because the purpose of store buffers is not just to hide acknowledgement latencies in multiprocessor cache coherence protocols, but to hide memory latencies in
general. Because memory is much slower than is cache on uniprocessors, store buffers on uniprocessors can help to hide write-miss memory latencies.
问: 那为何单处理器系统也需要存储缓冲区？
答: 因为存储缓冲区的作用不仅限于掩盖多处理器缓存一致性协议中的确认延迟，更在于普遍掩盖内存访问延迟。在单处理器系统中，由于内存速度远低于缓存速度，存储缓冲区有助于掩盖写入未命中时的内存访问延迟。

Q: So store-buffer entries are variable length? Isn’t that difficult to implement in hardware?
A: Here are two ways for hardware to easily handle variable length stores.
First, each store-buffer entry could be a single byte wide. Then an 64-bit store would consume eight store-buffer entries. This approach is simple and flexible, but one disadvantage is that each entry would need to replicate much of the address that was stored to.
Second, each store-buffer entry could be double the size of a cache line, with half of the bits containing the values stored, and the other half indicating which bits had been stored to. So, assuming a 32-bit cache line, a single-byte store of 0x5a to the low-order byte of a given cache line would result in 0xXXXXXX5a for the first half and 0x000000ff for the second half, where the values labeled X are arbitrary because they would be ignored. This approach allows multiple consecutive stores corresponding to a given cache line to be merged into a single store-buffer entry, but is space-inefficient for random stores of single bytes.
Much more complex and efficient schemes are of course used by actual hardware designers.
问：那么存储缓冲区的条目是可变长度的？这在硬件上实现起来不是很难吗？
答：硬件可以通过以下两种方式轻松处理可变长度的存储操作：
首先，每个存储缓冲区条目可以设计为单字节宽度。这样，一次64位存储操作会占用八个存储缓冲区条目。这种方法简单灵活，但缺点在于每个条目都需要重复存储大量与目标地址相关的信息。
其次，每个存储缓冲区条目可以设为缓存行大小的两倍：其中一半比特位用于存储具体数值，另一半则用作位掩码来标识哪些位已被写入。假设采用32位缓存行，当向某缓存线低字节存储单字节数据0x5a时，前半部分会记录为0xXXXXXX5a，后半部分位掩码则记为0x000000ff（标有X的数值可任意，因为它们会被忽略）。这种方法允许将针对同一缓存行的多个连续存储操作合并到单个缓冲区条目中，但对于随机单字节存储操作则存在空间效率低下的问题。
实际硬件设计人员当然会采用更为复杂高效的实现方案。

These store buffers are local to a given CPU or, on systems with hardware multi threading, local to a given core.

====== Store Forwarding
Breaking this guarantee is violently counter-intuitive to software types, so much so that the hardware guys took pity and implemented “store forwarding”, where each CPU refers to (or “snoops”) its store buffer as well as its cache when performing loads, as shown in Figure C.6. In other words, a given CPU’s stores are directly forwarded to its subsequent loads, without having to pass through the cache.

当CPU执行一个读取操作时，它需要从内存中获取数据。但如果在它之前刚刚执行了一个写入操作到同一个内存地址，那么这个最新的数据可能还在CPU的存储缓冲区中，还没来得及被写入到缓存/内存里。
存储转发/前向存储机制就是：在这种情况下，CPU会绕过缓存，直接从自己的存储缓冲区中把这个“尚未提交”的数据转发给后续的读取操作。

作用范围：存储转发通常只在同一个CPU核心内部有效。一个核心的存储操作不能被转发到另一个核心。
存储缓冲区：这是实现该技术的关键硬件结构，用于临时存放已执行但尚未提交到缓存层的写入数据。
存储转发停顿：虽然存储转发是优化，但在某些复杂情况下它也可能导致性能下降。例如，当一次读取操作的部分数据来自存储缓冲区，而另一部分数据需要从缓存中获取时（比如读写对象地址没对齐），CPU可能不得不等待存储操作完全提交到缓存后，才能从缓存执行一个完整的读取，这个过程被称为“存储转发停顿”。

====== Store Buffers and Memory Barriers
[source, c]
----
// variables “a” and “b” initially zero
void foo(void)
{
    a=1;
    smp_mb();
    b=1;
}

void bar(void)
{
    while (b == 0) continue;
    assert(a == 1);
}
----

The memory barrier smp_mb() will cause the CPU to flush its store buffer before applying each subsequent store to its variable’s cache line. The CPU could either simply stall until the store buffer was empty before proceeding, or it could use the store buffer to hold subsequent stores until all of the prior entries in the store buffer had been applied.

===== Store Sequences Result in Unnecessary Stalls
====== 背景
Unfortunately, each store buffer must be relatively small, which means that a CPU executing a modest sequence of stores can fill its store buffer (for example, if all of them result in cache misses). At that point, the CPU must once again wait for invalidations to complete in order to drain its store buffer before it can continue executing. This same situation can arise immediately after a memory barrier, when all subsequent store instructions must wait for invalidations to complete, regardless of whether or not these stores result in cache misses.
遗憾的是，每个存储缓冲区的容量都相对有限，这意味着即使CPU仅执行数量适中的存储指令序列，也可能将其存储缓冲区填满（例如，如果所有这些存储操作均导致缓存未命中）。此时，CPU将不得不再次等待无效化操作完成，以便清空其存储缓冲区，之后才能继续执行后续指令。同样的情况会立即发生在内存屏障之后——届时所有后续存储指令都必须等待无效化操作完成，无论这些存储操作是否会引起缓存未命中。
This situation can be improved by making invalidate acknowledge messages arrive more quickly. One way of accomplishing this is to use per-CPU queues of invalidate messages, or “invalidate queues”.

====== Invalidate Queues
invalidate acknowledge不能尽快回复的主要原因:
One reason that invalidate acknowledge messages can take so long is that they must ensure that the corresponding cache line is actually invalidated, and this invalidation can be delayed if the cache is busy, for example, if the CPU is intensively loading and storing data, all of which resides in the cache. In addition, if a large number of invalidate messages arrive in a short time period, a given CPU might fall behind in processing them, thus possibly stalling all the other CPUs.

However, the CPU need not actually invalidate the cache line before sending the acknowledgement. It could instead queue the invalidate message with the understanding that the message will be processed before the CPU sends any further messages regarding that cache line.
但事实上，CPU其实不需要完成invalidate操作就可以回送acknowledgement消息。该CPU完全可以将无效化消息暂存于队列中，其前提条件是必须保证在后续发送涉及该缓存行的任何相关消息之前，这些暂存的invalidate消息都已被处理完毕。

注意，x86 架构没有称为 Invalidate Queue 的独立硬件结构。
x86 采用 TSO(Total Store Order) 内存模型，即：
Store Buffer 是存在的：写操作会先进入存储缓冲区，这可能导致“Store-Load”乱序。
Invalidate 逻辑被妥善处理：对于读操作，CPU 会通过 窥探协议(Snooping Protocol) 确保它总能读到最新的数据。当一个缓存行需要被其他核心无效化时，这个“无效化请求”会被当前核心迅速响应并完成，以保证一致性。这个处理过程的延迟非常短，并且由硬件自动保证正确性。

ARM也没有名为 Invalidate Queue 的独立硬件结构。
Invalidate Queue 更多地是作为一种教学模型。

====== Invalidate Queues and Invalidate Acknowledge
A CPU with an invalidate queue may acknowledge an invalidate message as soon as it is placed in the queue, instead of having to wait until the corresponding line is actually invalidated. Of course, the CPU must refer to its invalidate queue when preparing to transmit invalidation messages—if an entry for the corresponding cache line is in the invalidate queue, the CPU cannot immediately transmit the invalidate message; it must instead wait until the invalidate-queue entry has been processed.
当然，在准备发送无效化消息时，CPU 必须查询自身的无效化队列-如果队列中已存在针对该缓存行的条目，则 CPU 不能立即发送无效化消息，而必须等待队列中的对应条目被处理完毕。

Placing an entry into the invalidate queue is essentially a promise by the CPU to process that entry before transmitting any MESI protocol messages regarding that cache line. As long as the corresponding data structures are not highly contended, the CPU will rarely be inconvenienced by such a promise.
However, the fact that invalidate messages can be buffered in the invalidate queue provides additional opportunity for memory-misordering, as discussed in the next section.
将条目加入无效化队列，本质上是 CPU 做出的一项承诺：在发送任何涉及该缓存行的 MESI 协议消息之前，必须先处理该队列条目。只要对应的数据结构没有处于高度竞争状态，这种承诺通常不会对 CPU 造成性能影响。
然而，正如下一节将要讨论的，无效化消息能在无效化队列中缓冲的特性，为内存乱序创造了新的可能性。

====== Invalidate Queues and Memory Barriers
Let us suppose that CPUs queue invalidation requests, but respond to them immediately. This approach minimizes the cache-invalidation latency seen by CPUs doing stores, but can defeat memory barriers, as seen in the following example.
[source, c]
----
// variables “a” and “b” initially zero
// “a” is replicated read-only (MESI “shared” state)
// “b” is owned by CPU 0 (MESI “exclusive” or “modified” state)
void foo(void) // cpu0 execute
{
    a=1;
    smp_mb();
    b=1;
}

void bar(void) // cpu1 execute
{
    while (b == 0) continue;
    assert(a == 1);
}
----

Then the sequence of operations might be as follows:

(1) CPU 0 executes a = 1. The corresponding cache line is read-only in CPU 0’s cache, so CPU 0 places the new value of “a” in its store buffer and transmits an “invalidate” message in order to flush the corresponding cache line from CPU 1’s cache.
CPU 0 执行 a = 1。对应的缓存行在 CPU 0 的缓存中处于只读状态，因此 CPU 0 将“a”的新值放入store buffer，并发送一个“invalidate即使无效”消息，以便从 CPU 1 的缓存中清除对应的缓存行。

(2) CPU 1 executes while (b == 0)continue, but the cache line containing “b” is not in its cache. It therefore transmits a “read” message.
CPU 1 执行 while (b == 0) continue，但包含“b”的缓存行不在其缓存中。因此它发送一个“读”消息。

(3) CPU 1 receives CPU 0’s “invalidate” message, queues it, and immediately responds to it.
CPU 1 收到 CPU 0 的“无效”消息，将其加入队列，并立即回复/响应该消息。

(4) CPU 0 receives the response from CPU 1, and is therefore free to proceed past the smp_mb() on line 4 above, moving the value of “a” from its store buffer to its cache line.
CPU 0 收到来自 CPU 1 的回复，因此可以继续执行第 4 行的 smp_mb() 之后的指令，将“a”的值从其存储缓冲区移动到其缓存行中。

(5) CPU 0 executes b = 1. It already owns this cache line (in other words, the cache line is already in either the “modified” or the “exclusive” state), so it stores the new value of “b” in its cache line.
CPU 0 执行 b = 1。它已经拥有该缓存行（即该缓存行已处于“修改”或“独占”状态），因此它将“b”的新值存储到其缓存行中。

(6) CPU 0 receives the “read” message, and transmits the cache line containing the now-updated value of “b” to CPU 1, also marking the line as “shared” in its own cache.
CPU 0 收到“读”消息，并将包含已更新值“b”的缓存行发送给 CPU 1，并标记该cacheline为shared状态。

(7) CPU 1 receives the cache line containing “b” and installs it in its cache.
CPU 1 收到包含“b”的缓存行，并将其将其应用到本地缓存。

(8) CPU 1 can now finish executing while (b ==0) continue, and since it finds that the value of “b” is 1, it proceeds to the next statement.
CPU 1 现在可以完成 while (b == 0) continue 的执行，由于它发现“b”的值为 1，于是继续执行下一条语句。

(9) CPU 1 executes the assert(a == 1), and, since the old value of “a” is still in CPU 1’s cache, this assertion fails.
CPU 1 执行 assert(a == 1)，由于“a”的旧值仍在 CPU 1 的缓存中，该断言失败。

(10) Despite the assertion failure, CPU 1 processes the queued “invalidate” message, and (tardily) invalidates the cache line containing “a” from its own cache.
尽管断言失败，CPU 1 仍处理队列中的“使无效”消息，并（延迟地）从其自己的缓存中使包含“a”的缓存行无效。

There is clearly not much point in accelerating invalidation responses if doing so causes memory barriers to effectively be ignored. However, the memory-barrier instructions can interact with the invalidate queue, so that when a given CPU executes a memory barrier, it marks all the entries currently in its invalidate queue, and forces any subsequent load to wait until all marked entries have been applied to the CPU’s cache.
如果加速无效响应会导致内存屏障失效，那么这样做显然没有意义。然而，内存屏障指令可以与无效队列交互，使得当某个 CPU 执行内存屏障时，它会标记当前在其无效队列中的所有条目，这些被标注的项次被称为marked entries，而随后CPU执行的任何的load操作都需要等到Invalidate Queue中所有marked entries完成对cacheline的操作之后才能进行。

因此，要想保证程序逻辑正确，需要给bar函数增加内存屏障的操作，具体如下:
[source, c]
----
// variables “a” and “b” initially zero
// “a” is replicated read-only (MESI “shared” state)
// “b” is owned by CPU 0 (MESI “exclusive” or “modified” state)
void foo(void) // cpu0 execute
{
    a=1;
    smp_mb();
    b=1;
}

void bar(void) // cpu1 execute
{
    while (b == 0) continue;
    smp_mb();
    assert(a == 1);
}
----
同上(bar函数没有加内存屏障)，前面7个步骤是一样的，到了第8步，执行开始不同了:

(8) CPU 1 can now finish executing while (b ==0) continue, and since it finds that the value of “b” is 1, it proceeds to the next statement, which is now a memory barrier.
CPU 1 此时可完成执行 while (b ==0) continue 循环，由于检测到"b"的值为1，便继续执行下一语句，此时遇到内存屏障。

(9) CPU 1 must now stall until it processes all preexisting messages in its invalidation queue.
CPU 1 必须暂停执行，直至其处理完失效队列中的所有待处理消息。

(10) CPU 1 now processes the queued “invalidate” message, and invalidates the cache line containing “a” from its own cache.
CPU 1 开始处理队列中的"失效"消息，将包含"a"的缓存行从自身缓存中置为无效。

(11) CPU 1 executes the assert(a == 1), and, since the cache line containing “a” is no longer in CPU 1’s cache, it transmits a “read” message.
CPU 1 执行 assert(a == 1) 断言，由于包含"a"的缓存行已不在其缓存中，遂发送"读"消息。

(12) CPU 0 responds to this “read” message with the cache line containing the new value of “a”.
CPU 0 响应此"读取"消息，传回包含新值"a"的缓存行。

(13) CPU 1 receives this cache line, which contains a value of 1 for “a”, so that the assertion does not trigger.
CPU 1 接收到该缓存行，其中"a"的值为1，因此断言未触发。

===== Read and Write Memory Barriers
In the previous section, memory barriers were used to mark entries in both the store buffer and the invalidate queue. But in our code fragment, foo() had no reason to
do anything with the invalidate queue, and bar() similarly had no reason to do anything with the store buffer. Many CPU architectures therefore provide weaker memory-barrier instructions that do only one or the other of these two. Roughly speaking, a “read memory barrier” marks only the invalidate queue (and snoops entries in the
store buffer) and a “write memory barrier” marks only the store buffer, while a full-fledged memory barrier does all of the above.

The software-visible effect of these hardware mechanisms is that a read memory barrier orders only loads on the CPU that executes it, so that all loads preceding the read memory barrier will appear to have completed before any load following the read memory barrier. Similarly, a write memory barrier orders only stores, again on the CPU that executes it, and again so that all stores preceding the write memory barrier will appear to have completed before any store following the write memory barrier. A full-fledged memory barrier orders both loads and stores, but again only on the CPU executing the memory barrier.

改进后的foo()和bar():
[source, c]
----
void foo(void)
{
    a=1;
    smp_wmb();
    b=1;
}

void bar(void)
{
    while (b == 0) continue;
    smp_rmb();
    assert(a == 1);
}
----
Some computers have even more flavors of memory barriers, but understanding these three variants will provide a good introduction to memory barriers in general.

===== 存储缓冲区与失效队列
- Store Buffer(存储缓冲区/写缓冲区)
是CPU核心内部的一个小型、高速的硬件队列，用于临时存放CPU核心想要写入到缓存（Cache）中的数据。
https://developer.arm.com/documentation/ddi0489/f/memory-system/l1-caches/store-buffer
https://community.intel.com/t5/Software-Tuning-Performance/Purpose-of-Load-Buffer-in-x86/td-p/1091736

- Invalidate Queue(失效队列/无效化队列)
临时存放“失效请求”的缓冲区，其目的是为了提高处理器的执行效率，避免处理器在写操作时长时间等待其他CPU的响应确认。

- 比较
特性    失效队列（无效化队列/Invalidate Queue）    存储缓冲区（写缓冲区/Store Buffer）
解决的问题    加速对缓存失效确认的响应    让CPU不必等待写入完成
工作原理    快速接收并确认其他CPU发来的“失效”消息，将其排队，稍后再处理    CPU将“写”指令的结果先暂存于此，然后继续执行，由后台完成写入缓存
位于何处    CPU的缓存控制器中（更靠近缓存一致性协议逻辑）    CPU核心与缓存之间
主要目的    避免CPU因等待缓存行失效而停滞    实现写操作的非阻塞
可能引起的问题    读取旧数据：CPU可能从自己已失效（但还在队列中）的缓存行里读取到旧数据    看到最新写入：其他CPU可能看不到本CPU刚刚写入的值（因为还在缓冲区里）
需要何种内存屏障    读内存屏障 确保在处理新的读操作前，先清空失效队列    写内存屏障 确保在处理新的写操作前，先清空存储缓冲区

Q: 存储缓冲区与失效队列有多大
大致大小	约 10 到 64 条目	约 8 到 32 条目
确定性信息	无法公开获得，是核心商业机密。	无法公开获得，是核心商业机密。

Q: 满了怎么办？
CPU核心会被强制“停滞”，直到有空闲条目可用。这被称为 “流水线停滞” 或 “内存顺序停滞” ，是性能杀手。

Q: 具体来说，存储缓冲区如果满了怎么办？
场景: CPU核心正在疯狂地执行存储指令（比如，在一个紧凑的循环中修改多个变量）。
核心只能空转，等待存储缓冲区出现空位；等待期间，存储缓冲区会持续工作。

Q: 具体来说，失效队列如果满了怎么办？
场景: 系统中有多个CPU核心正在频繁地修改共享数据，导致某个核心收到了大量的“失效”消息。
必须阻塞发送方，它会延迟对新的“失效”消息的确认，直到它能在失效队列中腾出空间来处理这个新请求；为了腾出空间，缓存控制器必须加速处理队列中现有的失效条目-即真正地查找并无效化本地缓存中对应的缓存行。

===== Example Memory-Barrier Sequences
====== Ordering-Hostile Architecture
let us insteaddesign a mythical but maximally memory-ordering-hostile computer architecture.
This hardware must obey the following ordering constraints [McK05a, McK05b]:
1. Each CPU will always perceive its own memory accesses as occurring in program order.
2. CPUs will reorder a given operation with a store only if the two operations are referencing different locations.
3. All of a given CPU’s loads preceding a read memory barrier (smp_rmb()) will be perceived by all CPUs to precede any loads following that read memory barrier.
4. All of a given CPU’s stores preceding a write memory barrier (smp_wmb()) will be perceived by all CPUs to precede any stores following that write memory barrier.
5. All of a given CPU’s accesses (loads and stores) preceding a full memory barrier (smp_mb()) will be perceived by all CPUs to precede any accesses following that memory barrier.
我们来设计一个虚构的、在内存排序上极度不友好的计算机架构。
该硬件必须遵守以下排序约束:
1. 对于每个CPU而言，从它自己的角度看，其内存访问的顺序总是符合program order的。
2. 仅当两个操作访问不同地址时，CPU 才会对某个操作与一次存储进行重排序。
3. 在某个 CPU 中，位于读内存屏障（smp_rmb()）之前的所有加载操作，在所有 CPU 看来，都必须先于该屏障之后的任何加载操作。
4. 在某个 CPU 中，位于写内存屏障（smp_wmb()）之前的所有存储操作，在所有 CPU 看来，都必须先于该屏障之后的任何存储操作。
5. 在某个 CPU 中，位于全内存屏障（smp_mb()）之前的所有访问（加载和存储），在所有 CPU 看来，都必须先于该屏障之后的任何访问。

Q: Does the guarantee that each CPU sees its own memory accesses in order also guarantee that each user-level thread will see its own memory accesses in order? Why or why not?
A: No. Consider the case where a thread migrates from one CPU to another, and where the destination CPU perceives the source CPU’s recent memory operations out of order.
To preserve user-mode sanity, kernel hackers must use memory barriers in the context-switch path. However, the locking already required to safely do a context switch
should automatically provide the memory barriers needed to cause the user-level task to see its own accesses in order. That said, if you are designing a super-optimized
scheduler, either in the kernel or at user level, please keep this scenario in mind!
问题：保证每个CPU按顺序看到自己的内存访问，是否也保证每个用户级线程按顺序看到自己的内存访问？
为什么能或为什么不能？
答：不能。考虑这样一种情况：一个线程从一个CPU迁移到另一个CPU，而目标CPU可能以乱序的方式感知源CPU最近的内存操作。
为保证用户态程序的正常运行，内核开发者人员必须在上下文切换路径中使用内存屏障。然而，安全执行上下文切换所需的锁定操作通常已经自动提供了所需的内存屏障，从而使用户级任务能够按顺序看到自己的内存访问。尽管如此，如果您正在设计一个超级优化的调度器（无论是在内核层还是用户层），请务必牢记这种情况！

===== Are Memory Barriers Forever
There have been a number of recent systems that are significantly less aggressive about out-of-order execution in general and re-ordering memory references in particular.
Will this trend continue to the point where memory barriers are a thing of the past?
近期出现了不少新系统，它们在乱序执行（尤其是内存访问重排序）方面明显不再那么激进。这一趋势是否会持续到内存屏障彻底退出历史舞台的程度？

===== Advice to Hardware Designers
1. I/O devices that ignore cache coherence
2. External busses that fail to transmit cache-coherence data
3. Device interrupts that ignore cache coherence
4. Inter-processor interrupts (IPIs) that ignore cache coherence
5. Context switches that get ahead of cache coherence
6. Overly kind simulators and emulators

===== 参考
perfbook C2
《Parallel Computer Architecture A Hardware / Software Approach》
《A Primer on Memory Consistency and Cache Coherence》2nd
Paper《Memory Barriers: a Hardware View for Software Hackers》by PE McKenney 2010

==== Q&A
===== mutex vs. atomic
https://stackoverflow.com/questions/15056237/which-is-more-efficient-basic-mutex-lock-or-atomic-integer

===== linux内核屏障为什么没有使用c atomic
Linux内核使用自己的内存屏障系统是因为：
历史原因：内核开发早于C/C++内存模型标准
技术需求：需要更精细的控制和优化
性能要求：需要最小化多核系统开销
硬件交互：需要处理设备、中断等特殊场景

==== cpu视角
Software MMs have converged on SC for data-race-free programs(SC-DRF).

===== C/C++11 mappings to processors
https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html

以memory_order_seq_cst为例:
Ubuntu(g++, VM, Intel i5 4 core):    汇编指令mfence
Mac OSX(g++,  Intel i5 2 core):      汇编指令xchgl(= lock xchgl)
Win10(VS2015, Intel i5 4 core):      _ReadWriteBarrier, _InterlockedExchange

- 对于x86:
* acquire、release、relaxed 通常只是mov，不需要额外操作
    ** 对于 acquire: 不会将 acquire 之后的 load/store 重排到它之前，即 load-load/load-store 不会重排
    ** 对于 release: 不会将 release 之前的 load/store 重排到它之后，即 load-store/store-store 不会重排
* seq_cst:
    ** store操作通常使用xchg(= lock xchg)/(mov;mfence)
    ** load操作通常也只是mov
* Note: there is an alternative mapping of C/C++11 to x86, which instead of locking (or fencing) the Seq Cst store locks/fences the Seq Cst load:
    ** Load Seq_Cst:	LOCK XADD(0) // alternative: MFENCE,MOV (from memory)
    ** Store Seq Cst:	MOV (into memory)

示例:
https://godbolt.org/z/195badroP
https://godbolt.org/z/4PvsMoah5

===== x86/x64
vol 2:
LFENCE—Load Fence
SFENCE—Store Fence
MFENCE—Memory Fence

vol3:
https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html
chapter8 multi-processor management
8.2 memory ordering
8.2.5 strengthening or weakening the memory-order model

重排类型	  x86
Load-Load	❌禁止
Load-Store	❌禁止
Store-Store	❌禁止
Store-Load	✅允许

Q: 为什么 x86 允许 Store-Load 重排呢？
A: x86允许Store-Load重排主要是为了性能优化。Store操作需要等待缓存一致性协议完成（等待其他CPU的invalidate acknowledge），这个过程可能比较慢。如果允许后续的Load操作先执行（只要Load不依赖Store的数据），CPU可以继续执行其他指令，提高流水线利用率。这种重排不会影响单线程程序的正确性，因为Load不会读取Store写入的值（否则会有数据依赖，CPU不会重排）。

Q: 为什么 x86 禁止 Store-Store 重排呢？
A: x86禁止Store-Store重排是为了保证写操作的顺序一致性。如果允许Store-Store重排，可能会导致其他CPU看到写操作的顺序与程序顺序不一致，这会破坏很多同步原语的正确性。例如，在释放锁之前写入共享数据，如果这两个Store可以重排，其他线程可能在看到锁被释放时，还看不到共享数据的更新，导致数据竞争。禁止Store-Store重排是TSO（Total Store Order）内存模型的核心特征之一。

参考:
https://en.wikipedia.org/wiki/Memory_ordering#In_symmetric_multiprocessing_(SMP)_microprocessor_systems

===== ARM
ARM CPU 部分：
ARMv8（2011年10月宣布）引入了 “SC load acquire” 和 “SC store release” 指令。
ARM GPU 部分：
当时 ARM GPU 的内存模型更强（完全顺序一致性 SC）。
ARM 宣布 GPU 将与 CPU 完全一致（fully coherent），并计划在 GPU 中也加入 “SC load acquire” 和 “SC store release” 指令。

ldr：普通加载指令（Load Register）
ldra：Load Acquire（带获取语义的加载）
str：普通存储指令（Store Register）
strl：Store Release（带释放语义的存储）

特性	ARMv7	ARMv8 (v8.0 及以后)
内存模型	弱内存模型（Weak Ordering）	弱内存模型，但提供 SC 原子指令
SC 原子支持	无硬件直接支持，需屏障+普通指令	有 ldar/stlr 专用指令
实现 SC 操作	需要多条指令 + 多个 dmb 屏障	单条指令，效率高
性能开销	高（屏障导致流水线停顿）	低（硬件优化）
编程复杂度	高（需手动管理屏障）	低（指令隐含正确语义）
对 SC-DRF 的硬件支持	弱，依赖大量屏障	强，硬件原生支持

==== kernel视角
https://github.com/orientye/understanding-the-linux-kernel/blob/main/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Linux%E5%86%85%E6%A0%B8/%E8%BF%9B%E7%A8%8B/%E5%90%8C%E6%AD%A5.asc#barrier[linux-kernel-barrier]

==== gcc atomic operations
https://gcc.gnu.org/wiki/Atomic/GCCMM/AtomicSync

https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html
https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html

==== llvm atomics
https://llvm.org/docs/Atomics.html

==== MSVC atomic
https://github.com/microsoft/STL/blob/main/stl/inc/atomic

==== c++
===== 概念
https://en.cppreference.com/w/cpp/atomic/memory_order

====== Sequenced-before
Sequenced-before 是 C++（以及其他类似语言，如 C）标准中定义的一种严格的、非对称的、可传递的二元关系，用于描述同一线程内两个操作的执行顺序。
简单地说：如果操作 A sequenced-before 操作 B，那么意味着在单线程视角下：
    A 必须在 B 之前完成。
    A 的所有效果（如写入内存）在 B 开始执行时都是可见的。

https://en.cppreference.com/w/cpp/atomic/memory_order.html#Sequenced-before
https://en.cppreference.com/w/cpp/language/eval_order.html
例如:
The first (left) operand of the built-in logical AND operator &&, the built-in logical OR operator || and the built-in comma operator , is sequenced before the second (right) operand.
再例如，f(g(), h()); // g() 和 h() 的调用谁先执行？不确定！但可以确定：g()和h()都 sequenced-before f()的执行。
很多“未定义行为”的本质，就是因为两个操作之间缺乏明确的 sequenced-before 关系，导致编译器无法确定一个唯一的、正确的执行顺序。

注意，Sequenced-before是单线程内的概念，它只管单线程，即无法约束硬件和编译器对其他线程的可见性顺序。

====== Carries dependency
Carries Dependency 是 sequenced-before 关系的一个子集，它描述了更紧密的数据依赖。所有 carries-dependency 关系必定也是 sequenced-before 关系，但反之不成立。
例如：a = 1; b = 2; 是 sequenced-before，但通常不 carry dependency（除非后面有依赖 b 的操作）。
而 a = 1; b = a + 1; 中，a=1 sequenced-before 且 carries dependency into b=a+1，因为 b 的计算直接依赖于 a 的值。
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Carries_dependency

====== Modification order
对于一个特定的原子变量，在程序的整个执行过程中，所有对它进行修改（写入）的操作所构成的一个全局一致的全序。
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Modification_order

Q: fetch_add(T, std::memory_order_relaxed) 有同步功能吗？
简单来说：在“线程间同步”层面没有功能，但在“原子变量自身”层面有保障。
可以从以下两个维度来拆解 fetch_add 使用 memory_order_relaxed 的表现：
1. 它没有传统意义上的线程间同步功能
无 Happen-before 关系：它不像 release 或 acquire 那样能建立跨线程的可见性屏障。
不保护其他变量：如果线程 A 用 relaxed 修改了 x，线程 B 即使读到了 x 的最新值，也不能保证能看到线程 A 在此之前对 data 等普通变量的修改。
2. 它在原子变量自身上具有两个核心保障
虽然它是 relaxed，但它依然是原子操作，这提供了两个关键特性：
原子性 (Atomicity)：即使 100 个线程同时对一个变量执行 fetch_add(1, relaxed)，最终结果一定准确地增加了 100。不会发生像普通 i++ 那样因为“读取-修改-写入”被中断而导致更新丢失的情况。
修改顺序一致性 (Modification Order)：对于同一个原子变量，所有线程看到的修改顺序是一致的。如果一个线程看到了变量值为 2，那么它后续绝对不会再读到该变量之前的旧值（如 1）。 

====== Release sequence
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Release_sequence
释放序列的核心思想：
一旦一个线程执行了释放操作，所有后续通过 RMW 操作"接触"到这个释放值的线程，以及从这些线程读取值的线程，都能看到释放操作之前的所有写入。
这是 C++ 内存模型中实现高效、可扩展同步的关键机制，允许在不使用全序（sequential consistency）的情况下，实现复杂的同步模式。

规则:
An atomic operation A that performs a release operation on an atomic object M synchronizes with an atomic operation B that performs an acquire operation on M and takes its value from any side effect in the release sequence headed by A.
https://eel.is/c++draft/atomics.order#2

- In C++, a release sequence is a mechanism in the CPP memory model that allows multiple threads to synchronize with a single release operation.

- A release sequence headed by a release operation A (on an atomic object M) is a maximal contiguous subsequence of modifications to M in its modification order, starting with A and followed by any number of:
** Atomic read-modify-write (RMW) operations (e.g., fetch_add, compare_exchange) performed by any thread.
** Writes performed by the same thread that performed A (Note: In C++20 and later, this part was narrowed to only include RMW operations to simplify the model).

https://stackoverflow.com/questions/38565650/what-does-release-sequence-mean
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0982r1.html
https://stackoverflow.com/questions/77076002/why-release-sequence-can-only-contain-read-modify-write-but-not-pure-write
https://comp.lang.cpp.moderated.narkive.com/riFynbxP/atomics-and-memory-model-the-release-sequence-and-synchronizes-with#

If some atomic is store-released and several other threads perform read-modify-write operations on that atomic, a "release sequence" is formed: all threads that perform the read-modify-writes to the same atomic synchronize with the first thread and each other even if they have no memory_order_release semantics. This makes single producer - multiple consumers situations possible without imposing unnecessary synchronization between individual consumer threads.
如果一个原子变量以存储-释放操作进行写入，随后多个其他线程对该原子变量执行读-修改-写操作，便会形成一个“释放序列”：所有对同一原子变量执行读-修改-写操作的线程，即使它们不具备 memory_order_release 语义，也会与首个线程及其他线程建立同步关系。这使得单生产者-多消费者的场景成为可能，同时避免了在各消费者线程之间施加不必要的同步开销。
参考: https://en.cppreference.com/w/c/atomic/memory_order#Release_sequence

编译器做了什么？
在编译器和硬件层面，释放序列（Release Sequence）的实现并不依赖于某种特殊的“序列指令”，而是通过禁止编译器重排序和利用硬件缓存一致性协议（Cache Coherence）共同完成的。
对于 Head (Release)：插入屏障，确保之前的写操作对所有核可见。
对于 Sequence (RMW)：仅生成原子操作代码（如 lock 前缀或 ldrex/strex 循环），不增加额外的屏障开销（如果是 relaxed）。
对于 Tail (Acquire)：插入屏障，确保后续操作能读取到最新的内存缓存。
这种机制允许开发者利用 RMW 操作的“连带效应”，在不牺牲中间环节性能（使用 relaxed）的情况下，实现跨线程的安全同步。

[source, cpp]
----
std::atomic<int> count{0};
// Thread 1 (Producer)
data = 42; 
count.store(1, std::memory_order_release); // Head of Release Sequence

// Thread 2 (Consumer A)
if (count.fetch_add(1, std::memory_order_relaxed) == 1) { 
    // This RMW continues the release sequence
}

// Thread 3 (Consumer B)
if (count.load(std::memory_order_acquire) == 2) {
    // Synchronizes with Thread 1 via the release sequence
    assert(data == 42); // Guaranteed to succeed
}
----
在释放序列中，中间的 RMW（读-修改-写）操作通过 缓存锁定（Cache Line Locking）维持了原子性：
当线程 2 执行 fetch_add 时，它必须获得该缓存行的独占权（Modified 状态）。
根据 MESI 协议，这会自动触发硬件层面的数据同步。
即便线程 2 是 relaxed，它的原子操作也像一根“导线”，将线程 1 的 Release 信号通过缓存一致性传导给了线程 3 的 Acquire 操作。

====== Synchronizes-with
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Synchronizes_with

If an atomic store in thread A is a release operation, an atomic load in thread B from the same variable is an acquire operation, and the load in thread B reads a value written by the store in thread A, then the store in thread A synchronizes-with the load in thread B.

Also, some library calls may be defined to synchronize-with other library calls on other threads.

同步（Synchronizes-with）的本质是强迫系统“对齐缓存”：
在 Release 端：强制将当前核心的写缓冲区（Store Buffer）数据刷出，或标记为其他核可见。
在 Acquire 端：强制使当前核心的无效队列（Invalidation Queue）生效，确保读取到的不是旧的缓存副本。

原子级别：需要匹配 release 和 acquire，并且必须真的读到那个值才能触发同步。
库级别：库函数（如 unlock/lock）在背后为维护了这些复杂的内存屏障，使用起来更简单。

[source, cpp]
----
std::atomic<int> flag(0);
Data* data = nullptr;

// 线程 A（生产者）
data = new Data();           // 1. 准备数据
data->init();                // 2. 初始化数据
flag.store(1, std::memory_order_release); // 3. 发布信号

// 线程 B（消费者）
if (flag.load(std::memory_order_acquire) == 1) { // 4. 读取信号
    data->use();            // 5. 安全使用数据
}
----
这段代码通过 flag 建立了一座跨越线程的“桥梁”：
Release (3)：线程 A 执行 release 存储。这样确保在指令 (3) 之前发生的所有内存写入（即数据的创建 1 和初始化 2）都必须离开 Store Buffer，或者至少对后续的 Acquire 线程可见。
Acquire (4)：线程 B 执行 acquire 加载。这样强制失效本地缓存中陈旧的数据，并确保在指令 (4) 之后发生的操作（即 5）不会被重排到 (4) 之前。
同步 (Synchronizes-with)：一旦线程 B 的 load 读到了线程 A 写入的 1，同步关系正式建立。
结论：线程 A 中 flag.store 之前的所有操作，对于线程 B 中 flag.load 之后的操作，都是可见的。

====== Dependency-ordered before
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Dependency-ordered_before

Between threads, evaluation A is dependency-ordered before evaluation B if any of the following is true:

1) A performs a release operation on some atomic M, and, in a different thread, B performs a consume operation on the same atomic M, and B reads a value written by any part of the release sequence headed(until C++20) by A.
2) A is dependency-ordered before X and X carries a dependency into B.

情况2依赖于情况1。必须先有一个情况1的release-consume对建立了初始的"dependency-ordered before"关系，然后才能通过情况2将这个关系扩展到更多操作。

例如:
int x = atomic.load(std::memory_order_consume);  // X
int y = x + 5;                 // y携带对x的依赖
int z = y * 2;                 // z携带对y的依赖，间接对x的依赖

====== Inter-thread happens-before
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Inter-thread_happens-before

Between threads, evaluation A inter-thread happens before evaluation B if any of the following is true:

1) A synchronizes-with B.
2) A is dependency-ordered before B.
3) A synchronizes-with some evaluation X, and X is sequenced-before B.
4) A is sequenced-before some evaluation X, and X inter-thread happens-before B.
5) A inter-thread happens-before some evaluation X, and X inter-thread happens-before B.

情况3:
[source, cpp]
----
#include <atomic>
#include <thread>
#include <cassert>

//假设有一个共享的数据缓冲区和一个状态标志。状态标志是一个原子变量，用于同步对缓冲区的访问

struct Data {
    int x;
    int y;
};

Data buffer; // 共享的非原子数据
std::atomic<int> flag{0}; // 原子的状态标志，0=空，1=就绪

// 线程 T1：准备数据并发布
void producer() {
    // A1: 修改共享数据（非原子操作）
    buffer.x = 42;
    buffer.y = 100;

    // X: 释放(store-release)原子标志，将之前的所有写入“推送”出去
    flag.store(1, std::memory_order_release); // 这是操作 X
    // 在单个线程T1内部，对buffer的修改（A1）顺序先于(X)
}

// 线程 T2：检查并消费数据
void consumer() {
    // 操作 B1：获取(load-acquire)原子标志
    int f = flag.load(std::memory_order_acquire);

    if (f == 1) {
        // B2: 消费共享数据
        assert(buffer.x == 42); // 这里必须看到 42
        assert(buffer.y == 100); // 这里必须看到 100
        // 在单个线程T2内部，加载标志(B1)顺序先于读取buffer(B2)
    }
}

int main() {
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join();
    t2.join();
    return 0;
}
----
让 A' = flag.store(1, std::memory_order_release) (即例子中的 X)。
让 X' = flag.load(std::memory_order_acquire) (即例子中的 B1)。
让 B' = assert(buffer.x == 42) (即我们例子中的 B)。
那么：
A' 同步于 X' 成立（release-acquire 配对）。
X' 顺序先于 B' 成立（T2 线程内的程序顺序）。
根据情况3：“A’ 同步于某个求值 X’，且 X’ 顺序先于 B’”，可以得出结论：A’ 线程间先发生于 B’。
这也意味着，在 flag.store 之前 T1 所做的所有写操作（包括写 buffer），其效果对 T2 中在 flag.load 之后的所有读操作（包括读 buffer）都是可见的。

情况4:
同样是上面的例子，令:
A = buffer.x = 42 和 buffer.y = 100（T1中的非原子写操作）
X = flag.store(1, std::memory_order_release)（T1中的release存储）
Y = flag.load(std::memory_order_acquire)（T2中的acquire加载）
B = assert(buffer.x == 42)（T2中的读buffer操作
那么有：
A 顺序先于 X
X 线程间先发生于 B（通过 X→Y→B 的传递），也就是情况3
这正是情况4。

情况5:
[source, cpp]
----
#include <atomic>
#include <thread>
#include <cassert>

std::atomic<int> x{0}, y{0};  // 两个同步变量
int data1 = 0, data2 = 0;     // 要保护的数据

// 线程T1：初始化数据，通过x同步
void thread1() {
    data1 = 42;                // 操作 A
    x.store(1, std::memory_order_release);  // 操作 X（同步点1）
}

// 线程T2：中间传递，通过x和y同步
void thread2() {
    // 等待T1的同步
    int x_val = x.load(std::memory_order_acquire);  // 操作 Y
    if (x_val == 1) {
        data2 = data1 + 10;    // 操作 M（依赖于data1）
        y.store(1, std::memory_order_release);  // 操作 Z（同步点2）
    }
}

// 线程T3：最终消费，通过y同步
void thread3() {
    // 等待T2的同步
    int y_val = y.load(std::memory_order_acquire);  // 操作 W
    if (y_val == 1) {
        assert(data1 == 42);   // 操作 B
        assert(data2 == 52);   // 操作 C
    }
}

int main() {
    std::thread t1(thread1);
    std::thread t2(thread2);
    std::thread t3(thread3);
    
    t1.join();
    t2.join();
    t3.join();
    return 0;
}
----

====== Happens-before
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Happens-before

Regardless of threads(不论是否涉及多个线程), evaluation A happens-before evaluation B if any of the following is true:

1) A is sequenced-before B.
2) A inter-thread happens before B.
The implementation is required to ensure that the happens-before relation is acyclic(无环的), by introducing additional synchronization if necessary (it can only be necessary if a consume operation is involved, see https://www.cl.cam.ac.uk/~pes20/cpp/popl085ap-sewell.pdf).

If one evaluation modifies a memory location, and the other reads or modifies the same memory location, and if at least one of the evaluations is not an atomic operation, the behavior of the program is undefined (the program has a data race) unless there exists a happens-before relationship between these two evaluations.

Simply happens-before:
Regardless of threads, evaluation A simply happens-before evaluation B if any of the following is true:
1) A is sequenced-before B.
2) A synchronizes-with B.
3) A simply happens-before X, and X simply happens-before B.
Note: without consume operations, simply happens-before and happens-before relations are the same.

以上是until C++26，
since C++26:

Happens-before:
Regardless of threads, evaluation A happens-before evaluation B if any of the following is true:
1) A is sequenced-before B.
2) A synchronizes-with B.
3) A happens-before X, and X happens-before B.

Strongly happens-before:
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Strongly_happens-before

====== Visible side-effects
The side-effect A on a scalar M (a write) is visible with respect to value computation B on M (a read) if both of the following are true:

1) A happens-before B.
2) There is no other side effect X to M where A happens-before X and X happens-before B.
If side-effect A is visible with respect to the value computation B, then the longest contiguous subset of the side-effects to M, in modification order, where B does not happen-before it is known as the visible sequence of side-effects (the value of M, determined by B, will be the value stored by one of these side effects).

Note: inter-thread synchronization boils down to preventing data races (by establishing happens-before relationships) and defining which side effects become visible under what conditions.

可见的副作用
对于标量 M 的副作用 A（一次写操作），相对于 M 的值计算 B（一次读操作）是可见的，当且仅当以下两个条件同时成立：

1) A 先发生于 B。
2) 不存在其他对 M 的副作用 X，使得 A 先发生于 X，且 X 先发生于 B。
如果副作用 A 相对于值计算 B 是可见的，那么 M 的副作用在修改顺序中，最长连续的子集（其中 B 不先发生于该子集内的副作用），被称为可见的副作用序列（此时 B 所确定的 M 的值，将是这些副作用之一所存储的值）。

注：线程间同步的核心在于防止数据竞争（通过建立先发生关系），并定义在何种条件下哪些副作用会成为可见的。

条件2定义了一个“最近一次可见的写”。
对于一个读操作 B，在所有“先发生于 B”的写操作中，条件2确保了 A 是其中最后一个（在 happens-before 顺序上）对 M 进行修改的操作。如果没有这个条件，B 可能会看到一个更早的、已经被覆盖掉的值。
简单来说：
“可见”不仅仅意味着“发生过”，还意味着它是“最后一个发生过且能影响当前读操作的写操作”。 条件2就是用来排除那些虽然发生过，但被后面其他写操作“覆盖”了的旧值。

====== Consume operation
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Consume_operation
Atomic load with memory_order_consume or stronger is a consume operation. Note that std::atomic_thread_fence imposes stronger synchronization requirements than a consume operation.

====== Acquire opertation
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Acquire_operation
Atomic load with memory_order_acquire or stronger is an acquire operation. The lock() operation on a Mutex is also an acquire operation. Note that std::atomic_thread_fence imposes stronger synchronization requirements than an acquire operation.

====== Release operation
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Release_operation
Atomic store with memory_order_release or stronger is a release operation. The unlock() operation on a Mutex is also a release operation. Note that std::atomic_thread_fence imposes stronger synchronization requirements than a release operation.

===== 本质
除了原子性本身，本质上还有顺序性和可见性两个问题:

    【1】______不可以重排到______的前面/后面
    【2】______对谁______可见
     或者说
    【1】同步原语的顺序约束
     在______操作之前/之后的内存访问，在______可见性保证下，具有什么样的执行顺序保证？
    【2】happens-before关系的建立
    通过使用______内存序，在线程间建立了什么样的happens-before关系？

内存序的核心是建立happens-before关系，而不是直接控制缓存一致性
内存序不是强制立即刷新缓存，而是建立逻辑上的顺序关系
硬件仍然需要时间传播数据，但顺序保证让程序逻辑正确
常见的误区是认为release/acquire会让数据"立即"跨线程可见
实际上，数据传播需要时间
但逻辑顺序保证了：当看到release的标志时，一定能看到 release 之前的所有内存操作（包括读取和写入）。

relaxed(松弛):
✅ 单个原子变量的修改全序一致性
✅ 无数据竞争（原子性）
❌ 不同变量间的顺序
❌ 线程间happens-before关系
适用场景：计数器、统计指标、无依赖的标志位

consume(消费):
✅ relaxed的所有保证
✅ 依赖链上的数据可见性（传递依赖）
❌ 非依赖数据的同步
❌ 完整的happens-before关系
现实状态：C++17后不推荐使用，编译器通常当作acquire处理

release/acquire(释放/获取):
✅ relaxed的所有保证
✅ 建立配对线程的happens-before关系
✅ release前的操作对acquire后的操作可见
❌ 全局顺序一致性
适用场景：锁、生产者-消费者、读写锁

seq_cst(顺序一致性):
✅ release/acquire的所有保证
✅ 所有seq_cst操作的单一全局全序
✅ 最直观的"如程序顺序"语义
❌ 性能代价（尤其在某些架构上）
适用场景：复杂同步算法、默认选择（安全第一）

===== Relaxed ordering
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Relaxed_ordering

Atomic operations tagged memory_order_relaxed are not synchronization operations; they do not impose an order among concurrent memory accesses. They only guarantee atomicity and modification order consistency.

《C++ Concurrency in Action 2nd Edition》5.3.3:
Operations on atomic types performed with relaxed ordering don’t participate in synchronizes-with relationships. Operations on the same variable within a single thread still obey happens-before relationships, but there’s almost no requirement on ordering relative to other threads. The only requirement is that accesses to a single atomic variable from the same thread can’t be reordered; once a given thread has seen a particular value of an atomic variable, a subsequent read by that thread can’t retrieve an earlier value of the variable. Without any additional synchronization, the modification order of each variable is the only thing shared between threads that are using memory_order_relaxed.

典型应用场景:
Typical use for relaxed memory ordering is incrementing counters, such as the reference counters of std::shared_ptr, since this only requires atomicity, but not ordering or synchronization (note that decrementing the std::shared_ptr counters requires acquire-release synchronization with the destructor).

[source, cpp]
----
#include <atomic>
#include <iostream>
#include <thread>
#include <vector>
 
std::atomic<int> cnt = {0};
 
void f()
{
    for (int n = 0; n < 1000; ++n)
        cnt.fetch_add(1, std::memory_order_relaxed);
}
 
int main()
{
    std::vector<std::thread> v;
    for (int n = 0; n < 10; ++n)
        v.emplace_back(f);
    for (auto& t : v)
        t.join();
    std::cout << "Final counter value is " << cnt << '\n';
}
----
输出: Final counter value is 10000

Q: relaxed的影响在哪里？
[source, cpp]
----
std::atomic<int> x{0}, y{0};

// 线程1
x.store(1, std::memory_order_relaxed);  // A
y.store(1, std::memory_order_relaxed);  // B

// 线程2
int r1 = y.load(std::memory_order_relaxed);  // C (可能看到1)
int r2 = x.load(std::memory_order_relaxed);  // D (可能看到0)
----
再有，
[source, cpp]
----
std::atomic<int> x{0};

// 线程1
x.store(1, std::memory_order_relaxed);  // A
x.store(2, std::memory_order_relaxed);  // B

// 线程2    A和B不能重排，不可能出现: r1=2同时r2=1
int r1 = x.load(std::memory_order_relaxed);  // C
int r2 = x.load(std::memory_order_relaxed);  // D
----

[source, cpp]
.《C++ Concurrency in Action 2nd Edition》Listing 5.5
----
std::atomic<bool> x,y;
std::atomic<int> z;

void write_x_then_y()
{
    x.store(true,std::memory_order_relaxed); //b
    y.store(true,std::memory_order_relaxed); //c
}

void read_y_then_x()
{
    while(!y.load(std::memory_order_relaxed)); //d
    if(x.load(std::memory_order_relaxed))      //e
        ++z;
}

int main()
{
    x=false;
    y=false;
    z=0;
    std::thread a(write_x_then_y);
    std::thread b(read_y_then_x);
    a.join();
    b.join();
    assert(z.load()!=0); //f
}
----
This time the assert f can fire, because the load of x e can read false, even though
the load of y d reads true and the store of x B happens before the store of y c. x
and y are different variables, so there are no ordering guarantees relating to the visibility of values arising from operations on each.
这次断言 f 可能会触发，因为即使对 y 的加载 d 读取到 true 并且对 x 的存储 b 发生在前，对 y 的存储 c 发生在后，对 x 的加载 e 也可能读取到 false。x 和 y 是不同的变量，因此没有任何排序保证来确保对每个变量操作产生的值的可见性。

参考:
《C++ Concurrency in Action 2nd Edition》5.3.3: UNDERSTANDING RELAXED ORDERING

===== Release-Acquire ordering
====== 概要
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Release-Acquire_ordering

If an atomic store in thread A is tagged memory_order_release, an atomic load in thread B from the same variable is tagged memory_order_acquire, and the load in thread B reads a value written by the store in thread A, then the store in thread A synchronizes-with the load in thread B.

All memory writes (including non-atomic and relaxed atomic) that happened-before the atomic store from the point of view of thread A, become visible side-effects in thread B. That is, once the atomic load is completed, thread B is guaranteed to see everything thread A wrote to memory. This promise only holds if B actually returns the value that A stored, or a value from later in the release sequence.

The synchronization is established only between the threads releasing and acquiring the same atomic variable. Other threads can see different order of memory accesses than either or both of the synchronized threads.

On strongly-ordered systems — x86, SPARC TSO, IBM mainframe, etc. — release-acquire ordering is automatic for the majority of operations. No additional CPU instructions are issued for this synchronization mode; only certain compiler optimizations are affected (e.g., the compiler is prohibited from moving non-atomic stores past the atomic store-release or performing non-atomic loads earlier than the atomic load-acquire). On weakly-ordered systems (ARM, Itanium, PowerPC), special CPU load or memory fence instructions are used.

Mutual exclusion locks, such as std::mutex or atomic spinlock, are an example of release-acquire synchronization: when the lock is released by thread A and acquired by thread B, everything that took place in the critical section (before the release) in the context of thread A has to be visible to thread B (after the acquire) which is executing the same critical section.

[source, cpp]
----
#include <atomic>
#include <cassert>
#include <string>
#include <thread>
 
std::atomic<std::string*> ptr;
int data;
 
void producer()
{
    std::string* p = new std::string("Hello");
    data = 42;
    ptr.store(p, std::memory_order_release);
}
 
void consumer()
{
    std::string* p2;
    while (!(p2 = ptr.load(std::memory_order_acquire)))
        ;
    assert(*p2 == "Hello"); // never fires
    assert(data == 42); // never fires
}
 
int main()
{
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join(); t2.join();
}
----
The following example demonstrates transitive release-acquire ordering across three threads, using a release sequence.
[source, cpp]
----
#include <atomic>
#include <cassert>
#include <thread>
#include <vector>
 
std::vector<int> data;
std::atomic<int> flag = {0};
 
void thread_1()
{
    data.push_back(42);
    flag.store(1, std::memory_order_release);
}
 
void thread_2()
{
    int expected = 1;
    // memory_order_relaxed is okay because this is an RMW,
    // and RMWs (with any ordering) following a release form a release sequence
    while (!flag.compare_exchange_strong(expected, 2, std::memory_order_relaxed))
    {
        expected = 1;
    }
}
 
void thread_3()
{
    while (flag.load(std::memory_order_acquire) < 2)
        ;
    // if we read the value 2 from the atomic flag, we see 42 in the vector
    assert(data.at(0) == 42); // will never fire
}
 
int main()
{
    std::thread a(thread_1);
    std::thread b(thread_2);
    std::thread c(thread_3);
    a.join(); b.join(); c.join();
}
----

====== acquire与release语义
windows:
https://learn.microsoft.com/en-us/windows-hardware/drivers/kernel/acquire-and-release-semantics
https://learn.microsoft.com/en-us/windows/win32/dxtecharts/lockless-programming

https://davekilian.com/acquire-release.html
A write-release guarantees that all preceding code completes before the releasing write
A read-acquire guarantees that all following code starts after the acquiring read

acquire: This is a read-acquire; block upcoming operations until this is done (but allow preceding operations to be delayed if it’s convenient).
release: This is a write-release: wait for all preceding operations to complete before doing this (but work ahead on stuff following this if it’s convenient).
acquire: 读 - 获取语义；阻止后续操作执行，直到该读取完成（但允许之前的操作根据情况延迟执行）。
release: 写 - 释放语义；等待所有之前的操作完成后再执行该写入（但允许提前处理之后的操作）。

https://preshing.com/20120913/acquire-and-release-semantics/
Acquire semantics is a property that can only apply to operations that read from shared memory, whether they are read-modify-write operations or plain loads. The operation is then considered a read-acquire. Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order.

Release semantics is a property that can only apply to operations that write to shared memory, whether they are read-modify-write operations or plain stores. The operation is then considered a write-release. Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order.

Fence Semantics
A fence semantics combines both acquire and release semantics behavior.

====== Other threads
Q: 如何理解上文中的 The synchronization is established only between the threads releasing and acquiring the same atomic variable. Other threads can see different order of memory accesses than either or both of the synchronized threads.
这种同步关系仅在释放和获取同一个原子变量的线程之间建立。其他线程所看到的内存访问顺序，可能与这两个已同步线程中的任何一个或两个所看到的顺序都不同。

如果在线程 A 中的原子存储（store）操作被标记为 memory_order_release，而线程 B 中对同一变量的原子加载（load）操作被标记为 memory_order_acquire，且线程 B 中的加载操作读取到了线程 A 存储的值，那么线程 A 的存储操作与线程 B 的加载操作之间就建立了同步关系 (synchronizes-with)。
从线程 A 的视角来看，所有在原子存储操作之前发生的内存写入（包括非原子写入和松散原子写入/relaxed atomic），在线程 B 中都会变成可见的副作用。也就是说，一旦原子加载操作完成，线程 B 保证能看到线程 A 写入内存的所有内容。这个承诺仅在以下情况下成立：线程 B 确实读取到了线程 A 存储的值，或者是该值在释放序列 (release sequence)中后续的值。
这种同步关系仅在释放和获取同一个原子变量的线程之间建立。其他线程所看到的内存访问顺序，可能与这两个已同步线程中的任何一个或两个所看到的顺序都不同。

“不同的顺序”：指的是其他线程（未参与同步的线程）观察到的内存操作（普通写入和原子写入）的先后次序，可能与同步线程（A 和 B）观察到的程序次序不一致。
原因：现代 CPU 和编译器会进行大量的指令重排优化。release 和 acquire 的作用就是在特定点上阻止这种重排跨越该点，从而在特定线程间建立一致的视图。但这个“栅栏”的效果并不自动传播给系统里的所有线程。
如果想在所有线程之间共享数据并保证一致性，通常有两种方式：
利用 release-acquire（或更强的顺序，例如锁）在所有竞争线程间建立统一的同步点。
让所有线程都通过 acquire 操作来读取某个“发布”后的值：即让所有线程都成为同步关系中的“B”。如果 C 也想安全地读取 data，它也必须对 x 进行一次 acquire 加载，并检查是否来自 A。
这句话也强调了并发编程中的一个关键且违反直觉的事实：即使两个线程通过同步正确通信，那么其他线程也未必能立即或一致地看到这个通信的结果。

[source, cpp]
----
// 普通数据
int data = 0;
// 原子变量
std::atomic<int> x(0);

// 线程 A
data = 42;                          // (1) 普通写入
x.store(1, std::memory_order_release); // (2) 释放操作

// 线程 B
int r = x.load(std::memory_order_acquire); // (3) 获取操作
if (r == 1) {
    print(data); // (4) 这里保证看到 data == 42
}

// 线程 C
print(data); // (5) 这里看到 data 可能是 0，也可能是 42，不确定！
int r2 = x.load(std::memory_order_relaxed); // (6) 这里看到 x 可能是 0 或 1
----

1. 释放-获取顺序建立的是双边关系，是“点对点”的同步，而非“全局”同步
当线程 A (Release) 与线程 B (Acquire) 成功同步时，它们之间建立了一道“逻辑屏障”。
但这种同步并不会自动广播给线程 C。如果线程 C 使用的是 memory_order_relaxed 或者没有与 A 建立同步，它可能看到 A 以完全不同的顺序发生。
2. 硬件层的视角
在现代多核 CPU 中，每个核心都有自己的存储缓冲区和无效化队列。
举例来说，
线程 A 进行了两次写入：x = 1; y = 1;（其中 y 是 release store）。
线程 B 观察到 y == 1 时，由于 acquire 语义，硬件会确保 B 刷新其缓存，从而看到 x == 1。
但是，线程 C 可能运行在另一个核心上，由于硬件拓扑结构或缓存传播的延迟，C 可能先接收到了 x = 1 的消息，过了很久才收到 y = 1 的消息；又或者，在 C 看来，这两个变量的更新顺序是乱序的。

===== Release-Consume ordering
====== 历史
release-consume 被废弃(deprecated in cpp26)的根本原因是其设计复杂性（对编译器和程序员而言）与其实践中的实际收益（性能优势）严重不匹配。最终，C++ 标准委员会选择了安全和简单，而不是极致的、脆弱的性能。

https://preshing.com/20140709/the-purpose-of-memory_order_consume-in-cpp11/
https://preshing.com/20141124/fixing-gccs-implementation-of-memory_order_consume/

https://news.ycombinator.com/item?id=36040457
memory_order_consume的初衷与动机旨在为 ARM、POWER 等弱内存序架构提供比 memory_order_acquire 更轻量级的同步选项，而非仅为已淘汰的 DEC Alpha。
在弱内存序架构如 ARM、Power 上 acquire 上需要内存屏障指令而 consume 通常不需要额外屏障。
但现实问题是编译器很难在优化时始终保持语言级别的数据依赖关系，导致标准难以实现，几乎所有编译器都将其降级实现为 acquire。

在 Linux 内核中，rcu_dereference其实就是 consume 语义在 C 语言层面的工业级实现。

====== 概要
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Release-Consume_ordering

If an atomic store in thread A is tagged memory_order_release, an atomic load in thread B from the same variable is tagged memory_order_consume, and the load in thread B reads a value written by the store in thread A, then the store in thread A is dependency-ordered before the load in thread B.

All memory writes (non-atomic and relaxed atomic) that happened-before the atomic store from the point of view of thread A, become visible side-effects within those operations in thread B into which the load operation carries dependency, that is, once the atomic load is completed, those operators and functions in thread B that use the value obtained from the load are guaranteed to see what thread A wrote to memory.
从线程 A 的视角来看，所有在原子存储之前发生的内存写入（非原子和松散原子），对于线程 B 中与加载操作存在依赖关系的操作指令是可见的。也就是说，一旦原子加载完成，线程 B 中那些使用该加载值的运算符和函数，保证能看到线程 A 写入内存的内容。

The synchronization is established only between the threads releasing and consuming the same atomic variable. Other threads can see different order of memory accesses than either or both of the synchronized threads.

On all mainstream CPUs other than DEC Alpha, dependency ordering is automatic, no additional CPU instructions are issued for this synchronization mode, only certain compiler optimizations are affected (e.g. the compiler is prohibited from performing speculative loads on the objects that are involved in the dependency chain).

Typical use cases for this ordering involve read access to rarely written concurrent data structures (routing tables, configuration, security policies, firewall rules, etc) and publisher-subscriber situations with pointer-mediated publication, that is, when the producer publishes a pointer through which the consumer can access information: there is no need to make everything else the producer wrote to memory visible to the consumer (which may be an expensive operation on weakly-ordered architectures). An example of such scenario is rcu_dereference.

[source, cpp]
----
#include <atomic>
#include <cassert>
#include <string>
#include <thread>

std::atomic<std::string*> ptr;
int data;

void producer()
{
    std::string* p = new std::string("Hello");
    data = 42;
    ptr.store(p, std::memory_order_release);
}

void consumer()
{
    std::string* p2;
    while (!(p2 = ptr.load(std::memory_order_consume)))
        ;
    assert(*p2 == "Hello"); // never fires: *p2 carries dependency from ptr
    assert(data == 42); // may or may not fire: data does not carry dependency from ptr
}

int main()
{
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join(); t2.join();
}
----

====== 应用示例
[source, cpp]
----
struct Config {
    int rule1;
    int rule2;
};

std::atomic<Config*> g_config{new Config{10, 20}};
int other_data = 0; // 与配置无关的其他数据

// --- 线程 A：生产者（发布新配置） ---
other_data = 99;                 // (1) 
Config* new_cfg = new Config{30, 40}; // (2)
g_config.store(new_cfg, std::memory_order_release); // (3) 释放操作
----

[source, cpp]
----
// --- acquire方式的线程 B：消费者 ---
Config* p = g_config.load(std::memory_order_acquire); // (4)
if (p) {
    print(p->rule1);   // (5) 必然看到 30
    print(other_data); // (6) 必然看到 99
}
----
代价：在 ARM 等架构上，acquire 会产生一个内存屏障指令（如 DMB）。这个屏障会强制 CPU 确保所有之前的写入（包括 other_data）都对 B 可见。
浪费：线程 B 其实只关心 p->rule1，它根本不关心 other_data。但为了同步 p，硬件不得不把 other_data 也给同步了，这增加了不必要的开销。

[source, cpp]
----
// --- consume方式的线程 B：消费者 ---
Config* p = g_config.load(std::memory_order_consume); // (4)
if (p) {
    print(p->rule1);   // (5) 必然看到 30（因为存在数据依赖）
    print(other_data); // (6) 可能看到 0，也可能看到 99（不确定！）
}
----
原理：p->rule1 的地址计算依赖于 p 的值（即 addr = p + offset）。这种硬件级别的数据依赖迫使 CPU 必须先加载 p 的最新值，才能计算出 rule1 的地址并读取它。
优势：在除 DEC Alpha 以外的所有现代 CPU（ARM, x86, RISC-V）上，这种“依赖加载”是硬件原生保证的，不需要任何额外的内存屏障指令。
性能：它的开销几乎等同于 relaxed，但却能保证你通过 p 访问到的结构体成员是最新的。

====== 依赖链
[source, cpp]
----
std::atomic<Node*> head;

// 有效的依赖链
Node* node = head.load(std::memory_order_consume);
if (node) {
    int value = node->data;          // ✓ 依赖 node
    Node* next = node->next;         // ✓ 依赖 node
    int x = next->value;             // ✓ 依赖 next（间接依赖 node）
}

// 无效的（依赖链断裂）
Node* node = head.load(std::memory_order_consume);
if (node) {
    // 依赖链可能断裂的情况：
    int* ptr = &node->data;          // 获取地址
    // ... 其他不相关操作 ...
    int value = *ptr;                // ✗ 可能看不到最新值，因为依赖链可能断裂
    
    // 或者通过函数参数传递
    process(node->data);             // ✓ 如果函数内联，依赖保持
    int x = node->data + global_var; // ✗ 混合了非依赖数据
}
----

===== Sequentially-consistent ordering
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Sequentially-consistent_ordering

Atomic operations tagged memory_order_seq_cst not only order memory the same way as release/acquire ordering (everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load), but also establish a single total modification order of all atomic operations that are so tagged.

顺序一致性是所有原子操作默认的内存顺序（除非显式指定其他顺序），它提供两个重要保证：
在单个线程中，所有原子操作的执行顺序与代码编写顺序一致
所有线程看到的所有原子操作都遵循同一个全局执行顺序

[source, cpp]
----
#include <atomic>
#include <thread>
#include <iostream>

std::atomic<int> x(0), y(0);
std::atomic<int> r1(0), r2(0);

void thread1() {
    x.store(1, std::memory_order_seq_cst);  // A
    r1.store(y.load(std::memory_order_seq_cst), std::memory_order_seq_cst);  // B
}

void thread2() {
    y.store(1, std::memory_order_seq_cst);  // C
    r2.store(x.load(std::memory_order_seq_cst), std::memory_order_seq_cst);  // D
}
----
在顺序一致性下，不可能出现 r1 == 0 && r2 == 0 的情况，这是因为:
全局总有一个操作（A或C）先执行
如果 A 先于 C：线程2会看到 x=1，所以 r2=1
如果 C 先于 A：线程1会看到 y=1，所以 r1=1

===== Relationship with volatile
https://en.cppreference.com/w/cpp/atomic/memory_order.html#Relationship_with_volatile
https://en.cppreference.com/w/c/atomic/memory_order.html#Relationship_with_volatile

===== thread_fence
https://en.cppreference.com/w/cpp/atomic/atomic_thread_fence.html

extern "C" void atomic_thread_fence( std::memory_order order ) noexcept;

Establishes memory synchronization ordering of non-atomic and relaxed atomic accesses, as instructed by order, without an associated atomic operation.

===== implement
1. 编译器层面禁止重排
2. CPU层面: <<C/C++11 mappings to processors>>

===== 参考
https://en.cppreference.com/w/cpp/language/memory_model
https://en.cppreference.com/w/c/language/memory_model.html
https://en.cppreference.com/w/c/atomic/memory_order
https://www.zhihu.com/question/24301047
https://cs.nju.edu.cn/xyfeng/teaching/FOPL/lectureNotes/think-cell_talk_memorymodel.pdf
UCSD CSE 160: https://cseweb.ucsd.edu/classes/wi17/cse160-a/index/lecture_index.html

==== java
https://docs.oracle.com/javase/specs/jls/se21/html/jls-17.html#jls-17.4
http://www.cs.umd.edu/~pugh/java/memoryModel/index.html
https://github.com/orientye/understand/blob/main/lan/java.asc#Java-Memory-Model-and-Thread

==== rust
https://doc.rust-lang.org/nomicon/atomics.html
https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html

==== csharp
Thread.MemoryBarrier():
This method issues a full memory barrier (or full-fence), meaning all reads and writes before the call must be visible to other processors before any reads and writes after the call can become visible.

volatile keyword: Marks a field such that every read has acquire semantics (no subsequent memory access can be reordered before the read) and every write has release semantics (no preceding memory access can be reordered after the write). This provides half-fences, which are often sufficient for simple flag signaling between threads.
Volatile.Read: Performs a read operation with acquire semantics, ensuring that subsequent memory operations cannot be reordered before this read.
Volatile.Write: Performs a write operation with release semantics, ensuring that preceding memory operations cannot be reordered after this write.

lock statement and Monitor class: These synchronization primitives create full memory barriers on entry and exit, handling the complexities of memory synchronization automatically.

Interlocked class methods: Operations like Interlocked.Increment() or Interlocked.CompareExchange() are atomic operations that also include implicit full memory barriers, ensuring the operation is treated as a single, non-interruptible action that is visible to all threads.

==== correctness
link:../lock-free.asc#correctness[correctness]

==== 工业实践
https://github.com/facebook/rocksdb/blob/master/memtable/skiplist.h
https://github.com/apache/incubator-brpc/blob/master/src/bthread/work_stealing_queue.h
https://elixir.bootlin.com/linux/latest/source/include/linux/maple_tree.h

==== 参考
《Shared Memory Consistency Models: A Tutorial》
https://github.com/MattPD/cpplinks/blob/master/atomics.lockfree.memory_model.md
https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/
perfbook: Chapter 15 Advanced Synchronization: Memory Ordering
https://www.hboehm.info/c++mm/
