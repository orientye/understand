= AI
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:homepage: http://orientye.com

== 概览

=== 概念
- 分类
机器学习(machine learning)
深度学习(deep learning)
监督学习(supervised learning)与无监督学习(unsupervised learning)
强化学习(reinforcement learning)
Q: 机器学习 vs. 统计学
参考:
https://www.zhihu.com/question/279973545
关系图: https://arxiv.org/pdf/1810.06339

- 张量(tensor)
n维数组，也称为张量(tensor)。
无论使用哪个深度学习框架，其张量类(在PyTorch和TensorFlow中为Tensor)都与Numpy的ndarray类似。但深度学习框架又比Numpy的ndarray多了一些重要功能：首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；其次，张量类支持自动微分。
深度学习操作的主要对象是张量。它提供了基本数学运算、广播、索引、切片、内存节省和转换其他Python对象等功能。

- AIGC
人工智能生成内容(Artificial Intelligence Generated Content)

- 大模型
大模型通常指的是在机器学习和深度学习领域中，具有大量参数和复杂结构的模型。这些模型通常需要大量的数据和计算资源进行训练，能够处理复杂的任务，如自然语言处理、图像识别、语音识别视频处理等。

- 大模型蒸馏技术
Model Distillation是一种模型压缩方法，旨在将大型、复杂的深度学习模型(通常称为教师模型)的知识转移到一个较小的、效率更高的模型(称为学生模型)中。通过这种方式，学生模型可以在保持较高性能的同时，减少计算资源的需求和推理时间。

== 机器学习

=== 概念
==== 学习
在机器学习中，学习(learning)是一个训练模型的过程。通过这个过程，发现正确的参数集，从而使模型强制执行所需的行为。换句话说，用数据训练(train)模型。

训练过程通常包含如下步骤:

    从一个随机初始化参数的模型开始，这个模型基本没有“智能”；
    获取一些数据样本；
    调整参数，使模型在这些样本中表现得更好；
    重复第(2)步和第(3)步，直到模型在任务中的表现令人满意。

==== 核心组件
- 组成

    可以用来学习的数据(data)
    如何转换数据的模型(model)
    一个目标函数(objective function)，用来量化模型的有效性
    调整模型参数以优化目标函数的算法(algorithm)

- 数据
理想的数据: 相同的维数(数据维数dimensionality)、海量、正确

- 模型
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习。

- 目标函数
即数据集通常可以分成两部分：训练数据集(training dataset，或称为训练集(training set))用于拟合模型参数，测试数据集(test dataset，或称为测试集(test set))用于评估拟合的模型。
验证集:
验证数据集(validation dataset): https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id6
过拟合(overfitting):
当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为过拟合的。就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。
正则化(regularization):
用于对抗过拟合的技术称为正则化。
欠拟合(underfitting):
欠拟合是机器学习中的一个常见问题，它发生在模型在训练数据上表现得不够好，即模型未能捕捉到数据中的潜在规律或模式，导致在训练集和测试集上都表现不佳。
参考: https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html

- 优化算法
当获得了一些数据源及其表示、一个模型和一个合适的损失函数，接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。深度学习中，大多流行的优化算法通常基于一种基本方法–梯度下降(gradient descent)。简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。然后，它在可以减少损失的方向上优化参数。

=== 监督学习
==== 概念
监督学习是机器学习的一种方法，可以由训练资料中学到或建立一个模式(函数/learning model)，并依此模式推测新的实例。训练资料是由输入对象(通常是向量)和预期输出所组成。函数的输出可以是一个连续的值(称为回归分析)，或是预测一个分类标签(称作分类)。

    回归(regression):
        主要用于预测数值型数据
        应用实例:
            股票价格波动的预测，房屋价格的预测等

    分类(classification):
        将实例数据划分到合适的类别中
        应用实例:
            判断网站是否被黑客入侵(二分类)，手写数字的自动识别(多分类)

==== 应用场景
回归
分类
标记问题
搜索
推荐系统
序列学习

==== 线性回归
===== 概念
回归(regression):
回归是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
在机器学习领域中的大多数任务通常都与预测(prediction)有关。

线性回归(linear regression):
可以追溯到19世纪初，它在回归的各种标准工具中最简单而且最流行。
线性回归基于几个简单的假设：首先，假设自变量和因变量之间的关系是线性的，即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声；其次，假设任何噪声都比较正常，如噪声遵循正态分布。

线性模型:
严格来说，有时候输入特征其实是一个仿射变换(affine transformation)。仿射变换的特点是通过加权和对特征进行线性变换(linear transformation)，并通过偏置项(偏置bias、偏移量offset或截距intercept)来进行平移(translation)。

代价函数(cost function/lost function损失函数):
损失函数能够量化目标的实际值与预测值之间的差距。通常会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
回归问题中最常用的损失函数是平方误差函数。

线性回归与深度网络:
可以将线性回归是描述为一个单层神经网络

===== 基本元素
====== 解析解
analytical solution
像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

===== 实现
====== 生成数据
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id2
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id2

====== 读取数据集
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id3
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id3

====== 初始化模型参数
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id4
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id5

====== 定义模型
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id5
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id4

====== 定义损失函数
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id6
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id6

====== 定义优化算法
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id7
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id7

====== 训练
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id8
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id8

==== 梯度下降
梯度下降(gradient descent):
梯度下降和梯度下降的变体不仅用于训练线性回归，还用于训练所有AI中一些最大和最复杂的模型。
梯度下降几乎可以优化所有深度学习模型。它通过不断地在损失函数递减的方向上更新参数来降低误差。

学习率(learning rate)alpha

实际中的执行可能会非常慢：因为在每一次更新参数之前，必须遍历整个数据集。因此，通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降(minibatch stochastic gradient descent)。

==== softmax回归
===== 概念
softmax regression，也称为多类逻辑回归(multinomial logistic regression)，是一种用于多分类问题的分类算法。
softmax回归是一种多分类的线性分类模型，它是逻辑回归在多分类问题上的推广。其输出是一个概率分布，表示样本属于各个不同类别的概率。

===== 分类问题

===== 网络架构

===== 全连接层的参数开销

===== softmax运算
softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。

===== 小批量样本的矢量化

===== 损失函数
- 对数似然
https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id8

- softmax及其导数
https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#subsec-softmax-and-derivatives

- 交叉熵损失
https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id10

===== 信息论基础
- 熵
信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布的熵(entropy)。

- 信息量

===== 实现
https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html

https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html

==== 图像分类数据集
https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html

==== 参考
https://zh.d2l.ai/chapter_linear-networks/index.html

=== 无监督学习
==== 概念
无监督学习是机器学习的一种方法，其目标是在不提供监督信息(如预定义的标签或结果)的条件下，通过学习未标记的数据来发现数据的内在结构和关系。

无监督学习的主要运用包含：聚类分析(cluster analysis)、关系规则(association rule)、维度缩减(dimensionality reduce)。它是监督式学习和强化学习等策略之外的一种选择。

一个常见的无监督学习是数据聚类。在人工神经网络中，生成对抗网络(GAN)、自组织映射(SOM)和适应性共振理论(ART)则是最常用的非监督式学习。

优势:
(1) 不需要标记数据：获取标记数据往往需要大量的人力、物力和时间成本。
(2) 发现新的知识和结构：能够揭示数据内部隐藏的结构和模式，这些信息可能是在预先标记的情况下无法发现的。例如，可以发现新的客户群体或者数据中的潜在关联。

局限性:
(1) 结果解释性相对较弱：由于没有明确的目标变量，无监督学习的结果可能比较难以解释。例如，在聚类任务中，很难确定划分出的簇在实际业务中的具体含义。
(2) 评估指标不明确：相比于监督学习有准确率、召回率等明确的评估指标，无监督学习的评估指标比较复杂，并且因任务而异。例如，对于聚类任务，评估聚类质量的指标有轮廓系数、DBI(Davies - Bouldin Index)等，但这些指标的解释和选择也需要根据具体情况而定。

== 强化学习
=== 概念
在强化学习问题中，智能体(agent)在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察(observation)，并且必须选择一个动作(action)，然后通过某种机制(有时称为执行器)将其传输回环境，最后智能体从环境中获得奖励(reward)。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程(markov decision process)。当状态不依赖于之前的操作时，称该问题为上下文赌博机(contextual bandit problem)。当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机(multi-armed bandit problem)。

=== 发展历史
1970年代-1980年代
强化学习的基础 Richard Sutton(强化学习奠基人)和Christopher Watkins等人提出了自适应动态规划(Adaptive Dynamic Programming)和Q-learning等基本方法。

1990年代-2000年代
Christopher Watkins提出了基于差分学习的Q-learning算法，是现代强化学习的重要里程碑。
Ronald J. Williams引入了基于梯度的强化学习算法，即策略梯度方法，为后来的进一步发展奠定了基础。

2013年以来 深度强化学习时代
通过深度神经网络的引入，强化学习得到了重大的推动和突破。
2013年 Deep Q-Network(DQN)是由DeepMind提出的一种结合深度神经网络和Q-learning的算法，首次实现了在Atari游戏中超越人类水平的表现。
2015年: AlphaGo

挑战: 如样本效率、探索与利用的平衡、通用性和可解释性等问题。

=== Markov Decision Process(MDP)
https://en.d2l.ai/chapter_reinforcement-learning/mdp.html

=== Value Iteration
https://en.d2l.ai/chapter_reinforcement-learning/value-iter.html

=== Q-Learning
https://en.d2l.ai/chapter_reinforcement-learning/qlearning.html

== 深度学习
=== 概念
- 发展
2010年开始
(1)大量数据
(2)廉价又高质量的传感器、廉价的数据存储以及廉价计算的普及，特别是GPU的普及

- 卷积神经网络(convolutional neural network, CNN)
主要用于处理图像数据。
它通过卷积层和池化层自动提取图像的特征，从而实现图像分类、目标检测等任务。

- 循环神经网络(recurrent neural network, RNN)
适用于处理序列数据，如文本、时间序列数据等。
它能够记住之前的信息，并利用这些信息来处理当前的输入。
长短期记忆网络(LSTM)和门控循环单元(GRU)是常见的 RNN 变体，能够更好地处理长序列数据中的长期依赖关系。

- 生成对抗网络(generative adversarial network, GAN)
由生成器(generator)和判别器(discriminator)组成。
生成器试图生成逼真的假数据，而判别器则试图区分真实数据和生成器生成的数据。
通过不断的对抗训练，生成器和判别器的性能都不断提高。

=== 多层感知机
==== 概念
Multilayer Perceptrons，即MLP
最简单的深度网络称为多层感知机。多层感知机由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。

多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。

常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

应用领域：
分类任务：MLP可以用于各种分类任务，如手写数字识别、图像分类、文本分类等。
回归任务：也可用于回归任务，如预测股票价格、气温变化等。通过对历史数据的学习，MLP能够建立输入特征(如时间序列数据、相关经济指标等)与目标变量(股票价格、气温等)之间的关系，从而进行预测。

==== 隐藏层
- 线性模型可能会出错
- 在网络中加入隐藏层
- 从线性到非线性
- 通用近似定理

==== 激活函数
激活函数(activation function)通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。

===== ReLU函数
最受欢迎的激活函数是修正线性单元(rectified linear unit，ReLU)，它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的非线性变换。给定元素x，ReLU函数被定义为该元素与的最大值:

    ReLU(x) = max(x, 0)

通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。

使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。

注意，ReLU函数有许多变体，包括参数化ReLU(Parameterized ReLU，pReLU)函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过:

    ReLU(x) = max(x, 0) + a * min(x, 0)

===== sigmoid函数
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html#sigmoid

===== tanh函数
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html#tanh

==== 实现
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html

==== 权重衰减(weight decay)
https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html

==== 暂退法(dropout)
暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。之所以被称为暂退法，因为从表面上看是在训练过程中丢弃(drop out)一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

    暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元
    暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用
    暂退法将活性值h替换为具有期望值的随机变量
    暂退法仅在训练期间使用

https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html

==== 前向传播、反向传播和计算图
- 前向传播
前向传播(forward propagation或forward pass): 按顺序(从输入层到输出层)计算和存储神经网络中每层的结果。
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2

- 反向传播
反向传播(backward propagation或backpropagation)指的是计算神经网络参数梯度的方法。根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4

- 训练神经网络
在训练神经网络时，前向传播和反向传播相互依赖。
在初始化模型参数后，交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是需要保留中间值，直到反向传播完成。这也是训练比单纯的预测需要更多的内存(显存)的原因之一。

- 参考
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html

==== 数值稳定性和模型初始化
- 梯度消失
vanishing gradient
在深度神经网络(尤其是使用反向传播算法进行训练的网络)中，梯度消失是指在反向传播过程中，梯度(用于更新网络权重的信号)随着网络层数的增加而变得越来越小，最终接近于零的现象。

- 梯度爆炸
exploding gradient
与梯度消失相反，梯度爆炸是指在反向传播过程中，梯度随着网络层数的增加而变得越来越大，最终导致数值溢出的现象。

- 参数初始化
https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#id6

https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html

==== 境和分布偏移
环境偏移:
Environment Shift
指模型的训练环境和实际应用环境之间存在差异。例如，在自动驾驶汽车的目标检测模型训练中，训练数据是在晴天收集的，而实际应用时可能会遇到雨天、雾天等不同天气状况，这种从晴天环境到其他天气环境的变化就是环境偏移。

分布偏移:
Distribution Shift
指数据分布在不同阶段(如训练阶段和测试阶段)发生了变化。具体来说，数据的输入特征、标签等的联合分布(其中是输入特征，是标签)在训练和测试过程中不再相同。以图像分类为例，训练集中猫的图像可能主要是某种品种且拍摄角度、背景比较单一，而测试集中猫的图像品种更多样、拍摄角度和背景也更复杂，这就导致了分布偏移。

https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html

=== 深度学习计算
==== 层和块
一个块可以由许多层组成；一个块可以由许多块组成。
块可以包含代码。
块负责大量的内部处理，包括参数初始化和反向传播。
层和块的顺序连接由Sequential块处理。

- 自定义块
https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#id3

- 顺序块
Sequential的设计是为了把其他模块串起来。
https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#id4

- 在前向传播函数中执行代码
https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#id5

==== 参数管理
- 参数访问
https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html#id2

- 参数初始化
https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html#id6

- 参数绑定
https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html#id9

==== 延后初始化
defers initialization，即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。
https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html

==== 自定义层
- 不带参数的层
https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html#id2

- 带参数的层
https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html#id3

==== 读写文件
- 加载和保存张量
https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html#id2

- 加载和保存模型参数
https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html#id3

==== GPU
- 计算设备
https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html#id1

- 张量与GPU
https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html#id2

- 神经网络与GPU
https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html#id6

=== 卷积神经网络(Convolutional Neural Networks)
==== 从全连接层到卷积
===== 不变性
卷积神经网络将空间不变性(spatial invariance)的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。
平移不变性(translation invariance)：
不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
局部性(locality)：
神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。
https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id2

===== 多层感知机的限制
- 平移不变性
https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id4
图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。

- 局部性
https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id5
局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。

===== 卷积
https://en.wikipedia.org/wiki/Convolution

在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。

卷积神经网络是一类特殊的神经网络，它可以包含多个卷积层。

https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id6

===== 通道(Channels)
https://en.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#channels

多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

==== 图像卷积(Convolutions for Images)
===== 互相关运算
卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算(cross-correlation)，而不是卷积运算。
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id2

===== 卷积层
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id3

===== 图像中目标的边缘检测
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id4

===== 学习卷积核
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id5

===== 互相关和卷积
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id6

===== Feature Map and Receptive Field
输出的卷积层有时被称为特征映射feature map，因为它可以被视为一个输入映射到下一层的空间维度的转换器。
在卷积神经网络中，对于某一层的任意元素，其receptive field是指在前向传播期间可能影响计算的所有元素(来自所有先前层)。
https://en.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#feature-map-and-receptive-field

==== 填充和步幅(Padding and Stride)
填充和步幅可用于有效地调整数据的维度。

- 填充
填充可以增加输出的高度和宽度。常用来使输出与输入具有相同的高和宽。
https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#id2

- 步幅
步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n。
https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#id3

==== 多输入多输出通道(Multiple Input and Multiple Output Channels)
- 多输入通道
https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#id2

- 多输出通道
https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#id3

- 1 X 1 卷积层
当以每像素为基础应用时，1 X 1卷积层相当于全连接层。
1 X 1卷积层通常用于调整网络层的通道数量和控制模型复杂性。
https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#times-1

==== 汇聚层(Pooling)
汇聚(pooling)层: 降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

- 最大汇聚层(maximum pooling)和平均汇聚层(average pooling)
https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html#id2

- 填充和步幅
https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html#id3

- 多个通道
https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html#id4

==== LeNet
LeNet是最早发布的卷积神经网络之一。
https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html

=== 现代卷积神经网络(Modern Convolutional Neural Networks)
==== AlexNet
2012年

- Representation Learning(学习表征)
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id3

- 模型设计
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id12

- 激活函数
Q: 为什么AlexNet将sigmoid激活函数改为更简单的ReLU激活函数？
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id13

- 容量控制和预处理
AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。这使得模型更健壮，更大的样本量有效地减少了过拟合。
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id14

- 读取数据集
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id15

- 训练AlexNet
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id16

==== VGG
2014

===== VGG块
https://zh.d2l.ai/chapter_convolutional-modern/vgg.html#id1

===== VGG网络
https://zh.d2l.ai/chapter_convolutional-modern/vgg.html#id3

==== NiN(Network in network)
2013
LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。
AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。网络中的网络(NiN)提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。

===== NiN块
NiN使用由一个卷积层和多个1X1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
https://zh.d2l.ai/chapter_convolutional-modern/nin.html#id2

===== NiN模型
NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层(即在所有位置上进行求和)。该汇聚层通道数量为所需的输出数量)。
移除全连接层可减少过拟合，同时显著减少NiN的参数。
https://zh.d2l.ai/chapter_convolutional-modern/nin.html#id3

==== Multi-Branch Networks(GoogLeNet)
2014

===== Inception块
https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html#inception

===== GoogLeNet模型
https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html#id2

==== 批量规范化(batch normalization)
2015

- 训练深层网络
批量规范化应用于单个可选层(也可以应用到所有层)，其原理如下：在每次训练迭代中，首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。
批量规范化层在训练模式(通过小批量统计数据规范化)和预测模式(通过数据集统计规范化)中的功能不同。在训练过程中，无法得知使用整个数据集来估计平均值和方差，因此只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id3

- 批量规范化层
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id6

- 实现
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id10
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id11

==== 残差网络(ResNet)
- 函数类
https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id1

- 残差块
https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id3

- ResNet模型
https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id4

==== 稠密连接网络(DenseNet)
2017
稠密连接网络在某种程度上是ResNet的逻辑扩展。

=== 循环神经网络(Recurrent Neural Networks)
==== 序列模型
===== 自回归模型(Autoregressive Models)
- 特点
自回归模型能够有效地捕捉时间序列数据中的线性关系。
通过最小化预测误差来估计模型参数，常用的方法包括最小二乘法。
可以用于预测时间序列的未来值，为动态系统的理解和预测提供有力工具。
线性关系：AR模型假定当前观测值与过去观测值之间存在线性关系。
平稳性：对于AR模型，通常要求时间序列是平稳的，意味着它的统计特性（如均值和方差）不随时间变化。非平稳序列可能需要通过差分等方法转换成平稳序列后再应用AR模型。
残差独立同分布：模型中的误差项应满足独立同分布条件，这保证了估计的有效性和预测的准确性。

- 限制
自回归模型通常假设时间序列是平稳的，即统计特性（如均值、方差）不随时间变化。如果时间序列不满足平稳性条件，可能需要进行适当的变换或差分处理。
自回归模型的阶数选择是一个重要问题。阶数过高可能导致模型过于复杂，增加计算量和过拟合风险；阶数过低则可能无法充分捕捉时间序列的动态特性。因此，需要在模型的拟合度和过拟合风险之间取得平衡。
自回归模型只适用于预测与自身前期相关的经济现象或受自身历史因素影响较大的现象，如矿的开采量、各种自然资源产量等。对于受社会因素影响较大的经济现象或复杂系统，可能需要考虑其他更复杂的模型或方法。

https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id5

===== 马尔可夫模型(Markov Model)
马尔可夫模型是一类用于描述具有马尔可夫性质的随机过程的概率模型。在这样的过程中，未来的状态仅依赖于当前的状态，而不受之前状态的影响。这种特性被称为“无记忆性”或“马尔可夫性质”。

https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id6

===== The Order of Decoding
https://en.d2l.ai/chapter_recurrent-neural-networks/sequence.html#the-order-of-decoding

===== 训练与预测
https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id10
https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id11

==== 文本预处理
https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html#sec-text-preprocessing

==== 语言模型和数据集
https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html

==== 循环神经网络
隐状态(hidden state)，也称为隐藏变量(hidden variable)，它存储了到时间步t-1的序列信息。

隐藏层和隐状态指的是两个截然不同的概念:
隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层；
隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入，并且这些状态只能通过先前时间步的数据来计算。

循环神经网络是具有隐状态的神经网络。

===== 无隐状态的神经网络
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#id2

===== 隐状态的循环神经网络
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-rnn-w-hidden-states

===== 基于循环神经网络的字符级语言模型
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#id4

===== 困惑度(Perplexity)
TODO: 补充计算

困惑度的最好的理解是下一个词元的实际选择数的调和平均数:
在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量。事实上，如果在没有任何压缩的情况下存储序列，这将是能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。

https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#perplexity

==== 实现
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html

==== 通过时间反向传播(Backpropagation Through Time)
通过时间反向传播仅仅适用于反向传播在具有隐状态的序列模型。
截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。
矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。
为了计算的效率，通过时间反向传播在计算期间会缓存中间值。

===== 循环神经网络的梯度分析
https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#subsec-bptt-analysis

===== 通过时间反向传播的细节
https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#id10

=== 现代循环神经网络
循环神经网络在实践中一个常见问题是数值不稳定性。尽管已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型来进一步处理它。

==== 长短期记忆网络(long short-term memory, LSTM)
1997
长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。
解决这一问题的最早方法之一是长短期存储器(LSTM)，它有许多与门控循环单元一样的属性。
长短期记忆网络的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年。

长短期记忆网络有三种类型的门: 输入门、遗忘门和输出门。
长短期记忆网络的隐藏层输出包括"隐状态"和"记忆元"。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
长短期记忆网络可以缓解梯度消失和梯度爆炸。

===== 门控记忆元(Gated Memory Cell)
https://en.d2l.ai/chapter_recurrent-modern/lstm.html#gated-memory-cell
https://zh.d2l.ai/chapter_recurrent-modern/lstm.html#id2

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/lstm.html#implementation-from-scratch
https://zh.d2l.ai/chapter_recurrent-modern/lstm.html#id7

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/lstm.html#concise-implementation
https://zh.d2l.ai/chapter_recurrent-modern/lstm.html#id11

==== 门控循环单元(Gated Recurrent Unit, GRU)
2014
门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。
重置门有助于捕获序列中的短期依赖关系。
更新门有助于捕获序列中的长期依赖关系。
重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。

===== Reset Gate and Update Gate
https://en.d2l.ai/chapter_recurrent-modern/gru.html#reset-gate-and-update-gate
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id5

===== Candidate Hidden State
https://en.d2l.ai/chapter_recurrent-modern/gru.html#candidate-hidden-state
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id6

===== Hidden State
https://en.d2l.ai/chapter_recurrent-modern/gru.html#hidden-state
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id7

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/gru.html#implementation-from-scratch
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id8

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/gru.html#concise-implementation
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id12

==== 深度循环神经网络(Deep Recurrent Neural Networks)
在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。
有许多不同风格的深度循环神经网络，如长短期记忆网络、门控循环单元、或经典循环神经网络。这些模型在深度学习框架的高级API中都有涵盖。
总体而言，深度循环神经网络需要大量的调参（如学习率和修剪）来确保合适的收敛，模型的初始化也需要谨慎。

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/deep-rnn.html#implementation-from-scratch

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/deep-rnn.html#concise-implementation

==== 双向循环神经网络(Bidirectional Recurrent Neural Networks)
在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。
双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。
双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。
由于梯度链更长，因此双向循环神经网络的训练代价非常高。

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/bi-rnn.html#implementation-from-scratch

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/bi-rnn.html#concise-implementation

==== 机器翻译与数据集(Machine Translation and the Dataset)
机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。
使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，可以将低频词元视为相同的未知词元。
通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。

https://en.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html
https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html

==== 编码器-解码器架构(The Encoder–Decoder Architecture)
编码器-解码器架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
解码器将具有固定形状的编码状态映射为长度可变的序列。
https://en.d2l.ai/chapter_recurrent-modern/encoder-decoder.html
https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html

==== 序列到序列学习(Sequence-to-Sequence Learning for Machine Translation)
===== Teacher Forcing
https://en.wikipedia.org/wiki/Teacher_forcing
Teacher Forcing是一种用来训练循环神经网络模型的方法，这种方法以上一时刻的输出作为下一时刻的输入。
在训练时，Teacher forcing是通过使用第t时刻的来自于训练集的期望输出y(t)作为下一时刻的输入x(t+1)，而不是直接使用网络的实际输出。
Q: 优点与缺点？
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#teacher-forcing

===== Encoder
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#encoder

===== Decoder
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#decoder

===== Encoder–Decoder for Sequence-to-Sequence Learning
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#encoderdecoder-for-sequence-to-sequence-learning

===== Loss Function with Masking
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#loss-function-with-masking

===== Training
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#training

===== Prediction
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#prediction

===== Evaluation of Predicted Sequences
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#evaluation-of-predicted-sequences

==== 束搜索(Beam Search)
===== 概念
https://zhuanlan.zhihu.com/p/82829880

===== Greedy Search
https://en.d2l.ai/chapter_recurrent-modern/beam-search.html#greedy-search
https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html#id2

===== Exhaustive Search(穷举搜索)
https://en.d2l.ai/chapter_recurrent-modern/beam-search.html#exhaustive-search
https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html#id3

===== Beam Search
https://en.d2l.ai/chapter_recurrent-modern/beam-search.html#id1
https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html#id5

=== Attention Mechanisms and Transformers
==== 概念
https://zhuanlan.zhihu.com/p/104393915

注意力机制的核心思想是:
在处理一个输入（如一段文本、图像等）时，并不是所有信息都同等重要。
通过赋予输入中不同部分不同的权重（即注意力权重），模型可以专注于更加重要的部分，从而提高效率和效果。

==== Queries, Keys, and Values

==== Attention Scoring Functions

==== The Bahdanau Attention Mechanism(Bahdanau注意力)

==== Multi-Head Attention(多头注意力)

==== Self-Attention and Positional Encoding(自注意力与位置编码)

==== The Transformer Architecture

==== Transformers for Vision

==== Large-Scale Pretraining with Transformers

=== 优化算法
==== Optimization and Deep Learning

==== Convexity(凸性)

==== Gradient Descent(梯度下降)

==== Stochastic Gradient Descent(梯度下降)

==== Minibatch Stochastic Gradient Descent(小批量随机梯度下降)

==== Momentum(动量法)

==== Adagrad

==== RMSProp

==== Adam

==== Learning Rate Scheduling(学习率调度器)

=== 计算性能
==== Compilers and Interpreters
==== Asynchronous Computation
==== Automatic Parallelism
==== Hardware
==== Training on Multiple GPUs
==== Concise Implementation for Multiple GPUs
==== Parameter Servers

=== 计算机视觉
==== Image Augmentation(图像增广)
==== Fine-Tuning(微调)
==== Object Detection and Bounding Boxes
==== Anchor Boxes

==== Multiscale Object Detection(多尺度目标检测)

==== The Object Detection Dataset

==== Single Shot Multibox Detection(单发多框检测)

==== Region-based CNNs (R-CNNs)

==== Semantic Segmentation and the Dataset

==== Transposed Convolution(转置卷积)

==== Fully Convolutional Networks

==== Neural Style Transfer

==== Image Classification (CIFAR-10) on Kaggle

==== Dog Breed Identification (ImageNet Dogs) on Kaggle

=== 自然语言处理(Natural Language Processing)
==== Pretraining(预训练)

==== Applications(应用)

=== Recommender Systems
https://en.d2l.ai/chapter_recommender-systems/index.html

=== Gaussian Processes
==== 介绍
https://en.wikipedia.org/wiki/Gaussian_process
https://en.d2l.ai/chapter_gaussian-processes/gp-intro.html

==== Gaussian Process Priors
https://en.d2l.ai/chapter_gaussian-processes/gp-priors.html

==== Gaussian Process Inference
https://en.d2l.ai/chapter_gaussian-processes/gp-inference.html

=== Hyperparameter Optimization
==== What Is Hyperparameter Optimization?
==== Hyperparameter Optimization API
==== Asynchronous Random Search
==== Multi-Fidelity Hyperparameter Optimization
==== Asynchronous Successive Halving

=== Generative Adversarial Networks(生成对抗网络)
2014
Generative Adversarial Networks (GANs) are a class of machine learning models introduced by Ian Goodfellow and his colleagues in 2014. GANs are used for generating synthetic data that resembles a given real dataset. They consist of two neural networks: a Generator and a Discriminator. These two networks are trained together in a competitive process.

==== Generative Adversarial Networks
===== Generate Some “Real” Data
===== Generator
===== Discriminator
===== Training

==== Deep Convolutional Generative Adversarial Networks
===== The Pokemon Dataset
===== The Generator
===== Discriminator
===== Training

== 工具
- Jupyter Notebook
Jupyter vs. IDE

- colab
https://colab.research.google.com/
https://www.geeksforgeeks.org/google-collab-vs-jupyter-notebook/

- GPU
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html

- kaggle
https://www.kaggle.com/

参考:
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html

== 框架/库
caff2:
2018年3月底，Caffe2并入了Pytorch。

CNTK:
2019年4月26日发布2.7.0稳定版后，微软对CNTK的更新逐渐减少，并开始将更多的精力投入到与PyTorch等框架的合作和整合上，CNTK也逐渐被一些新的技术和框架所替代。

chainer:
2019年12月，Preferred Networks宣布将其深度学习研究平台的开发工作从Chainer转移到PyTorch。

mxnet:
2022年底，MXNet的代码开发大部分停止，社区参与度放缓。2022年9月MXNet从Apache孵化器毕业成为顶级项目，但在同年11月却被移入Apache Attic，进入“只读”阶段，意味着该项目不再重建社区、修正Bug、发布新版本，正式宣告退休。

https://github.com/pytorch
https://github.com/tensorflow/tensorflow
https://github.com/keras-team/keras

参考:
https://www.zhihu.com/question/46587833

== project
https://github.com/vietnh1009/Super-mario-bros-PPO-pytorch
https://github.com/karpathy/nanoGPT
https://github.com/openai

https://github.com/meta-llama
https://github.com/deepseek-ai
https://github.com/QwenLM

== 参考
https://en.d2l.ai/
https://zh.d2l.ai/
https://space.bilibili.com/1567748478
https://www.bilibili.com/video/BV1Bq421A74G/
https://www.bilibili.com/video/BV1Wv411h7kN/
https://www.coursera.org/specializations/machine-learning-introduction#courses
https://cs231n.stanford.edu/
https://github.com/tangyudi/Ai-Learn
https://github.com/apachecn/ai-roadmap/
https://github.com/mahseema/awesome-ai-tools
https://www.zhihu.com/question/56952345/answer/632233718
https://github.com/ahkarami/Great-Deep-Learning-Books
《Deep Learning》https://www.deeplearningbook.org/
《Grokking Deep Learning》Andrew W. Trask
《Deep Learning with Python》Francois Chollet
《Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow》3rd Edition
《Reinforcement Learning: An Introduction》2nd Edition
《Deep Reinforcement Learning Hands-On》