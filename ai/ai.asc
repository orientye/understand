= AI
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:homepage: http://orientye.com

== 概览

=== 概念
- 分类
机器学习(machine learning)
深度学习(deep learning)
监督学习(supervised learning)与无监督学习(unsupervised learning)
强化学习(reinforcement learning)
参考:
https://www.zhihu.com/question/279973545
https://arxiv.org/pdf/1810.06339

- 张量(tensor)
n维数组，也称为张量(tensor)。
无论使用哪个深度学习框架，其张量类(在PyTorch和TensorFlow中为Tensor)都与Numpy的ndarray类似。但深度学习框架又比Numpy的ndarray多了一些重要功能：首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；其次，张量类支持自动微分。
深度学习操作的主要对象是张量。它提供了基本数学运算、广播、索引、切片、内存节省和转换其他Python对象等功能。

- AIGC
人工智能生成内容(Artificial Intelligence Generated Content)

== 机器学习

=== 概念
==== 学习
在机器学习中，学习(learning)是一个训练模型的过程。通过这个过程，发现正确的参数集，从而使模型强制执行所需的行为。换句话说，用数据训练(train)模型。

训练过程通常包含如下步骤：

    从一个随机初始化参数的模型开始，这个模型基本没有“智能”；
    获取一些数据样本；
    调整参数，使模型在这些样本中表现得更好；
    重复第(2)步和第(3)步，直到模型在任务中的表现令人满意。

==== 核心组件
可以用来学习的数据(data)；
如何转换数据的模型(model)；
一个目标函数(objective function)，用来量化模型的有效性；
调整模型参数以优化目标函数的算法(algorithm)。

数据:
理想的数据: 相同的维数(数据维数dimensionality)、海量、正确

模型:
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习。

目标函数:
即数据集通常可以分成两部分：训练数据集(training dataset，或称为训练集(training set))用于拟合模型参数，测试数据集(test dataset，或称为测试集(test set))用于评估拟合的模型。当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为过拟合(overfitting)的。就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。

优化算法:
当获得了一些数据源及其表示、一个模型和一个合适的损失函数，接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。深度学习中，大多流行的优化算法通常基于一种基本方法–梯度下降(gradient descent)。简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。然后，它在可以减少损失的方向上优化参数。

=== 监督学习
==== 概念
监督学习是机器学习的一种方法，可以由训练资料中学到或建立一个模式(函数/learning model)，并依此模式推测新的实例。训练资料是由输入对象(通常是向量)和预期输出所组成。函数的输出可以是一个连续的值(称为回归分析)，或是预测一个分类标签(称作分类)。

    回归(regression):
        主要用于预测数值型数据
        应用实例:
            股票价格波动的预测，房屋价格的预测等

    分类(classification):
        将实例数据划分到合适的类别中
        应用实例:
            判断网站是否被黑客入侵(二分类)，手写数字的自动识别(多分类)

==== 应用场景
回归
分类
标记问题
搜索
推荐系统
序列学习

==== 线性回归
===== 概念
回归(regression):
回归是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
在机器学习领域中的大多数任务通常都与预测(prediction)有关。

线性回归(linear regression):
可以追溯到19世纪初，它在回归的各种标准工具中最简单而且最流行。
线性回归基于几个简单的假设：首先，假设自变量和因变量之间的关系是线性的，即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声；其次，假设任何噪声都比较正常，如噪声遵循正态分布。

线性模型:
严格来说，有时候输入特征其实是一个仿射变换（affine transformation）。仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项（偏置bias、偏移量offset或截距intercept）来进行平移（translation）。

代价函数(cost function/lost function损失函数):
损失函数能够量化目标的实际值与预测值之间的差距。通常会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
回归问题中最常用的损失函数是平方误差函数。

线性回归与深度网络:
可以将线性回归是描述为一个单层神经网络

===== 基本元素
====== 解析解
analytical solution
像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

===== 实现
- 生成数据
- 读取数据集
- 初始化模型参数
- 定义模型
- 定义损失函数
- 定义优化算法

==== 梯度下降
梯度下降(gradient descent):
梯度下降和梯度下降的变体不仅用于训练线性回归，还用于训练所有AI中一些最大和最复杂的模型。
梯度下降几乎可以优化所有深度学习模型。它通过不断地在损失函数递减的方向上更新参数来降低误差。

学习率(learning rate)alpha

实际中的执行可能会非常慢：因为在每一次更新参数之前，必须遍历整个数据集。因此，通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降(minibatch stochastic gradient descent)。

==== softmax回归

===== 实现

==== 参考
https://zh.d2l.ai/chapter_linear-networks/index.html

=== 无监督学习
==== 概念
无监督学习是机器学习的一种方法，其目标是在不提供监督信息（如预定义的标签或结果）的条件下，通过学习未标记的数据来发现数据的内在结构和关系。

无监督学习的主要运用包含：聚类分析(cluster analysis)、关系规则(association rule)、维度缩减(dimensionality reduce)。它是监督式学习和强化学习等策略之外的一种选择。

一个常见的无监督学习是数据聚类。在人工神经网络中，生成对抗网络(GAN)、自组织映射(SOM)和适应性共振理论(ART)则是最常用的非监督式学习。

优势:
(1) 不需要标记数据：获取标记数据往往需要大量的人力、物力和时间成本。
(2) 发现新的知识和结构：能够揭示数据内部隐藏的结构和模式，这些信息可能是在预先标记的情况下无法发现的。例如，可以发现新的客户群体或者数据中的潜在关联。

局限性:
(1) 结果解释性相对较弱：由于没有明确的目标变量，无监督学习的结果可能比较难以解释。例如，在聚类任务中，很难确定划分出的簇在实际业务中的具体含义。
(2) 评估指标不明确：相比于监督学习有准确率、召回率等明确的评估指标，无监督学习的评估指标比较复杂，并且因任务而异。例如，对于聚类任务，评估聚类质量的指标有轮廓系数、DBI（Davies - Bouldin Index）等，但这些指标的解释和选择也需要根据具体情况而定。

== 强化学习
=== 概念
在强化学习问题中，智能体(agent)在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察(observation)，并且必须选择一个动作(action)，然后通过某种机制(有时称为执行器)将其传输回环境，最后智能体从环境中获得奖励(reward)。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程(markov decision process)。当状态不依赖于之前的操作时，称该问题为上下文赌博机(contextual bandit problem)。当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机(multi-armed bandit problem)。

== 深度学习
=== 概念
发展：
2010年开始
（1）大量数据
（2）廉价又高质量的传感器、廉价的数据存储以及廉价计算的普及，特别是GPU的普及

卷积神经网络(CNN)：
主要用于处理图像数据。它通过卷积层和池化层自动提取图像的特征，从而实现图像分类、目标检测等任务。

循环神经网络(RNN)：
适用于处理序列数据，如文本、时间序列数据等。它能够记住之前的信息，并利用这些信息来处理当前的输入。长短期记忆网络(LSTM)和门控循环单元(GRU)是常见的 RNN 变体，能够更好地处理长序列数据中的长期依赖关系。

生成对抗网络(GAN)：
由生成器和判别器组成。生成器试图生成逼真的假数据，而判别器则试图区分真实数据和生成器生成的数据。通过不断的对抗训练，生成器和判别器的性能都不断提高。

== 多层感知机
=== 概念
Multilayer Perceptrons，即MLP
最简单的深度网络称为多层感知机。多层感知机由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。

多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。

常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

== 计算机视觉

== 自然语言处理

== 工具
- Jupyter Notebook
Jupyter vs. IDE

- colab
https://colab.research.google.com/
https://www.geeksforgeeks.org/google-collab-vs-jupyter-notebook/

- GPU
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html

参考:
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html

== 框架/库
caff2:
2018年3月底，Caffe2并入了Pytorch。

CNTK:
2019年4月26日发布2.7.0稳定版后，微软对CNTK的更新逐渐减少，并开始将更多的精力投入到与PyTorch等框架的合作和整合上，CNTK也逐渐被一些新的技术和框架所替代。

chainer:
2019年12月，Preferred Networks宣布将其深度学习研究平台的开发工作从Chainer转移到PyTorch。

mxnet:
2022年底，MXNet的代码开发大部分停止，社区参与度放缓。2022年9月MXNet从Apache孵化器毕业成为顶级项目，但在同年11月却被移入Apache Attic，进入“只读”阶段，意味着该项目不再重建社区、修正Bug、发布新版本，正式宣告退休。

https://github.com/pytorch
https://github.com/tensorflow/tensorflow
https://github.com/keras-team/keras

参考:
https://www.zhihu.com/question/46587833

== project
https://github.com/vietnh1009/Super-mario-bros-PPO-pytorch
https://github.com/karpathy/nanoGPT
https://github.com/openai

== 参考
https://en.d2l.ai/
https://zh.d2l.ai/
https://space.bilibili.com/1567748478
https://www.bilibili.com/video/BV1Bq421A74G/
https://www.bilibili.com/video/BV1Wv411h7kN/
https://www.coursera.org/specializations/machine-learning-introduction#courses
https://cs231n.stanford.edu/
https://github.com/tangyudi/Ai-Learn
https://github.com/apachecn/ai-roadmap/
https://github.com/mahseema/awesome-ai-tools