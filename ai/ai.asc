= AI
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:homepage: http://orientye.com

== 概览

=== 概念
- 分类
机器学习(machine learning)
深度学习(deep learning)
监督学习(supervised learning)与无监督学习(unsupervised learning)
强化学习(reinforcement learning)
参考:
https://www.zhihu.com/question/279973545
https://arxiv.org/pdf/1810.06339

- 张量(tensor)
n维数组，也称为张量(tensor)。
无论使用哪个深度学习框架，其张量类(在PyTorch和TensorFlow中为Tensor)都与Numpy的ndarray类似。但深度学习框架又比Numpy的ndarray多了一些重要功能：首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；其次，张量类支持自动微分。
深度学习操作的主要对象是张量。它提供了基本数学运算、广播、索引、切片、内存节省和转换其他Python对象等功能。

- AIGC
人工智能生成内容(Artificial Intelligence Generated Content)

== 机器学习

=== 概念
==== 学习
在机器学习中，学习(learning)是一个训练模型的过程。通过这个过程，发现正确的参数集，从而使模型强制执行所需的行为。换句话说，用数据训练(train)模型。

训练过程通常包含如下步骤：

    从一个随机初始化参数的模型开始，这个模型基本没有“智能”；
    获取一些数据样本；
    调整参数，使模型在这些样本中表现得更好；
    重复第(2)步和第(3)步，直到模型在任务中的表现令人满意。

==== 核心组件
- 组成

    可以用来学习的数据(data)
    如何转换数据的模型(model)
    一个目标函数(objective function)，用来量化模型的有效性
    调整模型参数以优化目标函数的算法(algorithm)

- 数据
理想的数据: 相同的维数(数据维数dimensionality)、海量、正确

- 模型
深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习。

- 目标函数
即数据集通常可以分成两部分：训练数据集(training dataset，或称为训练集(training set))用于拟合模型参数，测试数据集(test dataset，或称为测试集(test set))用于评估拟合的模型。
验证集:
验证数据集(validation dataset): https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id6
过拟合(overfitting):
当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为过拟合的。就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。
正则化(regularization):
用于对抗过拟合的技术称为正则化。
欠拟合(underfitting):
欠拟合是机器学习中的一个常见问题，它发生在模型在训练数据上表现得不够好，即模型未能捕捉到数据中的潜在规律或模式，导致在训练集和测试集上都表现不佳。
参考: https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html

- 优化算法
当获得了一些数据源及其表示、一个模型和一个合适的损失函数，接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。深度学习中，大多流行的优化算法通常基于一种基本方法–梯度下降(gradient descent)。简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。然后，它在可以减少损失的方向上优化参数。

=== 监督学习
==== 概念
监督学习是机器学习的一种方法，可以由训练资料中学到或建立一个模式(函数/learning model)，并依此模式推测新的实例。训练资料是由输入对象(通常是向量)和预期输出所组成。函数的输出可以是一个连续的值(称为回归分析)，或是预测一个分类标签(称作分类)。

    回归(regression):
        主要用于预测数值型数据
        应用实例:
            股票价格波动的预测，房屋价格的预测等

    分类(classification):
        将实例数据划分到合适的类别中
        应用实例:
            判断网站是否被黑客入侵(二分类)，手写数字的自动识别(多分类)

==== 应用场景
回归
分类
标记问题
搜索
推荐系统
序列学习

==== 线性回归
===== 概念
回归(regression):
回归是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
在机器学习领域中的大多数任务通常都与预测(prediction)有关。

线性回归(linear regression):
可以追溯到19世纪初，它在回归的各种标准工具中最简单而且最流行。
线性回归基于几个简单的假设：首先，假设自变量和因变量之间的关系是线性的，即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声；其次，假设任何噪声都比较正常，如噪声遵循正态分布。

线性模型:
严格来说，有时候输入特征其实是一个仿射变换(affine transformation)。仿射变换的特点是通过加权和对特征进行线性变换(linear transformation)，并通过偏置项(偏置bias、偏移量offset或截距intercept)来进行平移(translation)。

代价函数(cost function/lost function损失函数):
损失函数能够量化目标的实际值与预测值之间的差距。通常会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
回归问题中最常用的损失函数是平方误差函数。

线性回归与深度网络:
可以将线性回归是描述为一个单层神经网络

===== 基本元素
====== 解析解
analytical solution
像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

===== 实现
====== 生成数据
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id2
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id2

====== 读取数据集
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id3
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id3

====== 初始化模型参数
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id4
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id5

====== 定义模型
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id5
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id4

====== 定义损失函数
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id6
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id6

====== 定义优化算法
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id7
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id7

====== 训练
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id8
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id8

==== 梯度下降
梯度下降(gradient descent):
梯度下降和梯度下降的变体不仅用于训练线性回归，还用于训练所有AI中一些最大和最复杂的模型。
梯度下降几乎可以优化所有深度学习模型。它通过不断地在损失函数递减的方向上更新参数来降低误差。

学习率(learning rate)alpha

实际中的执行可能会非常慢：因为在每一次更新参数之前，必须遍历整个数据集。因此，通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降(minibatch stochastic gradient descent)。

==== softmax回归
===== 概念
softmax regression，也称为多类逻辑回归(multinomial logistic regression)，是一种用于多分类问题的分类算法。
softmax回归是一种多分类的线性分类模型，它是逻辑回归在多分类问题上的推广。其输出是一个概率分布，表示样本属于各个不同类别的概率。

===== 分类问题

===== 网络架构

===== 全连接层的参数开销

===== softmax运算
softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。

===== 小批量样本的矢量化

===== 损失函数
- 对数似然
https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id8

- softmax及其导数
https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#subsec-softmax-and-derivatives

- 交叉熵损失
https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id10

===== 信息论基础
- 熵
信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布的熵(entropy)。

- 信息量

===== 实现
https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html

https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html

==== 图像分类数据集
https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html

==== 参考
https://zh.d2l.ai/chapter_linear-networks/index.html

=== 无监督学习
==== 概念
无监督学习是机器学习的一种方法，其目标是在不提供监督信息(如预定义的标签或结果)的条件下，通过学习未标记的数据来发现数据的内在结构和关系。

无监督学习的主要运用包含：聚类分析(cluster analysis)、关系规则(association rule)、维度缩减(dimensionality reduce)。它是监督式学习和强化学习等策略之外的一种选择。

一个常见的无监督学习是数据聚类。在人工神经网络中，生成对抗网络(GAN)、自组织映射(SOM)和适应性共振理论(ART)则是最常用的非监督式学习。

优势:
(1) 不需要标记数据：获取标记数据往往需要大量的人力、物力和时间成本。
(2) 发现新的知识和结构：能够揭示数据内部隐藏的结构和模式，这些信息可能是在预先标记的情况下无法发现的。例如，可以发现新的客户群体或者数据中的潜在关联。

局限性:
(1) 结果解释性相对较弱：由于没有明确的目标变量，无监督学习的结果可能比较难以解释。例如，在聚类任务中，很难确定划分出的簇在实际业务中的具体含义。
(2) 评估指标不明确：相比于监督学习有准确率、召回率等明确的评估指标，无监督学习的评估指标比较复杂，并且因任务而异。例如，对于聚类任务，评估聚类质量的指标有轮廓系数、DBI(Davies - Bouldin Index)等，但这些指标的解释和选择也需要根据具体情况而定。

== 强化学习
=== 概念
在强化学习问题中，智能体(agent)在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察(observation)，并且必须选择一个动作(action)，然后通过某种机制(有时称为执行器)将其传输回环境，最后智能体从环境中获得奖励(reward)。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程(markov decision process)。当状态不依赖于之前的操作时，称该问题为上下文赌博机(contextual bandit problem)。当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机(multi-armed bandit problem)。

== 深度学习
=== 概念
发展：
2010年开始
(1)大量数据
(2)廉价又高质量的传感器、廉价的数据存储以及廉价计算的普及，特别是GPU的普及

卷积神经网络(CNN)：
主要用于处理图像数据。它通过卷积层和池化层自动提取图像的特征，从而实现图像分类、目标检测等任务。

循环神经网络(RNN)：
适用于处理序列数据，如文本、时间序列数据等。它能够记住之前的信息，并利用这些信息来处理当前的输入。长短期记忆网络(LSTM)和门控循环单元(GRU)是常见的 RNN 变体，能够更好地处理长序列数据中的长期依赖关系。

生成对抗网络(GAN)：
由生成器和判别器组成。生成器试图生成逼真的假数据，而判别器则试图区分真实数据和生成器生成的数据。通过不断的对抗训练，生成器和判别器的性能都不断提高。

== 多层感知机
=== 概念
Multilayer Perceptrons，即MLP
最简单的深度网络称为多层感知机。多层感知机由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。

多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。

常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

应用领域：
分类任务：MLP可以用于各种分类任务，如手写数字识别、图像分类、文本分类等。
回归任务：也可用于回归任务，如预测股票价格、气温变化等。通过对历史数据的学习，MLP能够建立输入特征(如时间序列数据、相关经济指标等)与目标变量(股票价格、气温等)之间的关系，从而进行预测。

=== 隐藏层
- 线性模型可能会出错
- 在网络中加入隐藏层
- 从线性到非线性
- 通用近似定理

=== 激活函数
激活函数(activation function)通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。

==== ReLU函数
最受欢迎的激活函数是修正线性单元(Rectified linear unit，ReLU)，它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的非线性变换。给定元素x，ReLU函数被定义为该元素与的最大值:

    ReLU(x) = max(x, 0)

通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。

使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。

注意，ReLU函数有许多变体，包括参数化ReLU(Parameterized ReLU，pReLU)函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过:

    ReLU(x) = max(x, 0) + a * min(x, 0)

==== sigmoid函数
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html#sigmoid

==== tanh函数
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html#tanh

=== 实现
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html

=== 权重衰减(weight decay)
https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html

=== 暂退法(dropout)
暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。之所以被称为暂退法，因为从表面上看是在训练过程中丢弃(drop out)一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

    暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元
    暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用
    暂退法将活性值h替换为具有期望值的随机变量
    暂退法仅在训练期间使用

https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html

=== 前向传播、反向传播和计算图
- 前向传播
前向传播(forward propagation或forward pass): 按顺序(从输入层到输出层)计算和存储神经网络中每层的结果。
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2

- 反向传播
反向传播(backward propagation或backpropagation)指的是计算神经网络参数梯度的方法。根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4

- 训练神经网络
在训练神经网络时，前向传播和反向传播相互依赖。
在初始化模型参数后，交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是需要保留中间值，直到反向传播完成。这也是训练比单纯的预测需要更多的内存(显存)的原因之一。

- 参考
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html

=== 数值稳定性和模型初始化
https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html

== 计算机视觉

== 自然语言处理

== 工具
- Jupyter Notebook
Jupyter vs. IDE

- colab
https://colab.research.google.com/
https://www.geeksforgeeks.org/google-collab-vs-jupyter-notebook/

- GPU
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html

参考:
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html

== 框架/库
caff2:
2018年3月底，Caffe2并入了Pytorch。

CNTK:
2019年4月26日发布2.7.0稳定版后，微软对CNTK的更新逐渐减少，并开始将更多的精力投入到与PyTorch等框架的合作和整合上，CNTK也逐渐被一些新的技术和框架所替代。

chainer:
2019年12月，Preferred Networks宣布将其深度学习研究平台的开发工作从Chainer转移到PyTorch。

mxnet:
2022年底，MXNet的代码开发大部分停止，社区参与度放缓。2022年9月MXNet从Apache孵化器毕业成为顶级项目，但在同年11月却被移入Apache Attic，进入“只读”阶段，意味着该项目不再重建社区、修正Bug、发布新版本，正式宣告退休。

https://github.com/pytorch
https://github.com/tensorflow/tensorflow
https://github.com/keras-team/keras

参考:
https://www.zhihu.com/question/46587833

== project
https://github.com/vietnh1009/Super-mario-bros-PPO-pytorch
https://github.com/karpathy/nanoGPT
https://github.com/openai

== 参考
https://en.d2l.ai/
https://zh.d2l.ai/
https://space.bilibili.com/1567748478
https://www.bilibili.com/video/BV1Bq421A74G/
https://www.bilibili.com/video/BV1Wv411h7kN/
https://www.coursera.org/specializations/machine-learning-introduction#courses
https://cs231n.stanford.edu/
https://github.com/tangyudi/Ai-Learn
https://github.com/apachecn/ai-roadmap/
https://github.com/mahseema/awesome-ai-tools