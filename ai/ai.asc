= AI
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath

== 概览

=== 概念
- 分类
机器学习(machine learning)
监督学习(supervised learning)与无监督学习(unsupervised learning)
深度学习(deep learning)
强化学习(reinforcement learning)
Q: 机器学习 vs. 统计学
参考:
https://www.zhihu.com/question/279973545
关系图: https://arxiv.org/pdf/1810.06339

- 张量(tensor)
n维数组，也称为张量(tensor)。
深度学习操作的主要对象是张量。
它提供了基本数学运算、广播、索引、切片、内存节省和转换其它Python对象等功能。
无论使用哪个深度学习框架，其张量类(在PyTorch和TensorFlow中为Tensor)都与Numpy的ndarray类似。
但深度学习框架又比Numpy的ndarray多了一些重要功能:
首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；其次，张量类支持自动微分。

- AIGC
人工智能生成内容(Artificial Intelligence Generated Content)

- AGI
通用AI(Artificial General Intelligence)指的是一种能够理解、学习和执行任何人类能做的智能任务的人工智能系统。与当前的人工智能(如深度学习模型或专家系统)不同，AGI 不仅仅专注于某一特定领域或任务，而是具备广泛的认知能力，能够处理各种问题，类似于人类的智能。

- 大模型
大模型通常指的是在机器学习和深度学习领域中，具有大量参数和复杂结构的模型。这些模型通常需要大量的数据和计算资源进行训练，能够处理复杂的任务，如自然语言处理、图像识别、语音识别视频处理等。

- LLM
Large Language Model 大型语言模型。例如GPT，BERT。
vs. 大模型
大模型是一个广泛的术语，指的是具有大量参数和复杂结构的机器学习模型。例如，卷积神经网络(CNN)、循环神经网络(RNN)以及各种深度学习架构。

- 大模型蒸馏
Model Distillation是一种模型压缩方法，旨在将大型、复杂的深度学习模型(通常称为教师模型)的知识转移到一个较小的、效率更高的模型(称为学生模型)中。通过这种方式，学生模型可以在保持较高性能的同时，减少计算资源的需求和推理时间。

=== 学习
在机器学习中，学习(learning)是一个训练模型的过程。通过这个过程，发现正确的参数集，从而使模型强制执行所需的行为。换句话说，用数据训练(train)模型。

训练过程通常包含如下步骤:

    (1) 通常从一个随机初始化参数的模型开始，这个模型基本没有"智能"；
    (2) 获取一些数据样本；
    (3) 调整参数，使模型在这些样本中表现得更好；
    (4) 重复第(2)步和第(3)步，直到模型在任务中的表现令人满意。

=== 核心组件
==== 组成

    可以用来学习的数据(data)
    如何转换数据的模型(model)
    一个目标函数(objective function)，用来量化模型的有效性
    调整模型参数以优化目标函数的算法(algorithm)

    模型 数据 算力

    Q: 如果学习的结果不够理想，那么如何判断是模型的问题，还是优化的问题呢？

==== 数据
- 理想的数据
相同的维数(数据维数dimensionality)、海量、正确

- 数据集与训练集
数据集通常可以分成两部分:
    训练数据集(training dataset，或称为训练集(training set))用于拟合模型参数
    测试数据集(test dataset，或称为测试集(test set))用于评估拟合的模型

- 验证集
验证数据集(validation dataset):
https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id6
验证数据集是机器学习模型开发过程中用于调整超参数、监控模型性能以及防止过拟合的关键数据集。
核心作用:
    模型调优: 用于评估不同超参数(如学习率、网络层数、正则化系数等)的效果，选择最佳配置。
    早停(Early Stopping): 监控验证集上的性能，当性能不再提升时提前终止训练，防止过拟合。
    模型选择: 比较不同模型架构(如不同神经网络结构)的泛化能力。
与训练集、测试集的区别:
    训练集: 直接用于模型参数训练。
    验证集: 不参与训练，仅用于调优和监控。
    测试集: 最终评估模型性能，反映真实泛化能力(需在全部调优完成后使用)。
划分方法:
    传统划分: 将原始数据按比例分为训练集(60-80%)、验证集(10-20%)、测试集(10-20%)。
    交叉验证: 在小数据集中更可靠，如K折交叉验证(K-Fold CV)，每次轮流将一部分数据作为验证集。
    时间序列数据: 需按时间顺序划分(如前80%时间训练，后20%验证)。

- 过拟合(overfitting)
当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为过拟合的。就像在现实生活中，尽管模拟考试考得很好，真正的考试不一定百发百中。
Q: 举一个过拟合的例子？
Q: 如何防范过拟合？

- 正则化(regularization)
用于对抗过拟合

- 欠拟合(underfitting)
模型在训练数据上表现得不够好，即模型未能捕捉到数据中的潜在规律或模式，导致在训练集和测试集上都表现不佳。
参考: https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html
Q: 如何防范欠拟合？

- 参考
https://en.d2l.ai/chapter_multilayer-perceptrons/generalization-deep.html

==== 模型
深度学习与经典方法的区别主要在于:
前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习。

深度的含义其实就是多层。

==== 目标函数
===== 概念
目标函数(Objective Function)是机器学习和优化问题中需要最大化或最小化的函数，通常用于指导模型的训练过程。它是模型优化的核心，决定了算法的最终目标。目标函数可以包含损失函数(Loss Function)以及其他附加项(如正则化项)，具体形式取决于任务需求。

损失函数(Loss Function)，也称为代价函数(Cost Function)，是用于衡量模型预测结果与真实值之间差异的函数。它是模型优化的目标，通过最小化损失函数来调整模型参数，从而提高预测准确性。

目标函数与损失函数的区别:
https://www.zhihu.com/question/52398145

===== 常见损失函数分类及用途
====== 回归任务
- MAE(Mean Absolute Error，平均绝对误差)
ifndef::env-github[]
    stem:[\frac{1}{n}\sum|y_i-\hat{y}_i|]
endif::[]
ifdef::env-github[]
```math
\frac{1}{n}\sum|y_i-\hat{y}_i|
```
endif::[]

- MSE(Mean Squared Error，平均平方误差即均方误差)
ifndef::env-github[]
    stem:[\frac{1}{n}\sum(y_i-\hat{y}_i)^2]
endif::[]
ifdef::env-github[]
```math
\frac{1}{n}\sum(y_i-\hat{y}_i)^2
```
endif::[]

- Huber Loss
结合MSE和MAE，对异常值鲁棒且可微。

====== 分类任务(离散标签预测)
- 交叉熵损失（Cross-Entropy Loss）
** 二分类（Binary Cross-Entropy）
** 多分类（Categorical Cross-Entropy）
** 特点: 与Softmax激活函数配合使用，对概率预测敏感。

- Hinge Loss（支持向量机SVM使用）

    特点: 用于最大化分类间隔，适合二分类。

- Focal Loss

    解决类别不平衡问题，降低易分类样本的权重。

====== 其它任务
- KL散度(Kullback-Leibler Divergence)

    衡量两个概率分布的差异，用于生成模型(如VAE)。

- Triplet Loss

    用于度量学习(如人脸识别)，拉近同类样本，推开异类样本。

- IoU Loss(目标检测)

    直接优化预测框与真实框的交并比(Intersection over Union)。

===== mini-batch
用随机选择的小批量数据(mini-batch)作为全体训练数据的近似值。
numpy.random.choice():
https://numpy.org/devdocs/reference/random/generated/numpy.random.choice.html

===== 优化与损失函数的关系
- 梯度下降(Gradient Descent)通过计算损失函数的梯度更新模型参数。
- 损失函数的凸性、光滑性影响优化效率(如MSE的梯度比MAE更稳定)。

===== 超参数与模型参数
超参数是机器学习/深度学习模型在训练之前需要手动设定的参数，用于控制模型的训练过程、结构或优化策略。它们不能通过训练数据自动学习，而是依赖经验、实验或调优算法来确定。

超参数: 人为设定，指导训练过程(学习率)。
模型参数: 通过数据自动学习(如神经网络的权重)。

==== 优化算法
当获得了一些数据源及其表示、一个模型和一个合适的损失函数之后，接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。

深度学习中，大多流行的优化算法通常基于一种基本方法–梯度下降(gradient descent)。简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，损失会朝哪个方向移动，然后在可以减少损失的方向上优化参数。

==== 梯度下降(gradient descent)
===== 概念
梯度下降和梯度下降的变体不仅用于训练线性回归，还用于训练所有AI中一些最大和最复杂的模型。
梯度下降几乎可以优化所有深度学习模型。

梯度下降通过不断地在损失函数递减的方向上更新参数来降低误差。
梯度下降是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中。其核心思想是通过不断调整参数，使目标函数的值逐步减小，最终找到函数的局部最小值或全局最小值。
根据目的是寻找最小值还是最大值，梯度法的叫法有所不同。严格地讲，寻找最小值的梯度法称为梯度下降法(gradient descent method)，寻找最大值的梯度法称为梯度上升法(gradient ascent method)。但
是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题，因此"下降"还是"上升"的差异本质上并不重要。一般来说，深度学习中，梯度法主要是指梯度下降法。

- 局部极小值(local minimum)
- 局部极大值(local maximum)
- 鞍点(saddle point)
- 驻点(critical point，梯度为零的点，可能是极值或鞍点)

===== 基本元素
- 目标函数
通常表示为J(θ)，其中θ是模型的参数。目标函数衡量模型预测值与真实值之间的误差。

- 梯度
目标函数对参数的偏导数向量，表示为∇J(θ)。梯度指向函数值增加最快的方向。

    哈密顿算子(Hamiltonian):
        数学符号为▽(读作Nabla)
        在运算中既有微分又有矢量的双重运算性质
        其优点在于可以把对矢量函数的微分运算转变为矢量代数的运算。

- 学习率(Learning Rate)
控制每次参数更新的步长，表示为α。学习率过大可能导致震荡，过小则收敛速度慢。

===== 类型
根据每次迭代使用的数据量，梯度下降可以分为三种主要类型:

    批量梯度下降(Batch Gradient Descent):
        使用整个训练集计算梯度。
        优点: 稳定性高，收敛性好。
        缺点: 计算量大，尤其在数据集很大时，更新速度慢。

    随机梯度下降(Stochastic Gradient Descent, SGD):
        每次使用一个样本计算梯度并更新参数。
        优点: 计算速度快，可以在线学习，适用于大规模数据集。
        缺点: 收敛不稳定，可能会在最小值附近震荡。

    小批量梯度下降(Mini-batch Gradient Descent):
        每次使用一小部分样本(mini-batch)计算梯度。
        优点: 结合了批量和随机梯度下降的优点，收敛速度快且相对稳定。
        常用的小批量大小通常在32到256之间。
        Q: Small Batch vs. Large Batch

===== 梯度下降的步骤

    1. 初始化参数: 随机初始化参数θ
    2. 计算梯度: 计算目标函数在当前参数下的梯度 ∇J(θ)
    3. 更新参数: 沿着梯度的反方向更新参数，公式为: θ ← θ − α∇J(θ)
    4. 重复迭代: 重复步骤2和3，直到满足停止条件(如梯度接近零、达到最大迭代次数等)

===== Error Surface(误差面)
误差面是指模型损失函数随着参数变化而呈现的几何形状。它反映了模型在不同参数配置下的表现，通常通过损失函数的值来衡量。
误差表面是深度学习中一个核心概念，它描述了损失函数随参数变化的几何形状。由于其高维性和非凸性，优化深度学习模型具有挑战性。通过研究误差表面，可以更好地理解模型的优化行为，并设计更高效的训练方法。

- 误差表面的特点

    高维空间:
        深度学习模型通常有大量参数(权重和偏置)，因此误差表面存在于一个非常高维的空间中。
        虽然无法直接可视化高维空间，但可以通过低维切片或投影来近似理解。

    非凸性:
        深度神经网络的误差表面通常是非凸的，
        即存在多个局部最小值(local minima)和鞍点(saddle points)。
        局部最小值是参数空间中损失函数较小的点，但在全局范围内可能不是最优解。
        鞍点是梯度为零的点，但在某些方向上是局部最小值，而在另一些方向上是局部最大值。

    平坦区域和陡峭区域:
        误差表面可能包含平坦区域(plateaus)，这些区域的梯度接近于零，导致训练速度变慢。
        陡峭区域(sharp regions)可能导致梯度爆炸，使训练不稳定。

    对称性:
        由于神经网络的对称性(例如，隐藏层的神经元可以交换顺序)，误差表面通常具有多个等价的最小值。

- 误差表面与优化

    梯度下降:
        梯度下降法通过计算损失函数对参数的梯度，沿着误差表面的下降方向更新参数。
        在非凸误差表面上，梯度下降可能会陷入局部最小值或鞍点。

    局部最小值 vs 全局最小值:
        传统观点认为局部最小值是深度学习优化的主要障碍，
        但近年来的研究表明，许多局部最小值在损失值上接近全局最小值，因此对模型性能影响较小。
        鞍点可能是更严重的问题，尤其是在高维空间中。

    优化器的改进:
        为了应对误差表面的复杂性，
        现代优化器(如Adam、RMSProp等)引入了动量、自适应学习率等机制，
        以加速收敛并避免陷入不良区域。

- 误差表面的可视化

    由于误差表面存在于高维空间，直接可视化非常困难。常用的方法包括:
    低维投影:
        选择两个参数方向，固定其他参数，绘制损失函数的变化。
    随机方向:
        在参数空间中随机选择两个方向，绘制损失函数的变化。
    PCA降维:
        使用主成分分析(PCA)将高维参数空间降维到二维或三维，然后可视化损失函数。

- 误差表面的研究意义

    理解优化过程:
        通过分析误差表面，可以更好地理解优化算法的行为，例如为什么某些初始化方法或优化器效果更好。
    改进模型设计:
        了解误差表面的结构可以帮助设计更高效的模型架构和训练策略。
    解决训练问题:
        例如，梯度消失和梯度爆炸问题可以通过分析误差表面的梯度分布来诊断和解决。

===== 梯度下降的挑战
====== 局部最小值(local minima)
梯度下降可能陷入局部最小值，尤其是在非凸函数中。
flat minima vs. sharp minima

====== 鞍点(saddle point)
鞍点得名于马鞍的形状 - 沿着马鞍的长度方向(前后)是极小值，而沿着宽度方向(左右)是极大值。

鞍点是数学(尤其是微积分和优化理论)中的一个重要概念，指的是函数在某点处沿不同方向具有不同的极值性质。

在高维空间中，梯度下降可能在鞍点附近停滞。

====== 学习率选择
学习率过大或过小都会影响收敛效果。

====== Q&A
Q: gradient为零(统称为critical point)的时候，有哪几种情况？
Q: 如何知道critical point的类型，到底是local minima，还是saddle point？

====== Hessian矩阵
Hessian矩阵是一个由多变量函数的二阶偏导数组成的方阵，用于描述函数的局部曲率。它在优化、机器学习和数值分析中具有重要作用，尤其是在判断临界点的性质时。
Q: 如何计算Hessian矩阵？
Q: 如何解决奇异Hessian矩阵问题？

===== 改进方法
====== 动量法(momentum)
- 概念

    动量法是一种用于优化梯度下降算法的技术，旨在加速收敛并减少振荡。
    它通过引入"动量"项来累积之前的梯度信息，从而在更新参数时考虑历史梯度方向。

- 核心思想

    动量法在每次参数更新时，不仅使用当前的梯度，还结合之前梯度的指数加权平均。
    这样可以帮助算法在相关梯度方向上加速，并在振荡方向上减少波动。

- 优点

    加速收敛: 在梯度方向一致时，动量法可以加速更新。
    减少振荡: 在梯度方向不一致时，动量法可以平滑更新路径，减少振荡。

- 缺点

    超参数调整: 需要调整动量系数γ和学习率η。
    可能错过极小值:动量过大可能导致算法错过局部极小值。

====== 自适应学习率方法
如Adagrad、RMSprop、Adam等，根据梯度历史动态调整学习率。

====== 学习率衰减
随着迭代次数增加，逐步减小学习率。

===== 数学基础
梯度下降的数学基础是泰勒展开和凸优化理论。
对于凸函数，梯度下降可以保证收敛到全局最小值；对于非凸函数，只能保证收敛到局部最小值。

== 机器学习

=== 监督学习
- 概念

    监督学习是一种机器学习范式，其特点是使用标注数据(即有输入和对应的输出标签)来训练模型。
    训练资料是由输入对象(通常是向量)和预期输出所组成。
    函数的输出可以是一个连续的值(称为回归分析)，或是预测一个分类(离散)标签(称作分类)。

    回归(regression):
        主要用于预测数值型数据
        应用实例: 股票价格波动的预测，房屋价格的预测等
    分类(classification):
        将实例数据划分到合适的类别中
        应用实例: 判断网站是否被黑客入侵(二分类)，手写数字的自动识别(多分类)

- 核心特点

    数据要求: 需要有标注的数据集(输入特征和对应的标签)。

- 常用算法

    线性回归(Linear Regression)
    逻辑回归(Logistic Regression)
    支持向量机(SVM)
    决策树(Decision Trees)
    随机森林(Random Forest)

- 应用场景

    回归、分类、标记问题、搜索、推荐系统、序列学习等

=== 无监督学习
- 概念

    无监督学习是一种机器学习范式，其特点是使用未标注的数据(即只有输入，没有对应的输出标签)来训练模型。

- 核心特点

    数据要求: 不需要标注数据。
    目标: 发现数据中的隐藏结构、模式或规律。

- 典型任务

    聚类: 将数据划分为若干组(如客户细分)
    降维: 将高维数据映射到低维空间(如数据可视化)
    异常检测: 识别数据中的异常点
    关联规则学习: 发现数据中的关联关系(如购物篮分析)

- 常用算法

    K-Means
    层次聚类(Hierarchical Clustering)
    主成分分析(PCA)
    t-SNE
    自编码器(Autoencoders)
    高斯混合模型(GMM)

- 应用场景

    市场细分、图像分割、文本主题建模、数据压缩

=== vs. 深度学习
深度学习是一种方法: 深度学习是基于神经网络的学习方法，可以用于监督学习、无监督学习和强化学习。
监督学习和无监督学习是范式: 它们是机器学习的两种主要范式，分别依赖于有标签数据和无标签数据。

深度学习可以增强监督和无监督学习: 例如，使用深度神经网络进行图像分类(监督学习)或使用自编码器进行降维(无监督学习)。

== 深度学习
=== 概念
- 发展
2010年开始
(1)大量数据
(2)廉价又高质量的传感器、廉价的数据存储以及廉价计算的普及，特别是GPU的普及

- 卷积神经网络(convolutional neural network, CNN)
主要用于处理图像数据。
它通过卷积层和池化层自动提取图像的特征，从而实现图像分类、目标检测等任务。

- 循环神经网络(recurrent neural network, RNN)
适用于处理序列数据，如文本、时间序列数据等。
它能够记住之前的信息，并利用这些信息来处理当前的输入。
长短期记忆网络(LSTM)和门控循环单元(GRU)是常见的 RNN 变体，能够更好地处理长序列数据中的长期依赖关系。

- 生成对抗网络(generative adversarial network, GAN)
由生成器(generator)和判别器(discriminator)组成。
生成器试图生成逼真的假数据，而判别器则试图区分真实数据和生成器生成的数据。
通过不断的对抗训练，生成器和判别器的性能都不断提高。

=== 线性神经网络(Linear Neural Networks)
==== 线性神经网络之回归(Linear Neural Networks for Regression)
===== 概念
- 回归(regression)
回归是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
在机器学习领域中的大多数任务通常都与预测(prediction)有关。

- 线性回归(linear regression)
可以追溯到19世纪初，它在回归的各种标准工具中最简单而且最流行。
线性回归基于几个简单的假设: 首先，假设自变量和因变量之间的关系是线性的，即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声；其次，假设任何噪声都比较正常，如噪声遵循正态分布。

- 线性模型
严格来说，有时候输入特征其实是一个仿射变换(affine transformation)。仿射变换的特点是通过加权和对特征进行线性变换(linear transformation)，并通过偏置项(偏置bias、偏移量offset或截距intercept)来进行平移(translation)。

- 代价函数(cost function/lost function损失函数)
损失函数能够量化目标的实际值与预测值之间的差距。通常会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
回归问题中最常用的损失函数是平方误差函数(即MSE):

    MAE(Mean Absolute Error)与MSE(Mean Squared Error)是两种常见的回归模型评估指标:
        MAE 对所有误差赋予相等的权重，适用于对误差的分布没有强烈偏好时；
        MSE 强调惩罚较大的误差，适合在希望减少极端错误影响时使用。

- 线性回归与深度网络
可以将线性回归是描述为一个单层神经网络

===== 基本元素
====== 解析解
analytical solution
像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

===== 实现
====== 生成数据
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id2
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id2

====== 读取数据集
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id3
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id3

====== 初始化模型参数
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id4
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id5

====== 定义模型
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id5
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id4

====== 定义损失函数
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id6
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id6

====== 定义优化算法
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id7
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id7

====== 训练
https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#id8
https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html#id8

==== 线性神经网络之分类(Linear Neural Networks for Classification)
===== Softmax Regression
- 概念
softmax regression，也称为多类逻辑回归(multinomial logistic regression)，是一种用于多类分类问题的监督学习算法。
softmax回归是一种多分类的线性分类模型，它是逻辑回归在多分类问题上的推广。其输出是一个概率分布，表示样本属于各个不同类别的概率。

- vs. 逻辑回归
Softmax回归是逻辑回归的扩展，逻辑回归通常用于二分类问题，而Softmax回归则解决多分类问题。给定一个样本，它会输出各个类别的概率值，这些概率值的和为1。Softmax回归通常使用在多分类问题中，如手写数字识别、图像分类等任务。

- softmax函数的特征
softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的输出值的总和是1。输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，才可以把softmax函数的输出解释为"概率"。
另外，使用了softmax函数，各个元素之间的大小关系也不会改变。这是因为指数函数y = exp(x)是单调递增函数。

===== 图像分类数据集
https://en.d2l.ai/chapter_linear-classification/image-classification-dataset.html
https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html

===== The Base Classification Model

===== Softmax Regression实现
- 实现Softmax的注意事项
溢出问题:
进行softmax的指数函数的运算时，加上(或者减去)某个常数并不会改变运算的结果。为了防止溢出，一般会使用输入信号中的最大值。

- API
https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html
https://www.tensorflow.org/api_docs/python/tf/nn/softmax

===== Generalization in Classification

===== Environment and Distribution Shift

==== 参考
https://en.d2l.ai/chapter_linear-regression/index.html
https://en.d2l.ai/chapter_linear-classification/index.html
https://zh.d2l.ai/chapter_linear-networks/index.html

=== 多层感知机(multilayer perceptrons，MLP)
==== 概念
- 感知机
感知机(perceptron)由美国学者在1957年提出。
感知机被认为是神经网络(深度学习)的起源算法。
感知机的局限性就在于它只能表示线性空间，然而感知机通过叠加层能够进行非线性的表示，理论上还可以表示计算机进行的处理。

    • 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。
    • 感知机将权重和偏置设定为参数。
    • 使用感知机可以表示与门和或门等逻辑电路。
    • 异或门无法通过单层感知机来表示。
    • 使用2层感知机可以表示异或门。
    • 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。
    • 多层感知机(在理论上)可以表示计算机。

- 多层感知机
最简单的深度网络称为多层感知机。
多层感知机由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。
多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。
常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

- 应用领域
分类任务: 如手写数字识别、图像分类、文本分类等。
回归任务: 如预测股票价格、气温变化等。

==== 隐藏层
- 线性模型可能会出错
- 在网络中加入隐藏层
- 从线性到非线性
- 通用近似定理

==== 激活函数
===== 概念
激活函数(activation function)通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。

大多数激活函数都是非线性的。

===== 线性激活函数
线性激活函数的形式为: f(x)=ax+b 其中a和b是常数。

无论神经网络有多少层，使用线性激活函数的多层神经网络等效于一个单层线性模型。这是因为多个线性变换的组合仍然是线性变换。

线性激活函数无法引入非线性能力，因此无法解决复杂的非线性问题(如图像分类、自然语言处理等)。

适用场景:

    线性激活函数通常用于回归问题的输出层(例如预测连续值)。
    在某些特殊情况下(如降维或特征提取)，线性激活函数可能被用于隐藏层。

===== 非线性激活函数
非线性激活函数(如ReLU、Sigmoid、Tanh等)是神经网络中最常用的激活函数。

===== ReLU函数
====== 概念
最受欢迎的激活函数是修正线性单元(rectified linear unit，ReLU)，它实现简单，同时在各种预测任务中表现良好。ReLU提供了一种非常简单的非线性变换。给定元素x，ReLU函数被定义为该元素与的最大值: ReLU(x) = max(x, 0)

```math
\text{ReLU}(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
```

通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。

使用ReLU的原因是，它求导表现得特别好: 要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。

注意，ReLU函数有许多变体，包括参数化ReLU(Parameterized ReLU，pReLU)函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过:

    ReLU(x) = max(x, 0) + a * min(x, 0)

- 特点

    计算简单:
        ReLU的计算只需要一个简单的阈值操作(比较x和0)，因此计算速度非常快。
    非线性:
        ReLU是非线性函数，能够引入非线性能力，使神经网络可以学习复杂的模式。
    缓解梯度消失问题:
        在正区间(x>0)，ReLU的梯度恒为1，避免了梯度消失问题(尤其是在深层网络中)。
    稀疏激活:
        当输入为负时，ReLU输出0，这使得网络中的部分神经元被"关闭"，从而实现了稀疏激活，减少了计算量。

- 优点

    计算高效: ReLU的计算非常简单，只需要比较和取最大值操作。
    加速收敛: 由于在正区间的梯度恒为1，ReLU能够加速神经网络的训练过程。
    缓解梯度消失: 相比于Sigmoid和Tanh等激活函数，ReLU在正区间的梯度不会饱和，因此更适合深层网络。

- 缺点

    Dead ReLU问题:
        当输入 x≤0 时，ReLU的输出为0，梯度也为0。
        如果某些神经元在训练过程中始终输出0(即"死亡")，这些神经元将无法更新参数。
        这种现象通常发生在学习率设置过高或权重初始化不当时。
    非零中心化:
        ReLU的输出不是以0为中心的，这可能导致训练过程中的梯度更新效率降低。

- 变体

    Leaky ReLU:
        解决了Dead ReLU问题，允许负输入有较小的梯度。
    Parametric ReLU (PReLU):
        与Leaky ReLU类似，但 α 是一个可学习的参数。
        更灵活，能够自适应地调整负区间的斜率。
    Exponential Linear Unit (ELU):
        在负区间具有平滑的曲线，能够缓解Dead ReLU问题，同时加速收敛。
    Softplus:
        是ReLU的平滑版本，但计算较慢。

====== Leaky ReLU
```math
\text{LeakyReLU}(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}
```

====== ELU(Exponential Linear Unit)
```math
\text{ELU}(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
\alpha (e^x - 1) & \text{if } x \leq 0
\end{cases}
```

====== Softplus
```math
\text{Softplus}(x) = \log(1 + e^x)
```

===== sigmoid函数
- 概念
https://en.wikipedia.org/wiki/Sigmoid_function
也称S型函数，有多种Sigmoid函数。
特点是在一段区间内较快增长，该区间外区域饱和。

- 优点

    输出范围有限，适合需要概率输出的场景
    平滑且易于求导

- 缺点

    容易导致梯度消失，尤其在深层网络中
    输出不以零为中心，可能影响收敛速度
    计算复杂度较高，尤其是对于大范围输入

- 适用场景

    二分类问题的输出层
    需要将输出限制在 (0, 1) 之间的场景

- 参考
https://en.d2l.ai/chapter_multilayer-perceptrons/mlp.html#sigmoid-function
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html#sigmoid

===== tanh函数
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html#tanh

==== 实现
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html
https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html

==== 权重衰减(weight decay)
https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html

==== 暂退法(dropout)
暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。之所以被称为暂退法，因为从表面上看是在训练过程中丢弃(drop out)一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

    暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元
    暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用
    暂退法将活性值h替换为具有期望值的随机变量
    暂退法仅在训练期间使用

https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html

==== 前向传播、反向传播和计算图
- 前向传播
前向传播(forward propagation或forward pass): 按顺序(从输入层到输出层)计算和存储神经网络中每层的结果。
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2

- 反向传播
反向传播(backward propagation或backpropagation)指的是计算神经网络参数梯度的方法。根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4

- 训练神经网络
在训练神经网络时，前向传播和反向传播相互依赖。
在初始化模型参数后，交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。带来的影响之一是需要保留中间值，直到反向传播完成。这也是训练比单纯的预测需要更多的内存(显存)的原因之一。

- 参考
https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html

==== 数值稳定性和模型初始化
- 梯度消失
vanishing gradient
在深度神经网络(尤其是使用反向传播算法进行训练的网络)中，梯度消失是指在反向传播过程中，梯度(用于更新网络权重的信号)随着网络层数的增加而变得越来越小，最终接近于零的现象。

- 梯度爆炸
exploding gradient
与梯度消失相反，梯度爆炸是指在反向传播过程中，梯度随着网络层数的增加而变得越来越大，最终导致数值溢出的现象。

- 参数初始化
https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#id6

https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html

==== 境和分布偏移
环境偏移:
Environment Shift
指模型的训练环境和实际应用环境之间存在差异。例如，在自动驾驶汽车的目标检测模型训练中，训练数据是在晴天收集的，而实际应用时可能会遇到雨天、雾天等不同天气状况，这种从晴天环境到其他天气环境的变化就是环境偏移。

分布偏移:
Distribution Shift
指数据分布在不同阶段(如训练阶段和测试阶段)发生了变化。具体来说，数据的输入特征、标签等的联合分布(其中是输入特征，是标签)在训练和测试过程中不再相同。以图像分类为例，训练集中猫的图像可能主要是某种品种且拍摄角度、背景比较单一，而测试集中猫的图像品种更多样、拍摄角度和背景也更复杂，这就导致了分布偏移。

https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html

==== 多层感知机的缺陷

=== Builder's Guide
==== 层和块(Layers and Modules)
一个块可以由许多层组成；一个块可以由许多块组成。
块可以包含代码。
块负责大量的内部处理，包括参数初始化和反向传播。
层和块的顺序连接由Sequential块处理。

- 自定义块
https://en.d2l.ai/chapter_builders-guide/model-construction.html#a-custom-module
https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#id3

- 顺序块
Sequential的设计是为了把其它模块串起来。
https://en.d2l.ai/chapter_builders-guide/model-construction.html#the-sequential-module
https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#id4

- 在前向传播函数中执行代码
https://en.d2l.ai/chapter_builders-guide/model-construction.html#executing-code-in-the-forward-propagation-method
https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#id5

==== 参数管理(Parameter Management)
- 参数访问
https://en.d2l.ai/chapter_builders-guide/parameters.html#parameter-access
https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html#id2

- 参数绑定
https://en.d2l.ai/chapter_builders-guide/parameters.html#tied-parameters
https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html#id9

==== 参数初始化(Parameter Initialization)
https://en.d2l.ai/chapter_builders-guide/init-param.html
https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html#id6

==== 延后初始化(Lazy Initialization)
defers initialization，即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。
https://en.d2l.ai/chapter_builders-guide/lazy-init.html
https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html

==== 自定义层
- 不带参数的层
https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html#id2

- 带参数的层
https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html#id3

==== 读写文件
- 加载和保存张量
https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html#id2

- 加载和保存模型参数
https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html#id3

==== GPU
- 计算设备
https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html#id1

- 张量与GPU
https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html#id2

- 神经网络与GPU
https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html#id6

=== 卷积神经网络(Convolutional Neural Networks)
==== 概览
===== 优点
自动特征提取:
CNN能够自动从原始数据(如图像)中提取特征，无需手动设计特征提取方法。这大大简化了传统计算机视觉任务中的特征工程工作。

局部感受野:
卷积操作通过局部感受野的方式(即卷积核只关注局部区域)进行特征提取，减少了计算量，使得网络能够更高效地学习局部特征。

参数共享:
卷积层中的卷积核在整个输入数据上共享参数，这不仅减少了参数的数量，还提高了计算效率。

平移不变性:
通过卷积操作和池化层，CNN能够对输入数据的平移具有一定的鲁棒性，即在图像中移动目标时，网络依然能识别出该目标。

适应性强:
CNN对于不同类型的输入(如图像、语音等)具有很强的适应性，能够在不同领域中应用，且可以通过深层网络结构不断优化和提升性能。

高效的特征学习:
由于CNN结构的层次化特性，网络可以逐层学习从简单到复杂的特征，有助于提升模型的识别能力。

===== 缺点
计算量大:
尽管卷积层能减少参数量，但在处理大型图像或复杂任务时，CNN仍然需要大量的计算资源和较长的训练时间。这在计算力不足的设备上可能成为瓶颈。

需要大量标注数据:
CNN需要大量标注数据来进行训练，否则可能会导致过拟合问题。对于数据稀缺的任务，CNN可能表现不佳。

缺乏全局信息:
虽然CNN擅长提取局部特征，但由于局部感受野的限制，它可能无法很好地捕捉全局上下文信息。尽管池化和全连接层在某种程度上能弥补这一点，但在一些任务中仍然存在局部特征不足的情况。

模型解释性差:
CNN是一个“黑箱”模型，它通过复杂的网络层来进行决策。虽然网络能够产生高精度的结果，但它的决策过程和内部特征难以解释，这对于一些需要透明性和可解释性的应用场景(如医学影像)可能是一个缺点。

过拟合风险:
尽管通过正则化、数据增强等方法可以减轻过拟合，但在训练数据不足或网络过于复杂时，CNN仍然可能发生过拟合，导致模型在新数据上的表现不佳。

对小样本数据的适应性差:
CNN需要大量的数据来训练，尤其是在深层网络中。对于小样本数据，CNN的表现可能不如其他更为简单的机器学习模型。

===== 未来发展方向
轻量化模型: 如MobileNet、ShuffleNet。

自监督学习: 减少对标注数据的依赖。

多模态学习: 结合图像、文本、音频等多种数据。

==== 从全连接层到卷积
===== 不变性
卷积神经网络将空间不变性(spatial invariance)的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。
平移不变性(translation invariance):
不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为"平移不变性"。
局部性(locality):
神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是"局部性"原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。
https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id2

===== 多层感知机的限制
- 平移不变性
https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id4
图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。

- 局部性
https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id5
局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。

===== 卷积
https://en.wikipedia.org/wiki/Convolution

在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。

卷积神经网络是一类特殊的神经网络，它可以包含多个卷积层。

https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#id6

===== 通道(Channels)
https://en.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#channels

多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

==== 图像卷积(Convolutions for Images)
===== 互相关运算
卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算(cross-correlation)，而不是卷积运算。
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id2

===== 卷积层
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id3

===== 图像中目标的边缘检测
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id4

===== 学习卷积核
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id5

===== 互相关和卷积
https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#id6

===== Feature Map and Receptive Field
输出的卷积层有时被称为特征映射feature map，因为它可以被视为一个输入映射到下一层的空间维度的转换器。
在卷积神经网络中，对于某一层的任意元素，其receptive field是指在前向传播期间可能影响计算的所有元素(来自所有先前层)。
https://en.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#feature-map-and-receptive-field

==== 填充和步幅(Padding and Stride)
填充和步幅可用于有效地调整数据的维度。

- 填充
填充可以增加输出的高度和宽度。常用来使输出与输入具有相同的高和宽。
https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#id2

- 步幅
步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n。
https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#id3

==== 多输入多输出通道(Multiple Input and Multiple Output Channels)
- 多输入通道
https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#id2

- 多输出通道
https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#id3

- 1 X 1 卷积层
当以每像素为基础应用时，1 X 1卷积层相当于全连接层。
1 X 1卷积层通常用于调整网络层的通道数量和控制模型复杂性。
https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#times-1

==== 汇聚层(Pooling)
汇聚层: 降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

- 最大汇聚层(maximum pooling)和平均汇聚层(average pooling)
https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html#id2

- 填充和步幅
https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html#id3

- 多个通道
https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html#id4

==== LeNet
LeNet是最早发布的卷积神经网络之一。
https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html

=== 现代卷积神经网络(Modern Convolutional Neural Networks)
==== AlexNet
2012年

- Representation Learning(学习表征)
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id3

- 模型设计
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id12

- 激活函数
Q: 为什么AlexNet将sigmoid激活函数改为更简单的ReLU激活函数？
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id13

- 容量控制和预处理
AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。这使得模型更健壮，更大的样本量有效地减少了过拟合。
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id14

- 读取数据集
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id15

- 训练AlexNet
https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html#id16

==== VGG
2014

===== VGG块
https://zh.d2l.ai/chapter_convolutional-modern/vgg.html#id1

===== VGG网络
https://zh.d2l.ai/chapter_convolutional-modern/vgg.html#id3

==== NiN(Network in network)
2013
LeNet、AlexNet和VGG都有一个共同的设计模式:通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。
AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。网络中的网络(NiN)提供了一个非常简单的解决方案:在每个像素的通道上分别使用多层感知机。

===== NiN块
NiN使用由一个卷积层和多个1X1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
https://zh.d2l.ai/chapter_convolutional-modern/nin.html#id2

===== NiN模型
NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层(即在所有位置上进行求和)。该汇聚层通道数量为所需的输出数量。
移除全连接层可减少过拟合，同时显著减少NiN的参数。
https://zh.d2l.ai/chapter_convolutional-modern/nin.html#id3

==== Multi-Branch Networks(GoogLeNet)
2014

===== Inception块
https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html#inception

===== GoogLeNet模型
https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html#id2

==== 批量规范化(batch normalization)
2015

- 训练深层网络
批量规范化应用于单个可选层(也可以应用到所有层)，其原理如下:在每次训练迭代中，首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。
批量规范化层在训练模式(通过小批量统计数据规范化)和预测模式(通过数据集统计规范化)中的功能不同。在训练过程中，无法得知使用整个数据集来估计平均值和方差，因此只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id3

- 批量规范化层
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id6

- 实现
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id10
https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html#id11

==== 残差网络(ResNet)
- 函数类
https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id1

- 残差块
https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id3

- ResNet模型
https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id4

==== 稠密连接网络(DenseNet)
2017
稠密连接网络在某种程度上是ResNet的逻辑扩展。

=== 循环神经网络(Recurrent Neural Networks)
==== 概览
===== 优点
- 处理序列数据
RNN能够处理序列数据(如文本、语音、时间序列等)，并捕捉到数据之间的时间依赖关系。这使得它特别适用于自然语言处理、语音识别、机器翻译等任务。

- 共享权重
在RNN中，同一组权重会在每个时间步共享，这样可以大大减少参数数量，提高计算效率。

- 记忆性
RNN能够通过隐藏状态(hidden state)记住之前的输入信息，从而在处理时序数据时保留上下文信息，这对于时间序列分析或语言建模非常重要。

- 灵活性
RNN适用于不同长度的输入和输出序列，可以处理变长的序列任务，如翻译不同长度的句子，或者预测不同长度的时间序列。

===== 缺点
- 梯度消失和梯度爆炸问题
在训练过程中，RNN可能会遇到梯度消失或梯度爆炸的问题。尤其是当处理长时间序列时，梯度在反向传播时会变得非常小，导致模型无法有效地学习到长期依赖关系。反之，梯度也可能变得非常大，导致权重更新不稳定。

- 训练困难
RNN的训练比传统的前馈神经网络更困难，因为它涉及到时间步之间的依赖关系，计算过程更为复杂，需要更长的时间进行训练。

- 难以捕捉长期依赖
尽管RNN可以处理时序数据，但它通常只能捕捉到较短时间范围内的依赖关系。对于需要长时间记忆的任务，RNN的效果较差。

- 计算资源消耗大
由于RNN需要在每个时间步更新隐藏状态，因此它通常需要较长的计算时间，并且在处理大规模数据时，计算资源消耗较大。

===== 未来发展方向
注意力机制(Attention Mechanism): 提高模型对重要信息的关注能力。

Transformer模型: 基于自注意力机制，替代RNN在NLP任务中的应用。

强化学习结合RNN: 用于序列决策任务。

==== 序列模型
===== 自回归模型(Autoregressive Models)
- 特点
自回归模型能够有效地捕捉时间序列数据中的线性关系。
通过最小化预测误差来估计模型参数，常用的方法包括最小二乘法。
可以用于预测时间序列的未来值，为动态系统的理解和预测提供有力工具。
线性关系: AR模型假定当前观测值与过去观测值之间存在线性关系。
平稳性: 对于AR模型，通常要求时间序列是平稳的，意味着它的统计特性(如均值和方差)不随时间变化。非平稳序列可能需要通过差分等方法转换成平稳序列后再应用AR模型。
残差独立同分布:模型中的误差项应满足独立同分布条件，这保证了估计的有效性和预测的准确性。

- 限制
自回归模型通常假设时间序列是平稳的，即统计特性(如均值、方差)不随时间变化。如果时间序列不满足平稳性条件，可能需要进行适当的变换或差分处理。
自回归模型的阶数选择是一个重要问题。阶数过高可能导致模型过于复杂，增加计算量和过拟合风险；阶数过低则可能无法充分捕捉时间序列的动态特性。因此，需要在模型的拟合度和过拟合风险之间取得平衡。
自回归模型只适用于预测与自身前期相关的经济现象或受自身历史因素影响较大的现象，如矿的开采量、各种自然资源产量等。对于受社会因素影响较大的经济现象或复杂系统，可能需要考虑其他更复杂的模型或方法。

https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id5

===== 马尔可夫模型(Markov Model)
马尔可夫模型是一类用于描述具有马尔可夫性质的随机过程的概率模型。在这样的过程中，未来的状态仅依赖于当前的状态，而不受之前状态的影响。这种特性被称为"无记忆性"或"马尔可夫性质"。

https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id6

===== The Order of Decoding
https://en.d2l.ai/chapter_recurrent-neural-networks/sequence.html#the-order-of-decoding

===== 训练与预测
https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id10
https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id11

==== 文本预处理
https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html#sec-text-preprocessing

==== 语言模型和数据集
https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html

==== 循环神经网络
隐状态(hidden state)，也称为隐藏变量(hidden variable)，它存储了到时间步t-1的序列信息。

隐藏层和隐状态指的是两个截然不同的概念:
隐藏层是在从输入到输出的路径上(以观测角度来理解)的隐藏的层；
隐状态则是在给定步骤所做的任何事情(以技术角度来定义)的输入，并且这些状态只能通过先前时间步的数据来计算。

循环神经网络是具有隐状态的神经网络。

===== 无隐状态的神经网络
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#id2

===== 隐状态的循环神经网络
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-rnn-w-hidden-states

===== 基于循环神经网络的字符级语言模型
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#id4

===== 困惑度(Perplexity)
TODO: 补充计算

困惑度的最好的理解是下一个词元的实际选择数的调和平均数:
在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一词元的数量。事实上，如果在没有任何压缩的情况下存储序列，这将是能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。

https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#perplexity

==== 实现
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html
https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html

==== 通过时间反向传播(Backpropagation Through Time)
通过时间反向传播仅仅适用于反向传播在具有隐状态的序列模型。
截断是计算方便性和数值稳定性的需要。截断包括:规则截断和随机截断。
矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。
为了计算的效率，通过时间反向传播在计算期间会缓存中间值。

===== 循环神经网络的梯度分析
https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#subsec-bptt-analysis

===== 通过时间反向传播的细节
https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#id10

=== 现代循环神经网络
==== 概要
循环神经网络在实践中一个常见问题是数值不稳定性。尽管已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型来进一步处理它。

==== 长短期记忆网络(long short-term memory, LSTM)
===== 概念
1997
长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。
解决这一问题的最早方法之一是长短期存储器(LSTM)，它有许多与门控循环单元一样的属性。
长短期记忆网络的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年。

长短期记忆网络有三种类型的门: 输入门(Input Gate)、遗忘门(Forget Gate)和输出门(Output Gate)。
长短期记忆网络的隐藏层输出包括"隐状态"和"记忆元"。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
长短期记忆网络可以缓解梯度消失和梯度爆炸。

- 优势

    处理长序列数据:
        解决了传统RNN在处理长序列数据时的梯度消失和梯度爆炸问题，能够捕捉到时间序列数据中的长期依赖关系。
    灵活性:
        LSTM可以根据任务需求调整记忆单元中的信息流，适用于各种复杂的序列建模任务。
    广泛应用:
        LSTM在自然语言处理、语音识别、时间序列预测等领域取得了显著的成功。

- 局限性

    计算复杂度高:
        LSTM由于引入了门控机制，计算复杂度较高，训练时间较长。
    参数调优困难:
        LSTM的超参数(如隐藏层大小、学习率等)需要仔细调优，否则可能导致模型性能不佳。
    内存消耗大:
        LSTM在处理长序列数据时，内存消耗较大，尤其是在处理大规模数据集时。

===== 核心结构
LSTM的核心在于其记忆单元(Cell State)和三个门控机制: 输入门(Input Gate)、遗忘门(Forget Gate)和输出门(Output Gate)。这些门控机制通过Sigmoid函数和逐元素乘法操作来控制信息的流动。

- 记忆单元(Cell State)
LSTM的关键部分，负责在时间步之间传递信息。记忆单元可以看作是一个"传送带"，能够在整个链上保持信息的连续性。

- 遗忘门(Forget Gate)
决定哪些信息从记忆单元中丢弃。它通过Sigmoid函数输出一个0到1之间的值，0表示完全丢弃，1表示完全保留。

- 输入门(Input Gate)
决定哪些新信息将存储在记忆单元中。它由两部分组成: Sigmoid函数决定哪些值需要更新，tanh函数生成新的候选值。

- 输出门(Output Gate)
决定记忆单元中的哪些信息将输出到当前时间步的隐藏状态。它通过Sigmoid函数决定输出哪些部分，然后通过tanh函数将记忆单元的值压缩到-1到1之间，最后与Sigmoid函数的输出相乘得到最终的隐藏状态。

===== 门控记忆元(Gated Memory Cell)
https://en.d2l.ai/chapter_recurrent-modern/lstm.html#gated-memory-cell
https://zh.d2l.ai/chapter_recurrent-modern/lstm.html#id2

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/lstm.html#implementation-from-scratch
https://zh.d2l.ai/chapter_recurrent-modern/lstm.html#id7

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/lstm.html#concise-implementation
https://zh.d2l.ai/chapter_recurrent-modern/lstm.html#id11

==== 门控循环单元(Gated Recurrent Unit, GRU)
2014
门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。
重置门有助于捕获序列中的短期依赖关系。
更新门有助于捕获序列中的长期依赖关系。
重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。

===== Reset Gate and Update Gate
https://en.d2l.ai/chapter_recurrent-modern/gru.html#reset-gate-and-update-gate
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id5

===== Candidate Hidden State
https://en.d2l.ai/chapter_recurrent-modern/gru.html#candidate-hidden-state
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id6

===== Hidden State
https://en.d2l.ai/chapter_recurrent-modern/gru.html#hidden-state
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id7

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/gru.html#implementation-from-scratch
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id8

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/gru.html#concise-implementation
https://zh.d2l.ai/chapter_recurrent-modern/gru.html#id12

==== 深度循环神经网络(Deep Recurrent Neural Networks)
===== 概要
在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。
有许多不同风格的深度循环神经网络，如长短期记忆网络、门控循环单元、或经典循环神经网络，这些模型在深度学习框架的高级API中都有涵盖。
总体而言，深度循环神经网络需要大量的调参(如学习率和修剪)来确保合适的收敛，模型的初始化也需要谨慎。

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/deep-rnn.html#implementation-from-scratch

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/deep-rnn.html#concise-implementation

==== 双向循环神经网络(Bidirectional Recurrent Neural Networks)
在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。
双向循环神经网络与概率图模型中的"前向-后向"算法具有相似性。
双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。
由于梯度链更长，因此双向循环神经网络的训练代价非常高。

===== Implementation from Scratch
https://en.d2l.ai/chapter_recurrent-modern/bi-rnn.html#implementation-from-scratch

===== Concise Implementation
https://en.d2l.ai/chapter_recurrent-modern/bi-rnn.html#concise-implementation

==== 机器翻译与数据集(Machine Translation and the Dataset)
机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。
使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，可以将低频词元视为相同的未知词元。
通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。

https://en.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html
https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html

==== 编码器-解码器架构(The Encoder–Decoder Architecture)
编码器-解码器架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
解码器将具有固定形状的编码状态映射为长度可变的序列。
https://en.d2l.ai/chapter_recurrent-modern/encoder-decoder.html
https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html

==== 序列到序列学习(Sequence-to-Sequence Learning for Machine Translation)
===== Teacher Forcing
https://en.wikipedia.org/wiki/Teacher_forcing
Teacher Forcing是一种用来训练循环神经网络模型的方法，这种方法以上一时刻的输出作为下一时刻的输入。
在训练时，Teacher forcing是通过使用第t时刻的来自于训练集的期望输出y(t)作为下一时刻的输入x(t+1)，而不是直接使用网络的实际输出。
Q: 优点与缺点？
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#teacher-forcing

===== Encoder
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#encoder

===== Decoder
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#decoder

===== Encoder–Decoder for Sequence-to-Sequence Learning
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#encoderdecoder-for-sequence-to-sequence-learning

===== Loss Function with Masking
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#loss-function-with-masking

===== Training
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#training

===== Prediction
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#prediction

===== Evaluation of Predicted Sequences
https://en.d2l.ai/chapter_recurrent-modern/seq2seq.html#evaluation-of-predicted-sequences

==== 束搜索(Beam Search)
===== 概念
https://zhuanlan.zhihu.com/p/82829880

===== Greedy Search
https://en.d2l.ai/chapter_recurrent-modern/beam-search.html#greedy-search
https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html#id2

===== Exhaustive Search(穷举搜索)
https://en.d2l.ai/chapter_recurrent-modern/beam-search.html#exhaustive-search
https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html#id3

===== Beam Search
https://en.d2l.ai/chapter_recurrent-modern/beam-search.html#id1
https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html#id5

=== Attention Mechanisms and Transformers
==== 概念
https://zhuanlan.zhihu.com/p/104393915

注意力机制的核心思想是:
在处理一个输入(如一段文本、图像等)时，并不是所有信息都同等重要。
通过赋予输入中不同部分不同的权重(即注意力权重)，模型可以专注于更加重要的部分，从而提高效率和效果。

==== Queries, Keys, and Values

==== Attention Scoring Functions

==== The Bahdanau Attention Mechanism(Bahdanau注意力)

==== Multi-Head Attention(多头注意力)

==== Self-Attention and Positional Encoding(自注意力与位置编码)

==== The Transformer Architecture

==== Transformers for Vision

==== Large-Scale Pretraining with Transformers

=== 优化算法
==== 概念
- 梯度下降
* 优点:
    ** 简单易实现:
        梯度下降的核心思想直观，只需计算目标函数的梯度并沿负梯度方向更新参数。
    ** 广泛适用性:
        适用于大多数可微的目标函数，尤其在机器学习和深度学习中表现良好。
    ** 可扩展性强:
        通过随机梯度下降(SGD)或小批量梯度下降(Mini-batch Gradient Descent)，可以高效处理大规模数据集。
    ** 收敛性保证:
        对于凸函数，梯度下降能保证收敛到全局最优解；对于非凸函数，通常也能收敛到局部最优解。
    ** 灵活性强:
        可以与其他优化技术(如动量法、学习率衰减等)结合，进一步提升性能。
* 缺点:
    ** 局部最优问题:
        对于非凸函数，梯度下降可能陷入局部最优解，而非全局最优解。
    ** 学习率选择困难:
        学习率的选择对算法性能至关重要:
            学习率过小: 收敛速度慢，训练时间长。
            学习率过大: 可能导致震荡，甚至无法收敛。
    ** 收敛速度慢:
        在高维空间或平坦区域，梯度下降的收敛速度可能非常慢。
    ** 对初始值敏感:
        初始参数的选择可能影响最终结果，尤其是在非凸优化问题中。
    ** 梯度消失/爆炸问题:
        在深度神经网络中，梯度可能变得非常小(消失)或非常大(爆炸)，导致训练困难。
    ** 需要计算梯度:
        对于复杂函数，梯度的计算可能非常耗时，尤其是当目标函数不可微时，梯度下降无法直接使用。
    ** 对噪声敏感:
        在随机梯度下降(SGD)中，由于使用单个样本或小批量样本的梯度，更新方向可能带有噪声，导致震荡。
* 改进:
    为了改善梯度下降的缺点，很多变种方法(如随机梯度下降(SGD)、Adam、RMSprop等)应运而生，通过自适应调整学习率、加速收敛等方式提高其性能。

- 牛顿法 (Newton's Method)
* 原理:
    牛顿法是一种基于二阶导数的优化算法。它使用目标函数的二阶导数(Hessian矩阵)来近似地寻找最小值。
* 优点:
    相比梯度下降，牛顿法通常收敛速度更快，因为它考虑了二阶信息。
* 缺点:
    计算二阶导数(Hessian矩阵)非常耗时且空间复杂度高，尤其在高维情况下不适用。

- 拟牛顿法 (Quasi-Newton Methods)
* 原理:
    拟牛顿法是牛顿法的一种近似方法，它不直接计算Hessian矩阵，而是使用近似的方法来更新Hessian矩阵。
* 常见方法:
    BFGS (Broyden–Fletcher–Goldfarb–Shanno)
    L-BFGS (Limited-memory BFGS)
* 优点:
    更高效，尤其适用于大规模问题，避免了计算完整的Hessian矩阵。
* 缺点:
    尽管比牛顿法高效，但依然需要更多计算资源，相对于梯度下降可能仍然较为复杂。

- 共轭梯度法 (Conjugate Gradient)
* 原理:
    共轭梯度法是一种优化算法，主要用于解决线性方程组和二次优化问题。它通过构造一个新的方向来替代标准的梯度下降。
* 优点:
    不需要存储Hessian矩阵，适用于大规模问题。
* 缺点:
    主要用于二次优化问题，不适用于非线性问题。

- 模拟退火 (Simulated Annealing)
* 原理:
    模拟退火是一种基于随机搜索的全局优化算法，它模仿物理退火过程，即在高温时允许接受较差的解，并随着温度逐渐降低，逐步收敛到最优解。
* 优点:
    能够逃脱局部最优解，适用于全局优化问题。
* 缺点:
    收敛速度较慢，参数调整困难。

- 遗传算法 (Genetic Algorithm)
* 原理:
    遗传算法是一种模拟自然选择过程的启发式算法。通过选择、交叉、变异等操作，生成下一代解，并不断改进。
* 优点:
    强大的全局搜索能力，适用于复杂的优化问题。
* 缺点:
    计算量大，收敛速度慢，难以精确控制。

- 粒子群优化 (Particle Swarm Optimization, PSO)
* 原理:
    粒子群优化模拟了鸟群觅食的行为，通过群体中的每个"粒子"来搜索解空间。
* 优点:
    易于实现，具有全局优化能力。
* 缺点:
    收敛速度较慢，容易陷入局部最优。

- AdaGrad (Adaptive Gradient Algorithm)
* 原理:
    AdaGrad 是一种基于梯度的自适应学习率方法，根据每个参数的历史梯度大小动态调整学习率。
* 优点:
    自适应调整学习率，有助于避免梯度下降过程中出现过大的学习率。
* 缺点:
    学习率随着训练的进行逐渐衰减，可能导致后期收敛过慢。

- RMSprop
* 原理:
    RMSprop(Root Mean Square Propagation)是AdaGrad的一个改进版本，通过引入一个衰减因子来控制历史梯度的影响，防止学习率下降过快。
* 优点:
    在非平稳目标函数中表现良好，常用于神经网络训练。
* 缺点:
    需要调节一些超参数，如衰减因子。

- Adam (Adaptive Moment Estimation)
* 原理:
    Adam结合了AdaGrad和RMSprop的思想，利用一阶矩(均值)和二阶矩(方差)来动态调整学习率。
* 优点:
    在大多数情况下收敛得非常快，并且适用于大规模数据和参数。
* 缺点:
    参数较多，可能需要进行调节。

- Adadelta
* 原理:
    Adadelta是RMSprop的一个改进版，解决了AdaGrad学习率不断衰减的问题，采用了一个窗口来计算梯度的平方。
* 优点:
    能够避免学习率衰减的问题，并且在某些问题上表现比Adam更好。
* 缺点:
    比Adam更难调优，并且没有直接学习率参数。

- Nesterov加速梯度 (Nesterov Accelerated Gradient, NAG)
* 原理:
    NAG 是在标准梯度下降法基础上的一种改进，通过在更新之前提前利用梯度来加速收敛。
* 优点:
    可以显著提高收敛速度，尤其在某些非凸问题中表现优异。
* 缺点:
    需要额外的计算步骤，相对复杂。

- L-BFGS (Limited-memory BFGS)
* 原理:
    L-BFGS 是一种拟牛顿法，它通过有限内存来近似计算Hessian矩阵，从而减少计算和存储开销。
* 优点:
    高效，尤其适用于内存受限的场景。
* 缺点:
    相较于梯度下降，计算量依然较大。

- 深度学习中的优化方法
* SGD (Stochastic Gradient Descent):
    是梯度下降的一种变种，每次迭代仅使用一小批数据计算梯度，适用于大规模数据。
* Mini-batch SGD:
    将数据分成小批次进行计算，可以平衡计算效率和收敛速度。

- 小结
* 全局优化:
    遗传算法、粒子群优化、模拟退火。
* 二阶方法:
    牛顿法、拟牛顿法。
* 自适应方法:
    AdaGrad、RMSprop、Adam、Adadelta。
* 加速方法:
    Nesterov加速梯度、BFGS、L-BFGS。
* 每种优化方法有其独特的适用场景
    选择合适的优化方法通常依赖于问题的规模、复杂性、收敛要求以及是否需要全局搜索等因素。

==== Optimization and Deep Learning
===== 参考
https://en.d2l.ai/chapter_optimization/optimization-intro.html

==== Convexity(凸性)
===== 参考
https://en.d2l.ai/chapter_optimization/convexity.html

==== Gradient Descent(梯度下降)

===== 参考
https://en.d2l.ai/chapter_optimization/gd.html

==== Stochastic Gradient Descent(随机梯度下降)
===== 参考
https://en.d2l.ai/chapter_optimization/sgd.html

==== Minibatch Stochastic Gradient Descent(小批量随机梯度下降)
===== 参考
https://en.d2l.ai/chapter_optimization/minibatch-sgd.html

==== Momentum(动量法)
===== 概念
动量法是一种用于优化梯度下降算法的技术，旨在加速收敛并减少震荡。它通过引入动量的概念，利用历史梯度信息来调整当前的更新方向，从而在优化过程中更有效地穿越平坦区域和狭窄的谷底。

===== 核心思想
动量法借鉴了物理学中的动量概念，即在更新参数时不仅考虑当前的梯度，还考虑之前梯度的累积效应。具体来说，动量法通过引入一个动量变量v，来累积之前的梯度信息，并在更新时结合当前梯度和动量变量。

===== 算法步骤
====== 计算动量项
```math
v_t = \beta v_{t-1} + (1 - \beta) \nabla J(\theta_t)
```

    其中:
        vt 是当前时刻的动量变量
        β 是动量系数(通常取值为 0.9 左右)，控制历史梯度的影响。
        ∇J(θt) 是当前时刻的梯度

====== 更新参数
```math
\theta_{t+1} = \theta_t - \eta v_t
```

    其中:
        η 是学习率
        θt 是当前参数值

===== 优缺点
- 优点
* 加速收敛:在梯度方向一致时，动量法会加速更新，从而更快地接近最优解。
* 减少震荡:在梯度方向变化较大时，动量法可以平滑更新路径，减少震荡。
* 逃离局部极小值:动量法可以帮助模型逃离局部极小值，尤其是在损失函数表面不平滑时。

- 缺点
* 需要调参:动量系数 β 需要手动设置，选择不当可能影响性能。
* 可能 overshooting:如果动量过大，可能会导致参数更新过度，错过最优解。

- 与普通梯度下降的对比
* 普通梯度下降:每次更新只依赖当前梯度，容易陷入局部极小值或震荡。
* 动量法:通过累积历史梯度信息，能够更稳定地更新参数，加速收敛。

===== 变体
Nesterov Accelerated Gradient (NAG):
在计算梯度时，先根据动量项预估下一步的参数位置，再计算梯度。这种方法可以进一步减少震荡，提高收敛速度。

===== 参考
https://en.d2l.ai/chapter_optimization/momentum.html

==== Adagrad

==== RMSProp
===== 概念
RMSprop(Root Mean Square Propagation)是一种自适应学习率优化算法，广泛用于训练深度学习模型。它由 Geoffrey Hinton 提出，旨在解决梯度下降中学习率固定的问题，特别是在非凸优化问题中表现良好。RMSprop 通过调整每个参数的学习率，使得训练过程更加稳定和高效。

===== 核心思想
RMSprop 的核心思想是对每个参数的学习率进行自适应调整，具体方法如下:

- 梯度平方的指数移动平均
* 计算梯度的平方的指数移动平均(Exponential Moving Average, EMA)，用于调整每个参数的学习率。
* 这样可以抑制梯度变化较大的方向，同时放大梯度变化较小的方向。

- 自适应学习率
* 通过梯度平方的移动平均，RMSprop 动态调整每个参数的学习率。
* 梯度较大的参数会获得较小的学习率，梯度较小的参数会获得较大的学习率。

===== 算法步骤

```math
v_t = \beta v_{t-1} + (1 - \beta) g_t^2
```

```math
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} g_t
```

ifndef::env-github[]
latexmath:[$v_t = \beta v_{t-1} + (1 - \beta) g_t^2$]
latexmath:[$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} g_t$]
endif::[]

===== 优缺点
- 优点
* 自适应学习率
    每个参数的学习率根据其梯度历史动态调整，适合处理稀疏梯度问题。
* 缓解梯度震荡
    通过对梯度平方的移动平均，RMSprop 能够减少梯度变化较大的方向的震荡。
* 高效性
    相比传统的梯度下降法，RMSprop 通常能够更快地收敛。

- 缺点
* 对超参数敏感
    衰减率 ρ 和学习率 η 的选择 对算法性能有较大影响。
* 可能陷入局部最优
    在某些情况下，RMSprop 可能会陷入局部最优解。

===== 与其它优化算法的关系
- 与AdaGrad
RMSprop 可以看作是 AdaGrad 的改进版本。
AdaGrad 会累积所有历史梯度的平方，导致学习率过早衰减；而 RMSprop 使用指数移动平均，避免了这一问题。

- 与Adam
Adam 可以看作是 RMSprop 的扩展版本，结合了 Momentum 和 RMSprop 的思想。

===== 参考
https://en.d2l.ai/chapter_optimization/rmsprop.html

==== Adam
===== 概念
Adam(Adaptive Moment Estimation)是一种广泛使用的自适应学习率优化算法，结合了 Momentum 和 RMSprop 的思想。它通过计算梯度的一阶矩(均值)和二阶矩(未中心化的方差)来动态调整每个参数的学习率，从而在训练深度学习模型时表现出色。Adam 因其高效性和鲁棒性，成为深度学习中最流行的优化算法之一。

===== 核心思想
Adam 的核心思想是结合了动量和自适应学习率的优点:

动量(Momentum):
通过计算梯度的一阶矩(均值)，Adam 引入了动量机制，加速收敛并减少震荡。

自适应学习率(RMSprop):
通过计算梯度的二阶矩(未中心化的方差)，Adam 对每个参数的学习率进行自适应调整。

===== 算法步骤

```math
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
```

```math
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
```

```math
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
```

```math
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
```

```math
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
```

===== 优缺点
- 优点
* 自适应学习率
    每个参数的学习率根据其梯度历史动态调整，适合处理稀疏梯度问题。
* 动量机制
    通过动量加速收敛，减少震荡。
* 高效性
    在许多任务中，Adam 能够快速收敛，且对超参数的选择相对鲁棒。
* 偏差修正
    通过偏差修正，避免了初始阶段估计不准确的问题。

- 缺点
* 可能陷入局部最优
    在某些情况下，Adam 可能会收敛到次优解。
* 超参数选择
    虽然 Adam 对超参数的选择相对鲁棒，但 β1 和 β2 的设置仍会影响性能。
* 内存占用
    需要存储一阶矩和二阶矩，内存占用略高于 SGD。

===== 参考
https://en.d2l.ai/chapter_optimization/adam.html

==== Learning Rate Scheduling(学习率调度器)
===== 概念
Learning Rate Scheduler在训练过程中动态地调整学习率，从而提高模型的训练效率和最终的模型性能。通常，学习率在训练初期较大，以加速收敛，然后逐步减小，避免在接近最优解时震荡，确保更精确的收敛。

===== 常见的学习率调度器
- 固定学习率(Constant Learning Rate)

    在整个训练过程中，学习率保持不变。
    简单但可能无法适应不同训练阶段的需求。

- Step Decay(步进衰减)

    每隔固定步数或轮数，学习率按一定比例衰减。
    例如，每过 30 轮，学习率乘以 0.1。

- Exponential Decay(指数衰减)

    学习率按指数函数衰减。
    公式:lr = initial_lr * exp(-kt)，其中 k 为衰减率，t 为训练步数。

- Cosine Annealing(余弦退火)

    学习率按余弦函数周期性变化。
    公式:lr = initial_lr * 0.5 * (1 + cos(t * π / T))，T 为周期。

- Reduce on Plateau(基于验证集性能的调整)

    当验证集性能停滞时，学习率按比例衰减。
    例如，若验证损失在若干轮内未下降，学习率减半。

- Warmup(预热)

    训练初期逐步增加学习率，避免初始阶段的不稳定。
    通常与其它调度器结合使用。

- Cyclic Learning Rate(循环学习率)

    学习率在预设范围内周期性变化。
    有助于跳出局部最优。

- One-Cycle Learning Rate(单周期学习率)

    学习率先增后减，形成一个周期。
    通常与动量调整结合，提升训练效果。

===== 参考
https://en.d2l.ai/chapter_optimization/lr-scheduler.html

=== 计算性能
==== Compilers and Interpreters
==== Asynchronous Computation
==== Automatic Parallelism
==== Hardware
==== Training on Multiple GPUs
==== Concise Implementation for Multiple GPUs
==== Parameter Servers

=== 计算机视觉
==== Image Augmentation(图像增广)
==== Fine-Tuning(微调)
==== Object Detection and Bounding Boxes
==== Anchor Boxes

==== Multiscale Object Detection(多尺度目标检测)

==== The Object Detection Dataset

==== Single Shot Multibox Detection(单发多框检测)

==== Region-based CNNs (R-CNNs)

==== Semantic Segmentation and the Dataset

==== Transposed Convolution(转置卷积)

==== Fully Convolutional Networks

==== Neural Style Transfer

==== Image Classification (CIFAR-10) on Kaggle

==== Dog Breed Identification (ImageNet Dogs) on Kaggle

=== 自然语言处理(Natural Language Processing)
==== Pretraining(预训练)

==== Applications(应用)

=== Recommender Systems
https://en.d2l.ai/chapter_recommender-systems/index.html

==== Overview of Recommender Systems
==== The MovieLens Dataset
==== Matrix Factorization
==== AutoRec: Rating Prediction with Autoencoders
==== Personalized Ranking for Recommender Systems
==== Neural Collaborative Filtering for Personalized Ranking
==== Sequence-Aware Recommender Systems
==== Feature-Rich Recommender Systems
==== Factorization Machines
==== Deep Factorization Machines

=== Gaussian Processes
==== 介绍
https://en.wikipedia.org/wiki/Gaussian_process
https://en.d2l.ai/chapter_gaussian-processes/gp-intro.html

==== Gaussian Process Priors
https://en.d2l.ai/chapter_gaussian-processes/gp-priors.html

==== Gaussian Process Inference
https://en.d2l.ai/chapter_gaussian-processes/gp-inference.html

=== Hyperparameter Optimization
==== What Is Hyperparameter Optimization?
==== Hyperparameter Optimization API
==== Asynchronous Random Search
==== Multi-Fidelity Hyperparameter Optimization
==== Asynchronous Successive Halving

=== Generative Adversarial Networks(生成对抗网络)

==== Generative Adversarial Networks
===== 概念
2014
Generative Adversarial Networks (GANs) are a class of machine learning models introduced by Ian Goodfellow and his colleagues in 2014. GANs are used for generating synthetic data that resembles a given real dataset. They consist of two neural networks: a Generator and a Discriminator. These two networks are trained together in a competitive process.
GANs的核心思想是通过两个神经网络的对抗训练来生成高质量的数据。这两个网络分别是生成器(Generator)和判别器(Discriminator)。

===== 基本结构
- 生成器(Generator):
生成器的目标是生成与真实数据相似的假数据。它接收一个随机噪声向量作为输入，并输出生成的数据(如图像、文本等)。

- 判别器(Discriminator):
判别器的目标是区分输入数据是来自真实数据集还是生成器生成的假数据。它接收真实数据或生成数据作为输入，并输出一个概率值，表示输入数据是真实数据的可能性。

===== 训练过程
TODO:

GANs的训练过程是一个极小极大博弈(minimax game)，生成器和判别器在训练过程中相互对抗: 
生成器的目标: 生成尽可能逼真的数据，使得判别器无法区分生成数据和真实数据。
判别器的目标: 尽可能准确地区分真实数据和生成数据。

===== 训练步骤
训练判别器: 固定生成器，使用真实数据和生成数据训练判别器，使其能够更好地区分真实数据和生成数据。
训练生成器: 固定判别器，训练生成器使其生成的数据能够欺骗判别器。
交替训练: 重复上述两个步骤，直到生成器生成的数据足够逼真，判别器无法区分。

===== 应用
GANs在多个领域有广泛应用，包括但不限于:
图像生成: 生成逼真的图像，如人脸、风景等。
图像修复: 修复损坏或缺失部分的图像。
风格迁移: 将一种图像的风格迁移到另一种图像上。
数据增强: 生成额外的训练数据以提高模型的泛化能力。
文本生成: 生成自然语言文本。

===== 挑战
尽管GANs在许多任务中表现出色，但其训练过程仍然面临一些挑战:
训练不稳定: 生成器和判别器的训练需要精细的平衡，否则容易导致训练失败。
模式崩溃: 生成器可能只生成有限的几种样本，而无法覆盖整个数据分布。
评估困难: 如何客观评估生成数据的质量仍然是一个开放问题。

===== 变体
随着GANs的发展，出现了许多变体，以解决原始GANs训练不稳定、模式崩溃等问题。一些常见的变体包括: 
DCGAN(Deep Convolutional GAN): 使用卷积神经网络改进生成器和判别器。
WGAN(Wasserstein GAN): 使用Wasserstein距离作为损失函数，提高训练稳定性。
CycleGAN: 用于图像到图像的转换，无需成对数据。
StyleGAN: 生成高分辨率、高质量的图像，并允许对生成图像的风格进行精细控制。

===== Generate Some "Real" Data
===== Generator
===== Discriminator
===== Training

===== 参考
https://en.d2l.ai/chapter_generative-adversarial-networks/gan.html#generate-some-real-data

==== Deep Convolutional Generative Adversarial Networks
===== The Pokemon Dataset
===== The Generator
===== Discriminator
===== Training

== 强化学习
=== 概念
在强化学习问题中，智能体(agent)在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察(observation)，并且必须选择一个动作(action)，然后通过某种机制(有时称为执行器)将其传输回环境，最后智能体从环境中获得奖励(reward)。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程(markov decision process)。当状态不依赖于之前的操作时，称该问题为上下文赌博机(contextual bandit problem)。当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机(multi-armed bandit problem)。

=== vs. 深度学习
深度学习通过多层神经网络学习数据的特征表示。
强化学习通过与环境交互，基于奖励机制学习最优策略。
深度学习适用于数据驱动和需要感知的任务，如图像、语音、文本处理。
强化学习适用于需要决策和控制的场景，如游戏、机器人、资源管理。

=== 发展历史
- 1970年代-1980年代
强化学习的基础 Richard Sutton(强化学习奠基人)和Christopher Watkins等人提出了自适应动态规划(Adaptive Dynamic Programming)和Q-learning等基本方法。

- 1990年代-2000年代
Christopher Watkins提出了基于差分学习的Q-learning算法，是现代强化学习的重要里程碑。
Ronald J. Williams引入了基于梯度的强化学习算法，即策略梯度方法，为后来的进一步发展奠定了基础。

- 2013年以来 深度强化学习时代
通过深度神经网络的引入，强化学习得到了重大的推动和突破。
2013年 Deep Q-Network(DQN)是由DeepMind提出的一种结合深度神经网络和Q-learning的算法，首次实现了在Atari游戏中超越人类水平的表现。
2015年: AlphaGo

- 挑战
如样本效率、探索与利用的平衡、通用性和可解释性等问题。

=== 分类
强化学习的算法目前一般分为两类(是否有对环境建模):

    Model-free RL
    Model-based RL

=== Markov Decision Process(MDP)
==== 概念
马尔可夫决策过程(Markov Decision Process, MDP)是用于建模序列决策问题的数学框架，广泛应用于强化学习、运筹学等领域。

==== 基本组成

    状态(State):
        系统可能的所有状态集合，记为 S。
    动作(Action):
        在每个状态下可执行的动作集合，记为 A。
    转移概率(Transition Probability):
        在状态 s 执行动作 a 后转移到状态 s′ 的概率，记为 P(s′∣s,a)。
    奖励函数(Reward Function):
        在状态 s 执行动作 a 后转移到状态 s′ 时获得的即时奖励，记为 R(s,a,s′)。
    折扣因子(Discount Factor):
        用于平衡当前与未来奖励的重要性，记为 γ(0≤γ≤1)

==== 马尔可夫性质
具有马尔可夫性质，即未来状态只依赖于当前状态和动作，与过去状态无关:
```math
P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1} | s_t, a_t)
```

==== 参考
https://en.d2l.ai/chapter_reinforcement-learning/mdp.html

=== Value Iteration
==== 概念
值迭代(Value Iteration)是强化学习中的一种经典算法，用于求解马尔可夫决策过程(MDP)的最优策略。它通过迭代更新值函数来逼近最优值函数，并最终导出最优策略。

==== 参考
https://en.d2l.ai/chapter_reinforcement-learning/value-iter.html

=== Q-Learning
==== 概念
Q-learning 是一种无模型的强化学习算法，用于在给定的马尔可夫决策过程(MDP)中找到最优的动作选择策略。它是一种离策略(off-policy)算法，意味着它可以独立于智能体的行为来学习最优策略。

- 优点

    无模型:
        不需要环境模型。
    保证收敛:
        在一定条件下，Q-learning 保证收敛到最优策略。

- 局限性

    可扩展性:
        对于具有许多状态和动作的问题，Q 表可能变得非常大。
    收敛速度慢:
        学习可能很慢，特别是在大型或复杂的环境中。

- 扩展与变体

    深度 Q 学习(DQN):
        将 Q-learning 与深度神经网络结合，以处理高维状态空间。
    双 Q 学习:
        通过使用两个 Q 函数减少 Q 值的高估。
    SARSA:
        Q-learning 的一种在策略(on-policy)变体，基于实际采取的动作更新 Q 值。

==== 核心元素
- 马尔可夫决策过程(MDP)

    状态(S): 表示智能体可能处于的不同情况。
    动作(A): 智能体可以采取的可能行动。
    奖励(R): 采取动作后从环境获得的即时反馈。
    转移概率(P): 在给定动作下从一个状态转移到另一个状态的概率。

- Q值

    Q(s,a) 表示在状态 s 下采取动作 a 并随后遵循最优策略的预期累积奖励。

- 贝尔曼方程

==== 算法步骤
TODO:

==== 参考
https://en.d2l.ai/chapter_reinforcement-learning/qlearning.html
https://arxiv.org/pdf/1509.06461

== 工具
- Jupyter Notebook
Jupyter vs. IDE

- colab
https://colab.research.google.com/
https://www.geeksforgeeks.org/google-collab-vs-jupyter-notebook/

- GPU
https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html

- kaggle
https://www.kaggle.com/

- 参考
https://en.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html

== 框架/库
=== 历史与发展
- caff2
2018年3月底，Caffe2并入了Pytorch。

- CNTK
2019年4月26日发布2.7.0稳定版后，微软对CNTK的更新逐渐减少，并开始将更多的精力投入到与PyTorch等框架的合作和整合上，CNTK也逐渐被一些新的技术和框架所替代。

- chainer
2019年12月，Preferred Networks宣布将其深度学习研究平台的开发工作从Chainer转移到PyTorch。

- mxnet
2022年底，MXNet的代码开发大部分停止，社区参与度放缓。2022年9月MXNet从Apache孵化器毕业成为顶级项目，但在同年11月却被移入Apache Attic，进入"只读"阶段，意味着该项目不再重建社区、修正Bug、发布新版本，正式宣告退休。

=== pytorch
==== 分布式
===== 主要组件
- `torch.distributed` 包
提供分布式训练的核心功能，包括进程组管理、通信原语等。

- `torch.nn.parallel.DistributedDataParallel` (DDP)
用于多机多卡训练的模块，支持数据并行和梯度同步。

- `torch.utils.data.distributed.DistributedSampler`
用于在分布式训练中分配数据集的子集给每个进程。

- `torch.distributed.launch` 脚本
用于启动分布式训练的辅助工具。

===== 通信后端
PyTorch 支持多种通信后端，适用于不同的硬件和网络环境:

* *NCCL*
  - 适用于多 GPU 环境，支持高效的 GPU 间通信。
* *GLOO*
  - 适用于 CPU 和多机环境，支持 TCP 和 RDMA。
* *MPI*
  - 适用于高性能计算(HPC)环境，需要安装 MPI 实现(如 OpenMPI)。

===== 通信模式

- 点对点通信
支持两个进程之间的直接通信。
常用函数：
* `send()`：发送数据。
* `recv()`：接收数据。

- 集体通信
支持多个进程之间的协同通信。
常用函数：
* `broadcast()`：将数据从一个进程广播到所有进程。
* `scatter()`：将数据从一个进程分发到多个进程。
* `gather()`：将数据从多个进程收集到一个进程。
* `all_gather()`：将数据从所有进程收集到所有进程。
* `reduce()`：将所有进程的数据聚合到一个进程。
* `all_reduce()`：将所有进程的数据聚合到所有进程。

==== 参考
https://github.com/pytorch
https://pytorch.org/docs/

=== tensorflow
https://github.com/tensorflow/tensorflow
https://www.tensorflow.org/api_docs/

=== keras
https://github.com/keras-team/keras
https://keras.io/api/

=== 参考
https://www.zhihu.com/question/46587833

== project
=== 概览
https://github.com/vietnh1009/Super-mario-bros-PPO-pytorch

https://github.com/karpathy/nanoGPT
https://github.com/karpathy/llm.c
https://github.com/rasbt/LLMs-from-scratch

https://github.com/meta-llama
https://github.com/deepseek-ai
https://github.com/openai
https://github.com/QwenLM

https://github.com/NVIDIA/Megatron-LM
https://github.com/deepspeedai/DeepSpeed

=== deepseek
==== 概览
https://zhuanlan.zhihu.com/p/23048347789

==== paper
https://arxiv.org/pdf/2401.02954
https://arxiv.org/pdf/2401.06066
https://arxiv.org/pdf/2402.03300
https://arxiv.org/pdf/2405.04434
https://arxiv.org/pdf/2412.19437

==== source
https://github.com/deepseek-ai

=== OpenAI
==== paper
https://arxiv.org/pdf/1912.06680

==== source
https://github.com/openai

=== game
https://cloud.tencent.com/developer/article/1788317

==== mario
https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html

==== dota2
https://github.com/bilibili/LastOrder-Dota2

== 参考
- https://en.d2l.ai/
- https://zh.d2l.ai/

-《深度学习入门》《深度学习进阶》[日]斋藤康毅

- 李沐
https://space.bilibili.com/1567748478

- 李宏毅
https://space.bilibili.com/3546385131505733
https://www.bilibili.com/video/BV1Wv411h7kN/
https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ/playlists

- 吴恩达
https://www.bilibili.com/video/BV1Bq421A74G/

- https://ai.stanford.edu/courses/
- https://github.com/mahseema/awesome-ai-tools
- https://www.zhihu.com/question/56952345/answer/632233718
- https://github.com/ahkarami/Great-Deep-Learning-Books

- 《Deep Learning》https://www.deeplearningbook.org/
- 《Grokking Deep Learning》Andrew W. Trask
- 《Deep Learning with Python》Francois Chollet
- 《Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow》3rd Edition
- 《Reinforcement Learning: An Introduction》2nd Edition
- 《Deep Reinforcement Learning Hands-On》

- LLM
Lecture 1: Building LLMs from scratch: Series introduction https://youtu.be/Xpr8D6LeAtw?si=vPCmTzfUY4oMCuVl 
Lecture 2: Large Language Models (LLM) Basics https://youtu.be/3dWzNZXA8DY?si=FdsoxgSRn9PmXTTz 
Lecture 3: Pretraining LLMs vs Finetuning LLMs https://youtu.be/-bsa3fCNGg4?si=j49O1OX2MT2k68pl 
Lecture 4: What are transformers? https://youtu.be/NLn4eetGmf8?si=GVBrKVjGa5Y7ivVY 
Lecture 5: How does GPT-3 really work? https://youtu.be/xbaYCf2FHSY?si=owbZqQTJQYm5VzDx 
Lecture 6: Stages of building an LLM from Scratch https://youtu.be/z9fgKz1Drlc?si=dzAqz-iLKaxUH-lZ 
Lecture 7: Code an LLM Tokenizer from Scratch in Python https://youtu.be/rsy5Ragmso8?si=MJr-miJKm7AHwhu9 
Lecture 8: The GPT Tokenizer: Byte Pair Encoding https://youtu.be/fKd8s29e-l4?si=aZzzV4qT_nbQ1lzk 
Lecture 9: Creating Input-Target data pairs using Python DataLoader https://youtu.be/iQZFH8dr2yI?si=lH6sdboTXzOzZXP9 
Lecture 10: What are token embeddings? https://youtu.be/ghCSGRgVB_o?si=PM2FLDl91ENNPJbd 
Lecture 11: The importance of Positional Embeddings https://youtu.be/ufrPLpKnapU?si=cstZgif13kyYo0Rc 
Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs) https://youtu.be/mk-6cFebjis?si=G4Wqn64OszI9ID0b 
Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs) https://youtu.be/XN7sevVxyUM?si=aJy7Nplz69jAzDnC 
Lecture 14: Simplified Attention Mechanism - Coded from scratch in Python | No trainable weights https://youtu.be/eSRhpYLerw4?si=1eiOOXa3V5LY-H8c 
Lecture 15: Coding the self attention mechanism with key, query and value matrices https://youtu.be/UjdRN80c6p8?si=LlJkFvrC4i3J0ERj 
Lecture 16: Causal Self Attention Mechanism | Coded from scratch in Python https://youtu.be/h94TQOK7NRA?si=14DzdgSx9XkAJ9Pp 
Lecture 17: Multi Head Attention Part 1 - Basics and Python code https://youtu.be/cPaBCoNdCtE?si=eF3GW7lTqGPdsS6y 
Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained https://youtu.be/K5u9eEaoxFg?si=JkUATWM9Ah4IBRy2 
Lecture 19: Birds Eye View of the LLM Architecture https://youtu.be/4i23dYoXp-A?si=GjoIoJWlMloLDedg 
Lecture 20: Layer Normalization in the LLM Architecture https://youtu.be/G3W-LT79LSI?si=ezsIvNcW4dTVa29i 
Lecture 21: GELU Activation Function in the LLM Architecture https://youtu.be/d_PiwZe8UF4?si=IOMD06wo1MzElY9J 
Lecture 22: Shortcut connections in the LLM Architecture https://youtu.be/2r0QahNdwMw?si=i4KX0nmBTDiPmNcJ 
Lecture 23: Coding the entire LLM Transformer Block https://youtu.be/dvH6lFGhFrs?si=e90uX0TfyVRasvel 
Lecture 24: Coding the 124 million parameter GPT-2 model https://youtu.be/G3-JgHckzjw?si=peLE6thVj6bds4M0 
Lecture 25: Coding GPT-2 to predict the next token https://youtu.be/F1Sm7z2R96w?si=TAN33aOXAeXJm5Ro 
Lecture 26: Measuring the LLM loss function https://youtu.be/7TKCrt--bWI?si=rvjeapyoD6c-SQm3 
Lecture 27: Evaluating LLM performance on real dataset | Hands on project | Book data https://youtu.be/zuj_NJNouAA?si=Y_vuf-KzY3Dt1d1r 
Lecture 28: Coding the entire LLM Pre-training Loop https://youtu.be/Zxf-34voZss?si=AxYVGwQwBubZ3-Y9 
Lecture 29: Temperature Scaling in Large Language Models (LLMs) https://youtu.be/oG1FPVnY0pI?si=S4N0wSoy4KYV5hbv 
Lecture 30: Top-k sampling in Large Language Models https://youtu.be/EhU32O7DkA4?si=GKHqUCPqG-XvCMFG 