= ai-applications
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath

== 概念
- 大模型
通常指的是在机器学习和深度学习领域中，具有大量参数和复杂结构的模型。这些模型通常需要大量的数据和计算资源进行训练，能够处理复杂的任务，如自然语言处理、图像识别、语音识别视频处理等。

- 大语言模型(Large Language Model, 即LLM)
通过在海量文本上训练拥有巨量参数的神经网络(通常是 Transformer)，获得了令人惊叹的语言理解和生成能力。如GPT，BERT。

- 大模型蒸馏(Model Distillation)
大模型蒸馏是一种模型压缩方法，旨在将大型、复杂的深度学习模型(通常称为教师模型)的知识转移到一个较小的、效率更高的模型(称为学生模型)中。通过这种方式，学生模型可以在保持较高性能的同时，减少计算资源的需求和推理时间。

- 人工智能奇点(AI Singularity)
AI发展到某个临界点后，其智能水平超越人类，并能够自我改进、自我升级，导致技术发展呈指数级加速，最终超出人类预测和控制能力的理论概念。

== 计算机视觉(Computer Vision)
=== Image Augmentation(图像增广)
=== Fine-Tuning(微调)
=== Object Detection and Bounding Boxes
=== Anchor Boxes
=== Multiscale Object Detection(多尺度目标检测)
=== The Object Detection Dataset
=== Single Shot Multibox Detection(单发多框检测)
=== Region-based CNNs (R-CNNs)
=== Semantic Segmentation and the Dataset
=== Transposed Convolution(转置卷积)
=== Fully Convolutional Networks
=== Neural Style Transfer
=== Image Classification (CIFAR-10) on Kaggle
=== Dog Breed Identification (ImageNet Dogs) on Kaggle
=== 参考
https://en.d2l.ai/chapter_computer-vision/index.html

== 自然语言处理(Natural Language Processing)
=== Pretraining
==== 参考
https://en.d2l.ai/chapter_natural-language-processing-pretraining/index.html

=== Applications
==== 参考
https://en.d2l.ai/chapter_natural-language-processing-applications/index.html

== Recommender Systems(推荐系统)
=== Overview of Recommender Systems
=== The MovieLens Dataset
=== Matrix Factorization
=== AutoRec: Rating Prediction with Autoencoders
=== Personalized Ranking for Recommender Systems
=== Neural Collaborative Filtering for Personalized Ranking
=== Sequence-Aware Recommender Systems
=== Feature-Rich Recommender Systems
=== Factorization Machines
=== Deep Factorization Machines
=== 参考
https://en.d2l.ai/chapter_recommender-systems/index.html
《互联网大厂推荐算法实战》

== Advertising System(广告系统)
=== 参考
https://tech.meituan.com/2022/07/06/largescaledeeplearningmodel-engineeringpractice-in-mtwaimaiad.html

== Search System(搜索系统)

== LLM

=== 参考
- LLM
Lecture 1: Building LLMs from scratch: Series introduction https://youtu.be/Xpr8D6LeAtw?si=vPCmTzfUY4oMCuVl 
Lecture 2: Large Language Models (LLM) Basics https://youtu.be/3dWzNZXA8DY?si=FdsoxgSRn9PmXTTz 
Lecture 3: Pretraining LLMs vs Finetuning LLMs https://youtu.be/-bsa3fCNGg4?si=j49O1OX2MT2k68pl 
Lecture 4: What are transformers? https://youtu.be/NLn4eetGmf8?si=GVBrKVjGa5Y7ivVY 
Lecture 5: How does GPT-3 really work? https://youtu.be/xbaYCf2FHSY?si=owbZqQTJQYm5VzDx 
Lecture 6: Stages of building an LLM from Scratch https://youtu.be/z9fgKz1Drlc?si=dzAqz-iLKaxUH-lZ 
Lecture 7: Code an LLM Tokenizer from Scratch in Python https://youtu.be/rsy5Ragmso8?si=MJr-miJKm7AHwhu9 
Lecture 8: The GPT Tokenizer: Byte Pair Encoding https://youtu.be/fKd8s29e-l4?si=aZzzV4qT_nbQ1lzk 
Lecture 9: Creating Input-Target data pairs using Python DataLoader https://youtu.be/iQZFH8dr2yI?si=lH6sdboTXzOzZXP9 
Lecture 10: What are token embeddings? https://youtu.be/ghCSGRgVB_o?si=PM2FLDl91ENNPJbd 
Lecture 11: The importance of Positional Embeddings https://youtu.be/ufrPLpKnapU?si=cstZgif13kyYo0Rc 
Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs) https://youtu.be/mk-6cFebjis?si=G4Wqn64OszI9ID0b 
Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs) https://youtu.be/XN7sevVxyUM?si=aJy7Nplz69jAzDnC 
Lecture 14: Simplified Attention Mechanism - Coded from scratch in Python | No trainable weights https://youtu.be/eSRhpYLerw4?si=1eiOOXa3V5LY-H8c 
Lecture 15: Coding the self attention mechanism with key, query and value matrices https://youtu.be/UjdRN80c6p8?si=LlJkFvrC4i3J0ERj 
Lecture 16: Causal Self Attention Mechanism | Coded from scratch in Python https://youtu.be/h94TQOK7NRA?si=14DzdgSx9XkAJ9Pp 
Lecture 17: Multi Head Attention Part 1 - Basics and Python code https://youtu.be/cPaBCoNdCtE?si=eF3GW7lTqGPdsS6y 
Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained https://youtu.be/K5u9eEaoxFg?si=JkUATWM9Ah4IBRy2 
Lecture 19: Birds Eye View of the LLM Architecture https://youtu.be/4i23dYoXp-A?si=GjoIoJWlMloLDedg 
Lecture 20: Layer Normalization in the LLM Architecture https://youtu.be/G3W-LT79LSI?si=ezsIvNcW4dTVa29i 
Lecture 21: GELU Activation Function in the LLM Architecture https://youtu.be/d_PiwZe8UF4?si=IOMD06wo1MzElY9J 
Lecture 22: Shortcut connections in the LLM Architecture https://youtu.be/2r0QahNdwMw?si=i4KX0nmBTDiPmNcJ 
Lecture 23: Coding the entire LLM Transformer Block https://youtu.be/dvH6lFGhFrs?si=e90uX0TfyVRasvel 
Lecture 24: Coding the 124 million parameter GPT-2 model https://youtu.be/G3-JgHckzjw?si=peLE6thVj6bds4M0 
Lecture 25: Coding GPT-2 to predict the next token https://youtu.be/F1Sm7z2R96w?si=TAN33aOXAeXJm5Ro 
Lecture 26: Measuring the LLM loss function https://youtu.be/7TKCrt--bWI?si=rvjeapyoD6c-SQm3 
Lecture 27: Evaluating LLM performance on real dataset | Hands on project | Book data https://youtu.be/zuj_NJNouAA?si=Y_vuf-KzY3Dt1d1r 
Lecture 28: Coding the entire LLM Pre-training Loop https://youtu.be/Zxf-34voZss?si=AxYVGwQwBubZ3-Y9 
Lecture 29: Temperature Scaling in Large Language Models (LLMs) https://youtu.be/oG1FPVnY0pI?si=S4N0wSoy4KYV5hbv 
Lecture 30: Top-k sampling in Large Language Models https://youtu.be/EhU32O7DkA4?si=GKHqUCPqG-XvCMFG 

== AIGC与游戏
=== 概览
https://tisi.org/27780/

=== unity
https://github.com/Unity-Technologies/ml-agents
