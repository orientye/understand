= ai-applications-and-practices
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath

== 计算机视觉
=== Image Augmentation(图像增广)
=== Fine-Tuning(微调)
=== Object Detection and Bounding Boxes
=== Anchor Boxes

=== Multiscale Object Detection(多尺度目标检测)

=== The Object Detection Dataset

=== Single Shot Multibox Detection(单发多框检测)

=== Region-based CNNs (R-CNNs)

=== Semantic Segmentation and the Dataset

=== Transposed Convolution(转置卷积)

=== Fully Convolutional Networks

=== Neural Style Transfer

=== Image Classification (CIFAR-10) on Kaggle

=== Dog Breed Identification (ImageNet Dogs) on Kaggle

== 自然语言处理(Natural Language Processing)
=== Pretraining(预训练)

=== Applications(应用)

== Recommender Systems
https://en.d2l.ai/chapter_recommender-systems/index.html

=== Overview of Recommender Systems
=== The MovieLens Dataset
=== Matrix Factorization
=== AutoRec: Rating Prediction with Autoencoders
=== Personalized Ranking for Recommender Systems
=== Neural Collaborative Filtering for Personalized Ranking
=== Sequence-Aware Recommender Systems
=== Feature-Rich Recommender Systems
=== Factorization Machines
=== Deep Factorization Machines
=== 参考
《互联网大厂推荐算法实战》

== 广告系统
=== 参考
https://tech.meituan.com/2022/07/06/largescaledeeplearningmodel-engineeringpractice-in-mtwaimaiad.html

== 搜索系统

== LLM

=== 参考
- LLM
Lecture 1: Building LLMs from scratch: Series introduction https://youtu.be/Xpr8D6LeAtw?si=vPCmTzfUY4oMCuVl 
Lecture 2: Large Language Models (LLM) Basics https://youtu.be/3dWzNZXA8DY?si=FdsoxgSRn9PmXTTz 
Lecture 3: Pretraining LLMs vs Finetuning LLMs https://youtu.be/-bsa3fCNGg4?si=j49O1OX2MT2k68pl 
Lecture 4: What are transformers? https://youtu.be/NLn4eetGmf8?si=GVBrKVjGa5Y7ivVY 
Lecture 5: How does GPT-3 really work? https://youtu.be/xbaYCf2FHSY?si=owbZqQTJQYm5VzDx 
Lecture 6: Stages of building an LLM from Scratch https://youtu.be/z9fgKz1Drlc?si=dzAqz-iLKaxUH-lZ 
Lecture 7: Code an LLM Tokenizer from Scratch in Python https://youtu.be/rsy5Ragmso8?si=MJr-miJKm7AHwhu9 
Lecture 8: The GPT Tokenizer: Byte Pair Encoding https://youtu.be/fKd8s29e-l4?si=aZzzV4qT_nbQ1lzk 
Lecture 9: Creating Input-Target data pairs using Python DataLoader https://youtu.be/iQZFH8dr2yI?si=lH6sdboTXzOzZXP9 
Lecture 10: What are token embeddings? https://youtu.be/ghCSGRgVB_o?si=PM2FLDl91ENNPJbd 
Lecture 11: The importance of Positional Embeddings https://youtu.be/ufrPLpKnapU?si=cstZgif13kyYo0Rc 
Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs) https://youtu.be/mk-6cFebjis?si=G4Wqn64OszI9ID0b 
Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs) https://youtu.be/XN7sevVxyUM?si=aJy7Nplz69jAzDnC 
Lecture 14: Simplified Attention Mechanism - Coded from scratch in Python | No trainable weights https://youtu.be/eSRhpYLerw4?si=1eiOOXa3V5LY-H8c 
Lecture 15: Coding the self attention mechanism with key, query and value matrices https://youtu.be/UjdRN80c6p8?si=LlJkFvrC4i3J0ERj 
Lecture 16: Causal Self Attention Mechanism | Coded from scratch in Python https://youtu.be/h94TQOK7NRA?si=14DzdgSx9XkAJ9Pp 
Lecture 17: Multi Head Attention Part 1 - Basics and Python code https://youtu.be/cPaBCoNdCtE?si=eF3GW7lTqGPdsS6y 
Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained https://youtu.be/K5u9eEaoxFg?si=JkUATWM9Ah4IBRy2 
Lecture 19: Birds Eye View of the LLM Architecture https://youtu.be/4i23dYoXp-A?si=GjoIoJWlMloLDedg 
Lecture 20: Layer Normalization in the LLM Architecture https://youtu.be/G3W-LT79LSI?si=ezsIvNcW4dTVa29i 
Lecture 21: GELU Activation Function in the LLM Architecture https://youtu.be/d_PiwZe8UF4?si=IOMD06wo1MzElY9J 
Lecture 22: Shortcut connections in the LLM Architecture https://youtu.be/2r0QahNdwMw?si=i4KX0nmBTDiPmNcJ 
Lecture 23: Coding the entire LLM Transformer Block https://youtu.be/dvH6lFGhFrs?si=e90uX0TfyVRasvel 
Lecture 24: Coding the 124 million parameter GPT-2 model https://youtu.be/G3-JgHckzjw?si=peLE6thVj6bds4M0 
Lecture 25: Coding GPT-2 to predict the next token https://youtu.be/F1Sm7z2R96w?si=TAN33aOXAeXJm5Ro 
Lecture 26: Measuring the LLM loss function https://youtu.be/7TKCrt--bWI?si=rvjeapyoD6c-SQm3 
Lecture 27: Evaluating LLM performance on real dataset | Hands on project | Book data https://youtu.be/zuj_NJNouAA?si=Y_vuf-KzY3Dt1d1r 
Lecture 28: Coding the entire LLM Pre-training Loop https://youtu.be/Zxf-34voZss?si=AxYVGwQwBubZ3-Y9 
Lecture 29: Temperature Scaling in Large Language Models (LLMs) https://youtu.be/oG1FPVnY0pI?si=S4N0wSoy4KYV5hbv 
Lecture 30: Top-k sampling in Large Language Models https://youtu.be/EhU32O7DkA4?si=GKHqUCPqG-XvCMFG 

== 游戏AIGC
