= reinforcement-learning
:hardbreaks-option:
:revnumber: 0.0.1
:author: orient
:toc:
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:stem: latexmath

== 概念
在强化学习问题中，智能体(agent)在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察(observation)，并且必须选择一个动作(action)，然后通过某种机制(有时称为执行器)将其传输回环境，最后智能体从环境中获得奖励(reward)。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程(markov decision process)。
当状态不依赖于之前的操作时，称该问题为上下文赌博机(contextual bandit problem)。
当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机(multi-armed bandit problem)。

Q: 对于路径规划问题，A*算法与强化学习有哪些区别？

- vs. 深度学习

    深度学习通过多层神经网络学习数据的特征表示。
    强化学习通过与环境交互，基于奖励机制学习最优策略。
    
    深度学习适用于数据驱动和需要感知的任务，如图像、语音、文本处理。
    强化学习适用于需要决策和控制的场景，如游戏、机器人、资源管理。

- 发展历史
** 1970年代-1980年代
    强化学习的基础 Richard Sutton(强化学习奠基人)和Christopher Watkins等人提出了自适应动态规划(Adaptive Dynamic Programming)和Q-learning等基本方法。
** 1990年代-2000年代
    Christopher Watkins提出了基于差分学习的Q-learning算法，是现代强化学习的重要里程碑。
    Ronald J. Williams引入了基于梯度的强化学习算法，即策略梯度方法，为后来的进一步发展奠定了基础。
** 2013年以来 深度强化学习时代
    通过深度神经网络的引入，强化学习得到了重大的推动和突破。
    2013年 Deep Q-Network(DQN)是由DeepMind提出的一种结合深度神经网络和Q-learning的算法，首次实现了在Atari游戏中超越人类水平的表现。
    2015年: AlphaGo

- 挑战
如样本效率、探索与利用的平衡、通用性和可解释性等问题。

- 分类
强化学习的算法目前一般分为两类(是否有对环境建模):

    Model-free RL
    Model-based RL

== Markov Decision Process(MDP)
=== 概念
马尔可夫决策过程(Markov Decision Process, MDP)是用于建模序列决策问题的数学框架，广泛应用于强化学习、运筹学等领域。

=== 基本组成

    状态(State):
        系统可能的所有状态集合，记为 S。
    动作(Action):
        在每个状态下可执行的动作集合，记为 A。
    转移概率(Transition Probability):
        在状态 s 执行动作 a 后转移到状态 s′ 的概率，记为 P(s′∣s,a)。
    奖励函数(Reward Function):
        在状态 s 执行动作 a 后转移到状态 s′ 时获得的即时奖励，记为 R(s,a,s′)。
    折扣因子(Discount Factor):
        用于平衡当前与未来奖励的重要性，记为 γ(0≤γ≤1)

=== 马尔可夫性质
具有马尔可夫性质，即未来状态只依赖于当前状态和动作，与过去状态无关:
ifndef::env-github[]
stem:[P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1} | s_t, a_t)]
endif::[]
ifdef::env-github[]
```math
P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1} | s_t, a_t)
```
endif::[]

=== 参考
https://en.d2l.ai/chapter_reinforcement-learning/mdp.html

== (Bellman Equation)贝尔曼方程

== (Bellman Optimality Equation)贝尔曼最优公式

== 值迭代(Value Iteration)与策略迭代(Policy Iteration)
=== 概念
值迭代(Value Iteration)是强化学习中的一种经典算法，用于求解马尔可夫决策过程(MDP)的最优策略。它通过迭代更新值函数来逼近最优值函数，并最终导出最优策略。

=== 参考
https://en.d2l.ai/chapter_reinforcement-learning/value-iter.html

== 蒙特卡洛方法

== 随机近似与随机梯度下降

== 时序差分方法
=== Q-Learning
==== 概念
- 概念
Q-learning 是一种无模型的强化学习算法，用于在给定的马尔可夫决策过程(MDP)中找到最优的动作选择策略。它是一种离策略(off-policy)算法，意味着它可以独立于智能体的行为来学习最优策略。

- 优点

    无模型:
        不需要环境模型。
    保证收敛:
        在一定条件下，Q-learning 保证收敛到最优策略。

- 局限性

    可扩展性:
        对于具有许多状态和动作的问题，Q 表可能变得非常大。
    收敛速度慢:
        学习可能很慢，特别是在大型或复杂的环境中。

- 扩展与变体

    深度 Q 学习(DQN):
        将 Q-learning 与深度神经网络结合，以处理高维状态空间。
    双 Q 学习:
        通过使用两个 Q 函数减少 Q 值的高估。
    SARSA:
        Q-learning 的一种在策略(on-policy)变体，基于实际采取的动作更新 Q 值。

==== DQN(深度Q网络，Deep Q-Network)
===== 变种
- Double DQN
解耦动作选择和Q值评估，缓解过估计
https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html
https://arxiv.org/pdf/1509.06461

- Dueling DQN
分离状态价值和优势函数

- Prioritized Replay
优先回放高TD误差的经验，加速学习

===== 核心元素
- 马尔可夫决策过程(MDP)

    状态(S): 表示智能体可能处于的不同情况。
    动作(A): 智能体可以采取的可能行动。
    奖励(R): 采取动作后从环境获得的即时反馈。
    转移概率(P): 在给定动作下从一个状态转移到另一个状态的概率。

- Q值

    Q(s,a) 表示在状态 s 下采取动作 a 并随后遵循最优策略的预期累积奖励。

- 贝尔曼方程

===== 算法步骤
TODO:

==== 参考
https://en.d2l.ai/chapter_reinforcement-learning/qlearning.html
https://arxiv.org/pdf/1509.06461

== 值函数近似

== 策略梯度方法

== Actor-Critic方法

== 动态规划法

== 参考
- 赵世钰
强化学习的数学原理_西湖大学:
https://www.icourse163.org/course/XHUN-1470436188?tid=1475125444
https://www.bilibili.com/video/BV1sd4y167NS
https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning

- 王树森
https://www.bilibili.com/video/BV1hhbSzjEi1/

- 《Reinforcement Learning: An Introduction》2nd Edition
- 《Deep Reinforcement Learning Hands-On》
