:toc:
:toclevels: 5
:hardbreaks-option:

== 集群

=== cluster

==== 简介
- 多主多从的方式
多个主节点，每个主节点上可以挂载多个从节点。

- 数据分片
将数据分散到多个主节点上，而每个主节点都可以对外提供读写服务。

- 节点
redis cluster是一个去中心化的集群，每个节点都会与其他节点保持互连，使用gossip协议来交换彼此的信息，以及探测新加入的节点信息。并且redis cluster无需任何代理，客户端会直接与集群中的节点直连。

- 优点
无中心架构；
数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布；
可扩展性：可线性扩展到1000多个节点，节点可动态添加或删除；
高可用性：部分节点不可用时，集群仍可用。通过增加slave做standby数据副本，能够实现故障自动failover，节点之间通过gossip协议交换状态信息，用投票机制完成slave到master的角色提升；

- 缺点
client实现复杂，驱动要求实现smart client，缓存slots mapping信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。目前仅JedisCluster相对成熟，异常处理部分还不完善，比如常见的"max redirect exception"。
节点会因为某些原因发生阻塞(阻塞时间大于clutser-node-timeout)，被判断下线，这种failover是没有必要的。
数据通过异步复制，不保证数据的强一致性。
多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。
slave在集群中充当"冷备"，不能缓解读压力，当然可以通过SDK的合理设计来提高slave资源的利用率。
Key批量操作限制，如使用mset、mget目前只支持具有相同slot值的Key执行批量操作。对于映射为不同slot值的Key由于Keys不支持跨slot查询，所以执行mset、mget、sunion等操作支持不友好。
Key事务操作支持有限，只支持多key在同一节点上的事务操作，当多个Key分布于不同的节点上时无法使用事务功能。
Key作为数据分区的最小粒度，不能将一个很大的键值对象如hash、list等映射到不同的节点。
不支持多数据库空间，单机下的redis可以支持到16个数据库，集群模式下只能使用1个数据库空间，即db0。
复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。
避免产生hot-key，导致主库节点成为系统的短板。
避免产生big-key，导致网卡撑爆、慢查询等。
重试时间应该大于cluster-node-time时间。
redis cluster不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。

- 参考
https://fanlv.fun/2019/08/17/redis-ha/

==== 分片
redis cluster将所有数据划分为16384的slots，比codis的1024个槽划分的更为精细，每个节点负责其中一部分槽位。
槽位的信息存储于每个节点中，不像codis那样需要额外的分布式存储来存储节点槽位信息。
当客户端来连接集群时，客户端会得到一份集群的槽位配置信息。这样当客户端要查找某个key时，可以直接定位到目标节点。这点也不同于codis，codis需要通过proxy来定位目标节点，redis cluster则是直接定位。客户端为了可以直接定位某个具体的key所在的节点，需要缓存槽位相关信息。同时槽位的信息可能会存在客户端与服务器不一致的情况，因此需要纠正机制来实现槽位信息的校验调整。

槽数:
[source, c]
.https://github.com/redis/redis/blob/unstable/src/cluster.h
----
#define CLUSTER_SLOTS 16384
----

Q: 哈希槽 vs. 一致性哈希
1. key的定位方式不同:
redis cluster key的定位规则是根据CRC-16(key)%16384的值来判断key属于哪个槽区，进而判断key属于哪个节点。
一致性哈希是根据hash(key)的值来顺时针找第一个hash(ip)的节点，从而确定key存储在哪个节点。
2. 节点宕机的处理方式不同:
一致性哈希通过虚拟节点来实现节点宕机后的数据转移并保证数据的完整性和可用性。
redis cluster采用master节点写入数据，slave节点同步数据。当master节点宕机后，系统会选举出一个slave节点变成master节点。
3. 扩容缩容的处理方式不同:
一致性哈希算法在新增和删除节点后，数据会按照顺时针来重新分布节点。
而redis cluster的新增和删除节点需要手动来分配槽区。

槽位:
unsigned int keyHashSlot(char *key, int keylen):
https://github.com/redis/redis/blob/unstable/src/cluster.c

hash-tags:
https://redis.io/docs/reference/cluster-spec/#hash-tags

==== 扩容缩容

==== 实践
https://developer.aliyun.com/article/136152

==== 命令
https://redis.io/commands/?group=cluster

==== 实现
https://github.com/redis/redis/blob/unstable/src/cluster.h
https://github.com/redis/redis/blob/unstable/src/cluster.c

gossip protocol:
struct clusterMsgDataGossip

==== 参考
https://redis.io/topics/cluster-tutorial
https://redis.io/docs/reference/cluster-spec/
https://redis.io/docs/management/scaling/

=== codis
==== 简介
- 优点

    开发简单，对应用几乎透明
    性能比Twemproxy好
    图形化界面，容易扩缩容，运维方便

- 缺点

    代理依旧影响性能
    组件过多，需要很多机器资源
    修改了redis代码，和官方同步困难，新特性跟进缓慢
    有些命令不支持/支持有限: https://github.com/CodisLabs/codis/blob/master/doc/unsupported_cmds.md

- 特点
动态扩容/缩容，增减redis实例对client完全透明、不需要重启服务，不需要业务方担心Redis内存爆掉的问题；也不用担心申请太大, 造成浪费；业务方也不需要自己维护Redis.
扩容可以直接界面的"Auto Rebalance"按钮，缩容只需要将要下线的实例拥有的slot迁移到其它实例，然后在界面上删除下线的group即可。

- 架构
codis proxy: 接收客户端请求，把请求转发给codis server
codis server: 二次开发的redis实例，具有额外的数据结构，支持数据迁移操作
zookeeper: 集群元数据（数据位置信息和codis proxy信息）
codis dashboard与codis fe: 集群管理工具
    codis dashboard: 增删codis server、codis proxy和进行数据迁移
    codis fe: 提供dashboard的web操作界面
https://github.com/CodisLabs/codis/blob/master/doc/tutorial_zh.md

==== 分片
采用pre-sharding的技术来实现数据的分片, 默认分成1024个slots(0-1023), 对于每个key来说, 通过以下公式确定所属的slot id: slot_id = crc32(key) % 1024。
每一个slot都会有一个且必须有一个特定的server group id来表示这个slot的数据由哪个server group来提供。数据的迁移也是以slot为单位的。

==== 数据迁移
在源server上，从要迁移的slot中随机选择一个数据，发送给目的server;
源server接收到目标ack后，删除本地数据；
不断重复上面的迁移过程，直到要迁移的slot中的数据全部迁移完成。
注意事项:
在auto rebalance中不影响redis集群的性能，系统同时只会对几个slot进行迁移，尽量不影响其它slot的读写。
数据迁移的粒度优化到 key，针对单个key进行迁移，大key若能拆分成小Key分批次异步迁移、并在迁移过程中该Key可读、不可写，只要迁移速度够快，业务一般是可以接受的。

==== 扩容缩容

==== vs. redis cluster
https://github.com/CodisLabs/codis/blob/master/doc/FAQ_zh.md#%E7%9B%B8%E5%AF%B9%E4%BA%8Eredis-cluster%E7%9A%84%E4%BC%98%E5%8A%A3

==== vs. twemproxy
最大的区别: codis支持动态水平扩展
https://github.com/CodisLabs/codis/blob/master/doc/FAQ_zh.md#%E7%9B%B8%E5%AF%B9%E4%BA%8Etwemproxy%E7%9A%84%E4%BC%98%E5%8A%A3

==== 参考
https://github.com/CodisLabs/codis/blob/master/doc/FAQ_zh.md
https://github.com/CodisLabs/codis/blob/master/doc/tutorial_zh.md
https://github.com/CodisLabs/codis/blob/master/doc/unsupported_cmds.md

=== 数据倾斜
数据量倾斜:

    bigkey
        解决: 合理设计key
    slot手工分配不均
        解决: 合理分配slot
    使用了hash-tag
        解决: 如果hash-tag导致了数据倾斜，避免使用hash-tag

数据访问倾斜:

    存在热点数据，导致大量访问请求集中到了热点数据所在的实例上
    解决:
        只读的热点数据，可以弄成多个副本
        有写的热点数据，可以给实例增加资源

==== 参考
https://github.com/CodisLabs/codis/tree/master/doc
https://github.com/CodisLabs/codis
https://www.infoq.cn/article/b9qc-emhwuwt4qtm1xjh
